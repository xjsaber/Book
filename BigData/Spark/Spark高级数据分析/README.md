# Spark高级数据分析（第2版） #

## 第1章 大数据分析

### 1.1 管理科学面临的挑战

1. 成功的分析中，绝大部分工作是数据预处理。
2. **迭代**是数据科学的基础之一。
3. 构建完表现卓越的模型不等于大功告成。 数据科学的目标在于让数据对不懂数据科学的人有用。

### 1.2 认识Apache Spark

Spark继承了MapReduce的线性扩展性和容错性，同时对它做了一些重量级扩展。

1. Spark摒弃了MapReduce先map再reduce这样的严格方式，Spark引擎可以执行更通用的有向无环图（directed acyclic graph，DAG）算子。

## 第2章 用Scala和Spark进行数据分析

数据清洗是数据科学项目的第一步，往往也是最重要的一步。

### 2.1 数据科学家的Scala

* 性能开销小
* 能用上最新的版本和最好的功能
* 有助于你了解Spark的原理

### 2.2 Spark编程模型

Spark编程始于数据集，而数据集往往存放在分布式持久化存储之上，比如HDFS。编写Spark程序通常包括一系列相关步骤。

1. 在输入数据集上定义一组转换
2. 调用action，可以将转换后的数据集保存到持久化存储上，或者把结果返回到驱动程序的本地内存。
3. 运行本地计算，处理分布式计算的结果。

### 2.3 记录关联问题

身份解析、记录去重、合并-清除，以及列表清洗。

### 2.4 小试牛刀：Spark shell和SparkContext

1. 从资料库下载数据
2. 在HDFS上为这块数据创建一个目录，然后将数据集文件复制到HDFS上

Spark有两种方式可以创建RDD：

* 用SparkContext基于外部数据源创建RDD，外部数据源包括HDFS上的文件、通过JDBC访问的数据库表或Spark shell中创建的本地对象集合；
* 在一个或多个已有RDD上执行转换操作来创建RDD，这些转换操作包括记录过滤、对具有相同键值的记录做汇总、把多个RDD关联在一起等。

弹性分布式数据集（RDD），RDD以分区（partition）的形式分布在集群中的多个机器上，每个分区代表了数据集中的一个子集。分区定义了Spark中数据的并行单位。Spark框架并行处理多个分区，一个分区的数据对象则是顺序处理。创建RDD最简单的方法是在本地对象集合上调用SparkContext和parallelize方法。

### 2.5 把数据从集群上获取到客户端



