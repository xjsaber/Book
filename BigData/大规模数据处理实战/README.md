# 大规模数据处理实战 #

# 开篇词 #

## 开篇词 | 从这里开始，带你走上硅谷一线系统架构师之路 ##

* 这个技术解决了哪些痛点？
* 别的技术为什么不能解决？
* 这个技术用怎样的方法解决问题？
* 采用这个技术真的是最好的方法吗？
* 如果不用这个技术，你会怎样独立解决这类方法？

如果没有这些深层次的思考，你就永远只是在赶技术的时髦而已，不会拥有影响他人的技术领导力。

### 为什么写大规模数据处理？ ###

1. 低估了数据处理的重要性。
2. 低估了数据处理工程师在组织架构上的重要性。
3. 低估了数据处理规模变大带来的复杂度。 
4. 高估了上手数据处理的难度。

# 模块一|直通硅谷大规模数据处理技术 #

## 01 | 为什么MapReduce会被硅谷一线公司淘汰 ##

### 石器时代 ###

### 青铜时代 ###

### 蒸汽机时代 ###

### 高昂的维护成本 ###

数据搜集部分

1. 数据导入（data ingestion）
2. 数据统一化（data normalization）
3. 数据压缩（compression）
4. 数据备份（backup）

### 时间性能“达不到”用户的期待 ###

### 小结 ###

两个Mapreduce之所以被硅谷一线公司淘汰的“致命伤”：高昂的维护成本和达不到用户期待的时间性能。

FlumeJava：更好的可测试性；更好的可监控性；从1条数据到1亿条数据无缝扩展，不需要修改一行代码，等等。

### 思考题 ###

如果你在 Facebook 负责处理例子中的用户数据，你会选择什么分片函数，来保证均匀分布的数据分片?

## 02 | MapReduce后谁主沉浮：怎样设计下一代数据处理技术 ##

FlumeJava

1. 经受了众多产品线，超大规模数据量例如亿级用户的经验；
2. 自发地被众多内部开发者采用，简单易用而受开发者欢迎；
3. 能通过内部领域内专家的评审；
4. 比上一代技术仅仅提高 10% 是不够的，必须要有显著的比如 70% 的提高，才能够说服整个公司付出技术迁移的高昂代价。

#### 我们需要一种技术抽象让多步骤数据处理变得易于维护 ####

维护协调多个步骤的数据处理在业务中非常常见

![449ebd6c5950f5b7691d34d13a781ac7.jpg](/img/449ebd6c5950f5b7691d34d13a781ac7.jpg)

为了解决这个问题，作为架构师的我们或许可以用有向无环图（DAG）来抽象表达。因为有向图能为多个步骤的数据处理依赖关系，建立很好的模型。(数据结构与算法之美)

* 如果用 MapReduce 来实现的话，在这个图里面，每一个箭头都会是一个独立的 Map 或 Reduce。如果我们用有向图建模，图中的每一个节点都可以被抽象地表达成一种通用的数据集，每一条边都被表达成一种通用的数据变换。，你就可以用数据集和数据变换描述极为宏大复杂的数据处理流程。（我们不想要复杂的配置，需要能自动进行性能优化）
* MapReduce 的另一个问题是，配置太复杂了。以至于错误的配置最终导致数据处理任务效率低下。（人容易犯错，让人少做一点，让机器多做一点）

---
* 把两条数据处理过程进行合并
* 自动的优化是计算资源的自动弹性分配。

在数据处理开始前，我们需要有一个自动优化的步骤和能力，而不是按部就班地就把每一个步骤就直接扔给机器去执行了。

#### 我们要能把数据处理的描述语言，与背后的运行引擎解耦合开来 ####

有向图

用有向图进行数据处理描述的话，实际上**数据处理描述语言**部分完全可以和后面的**运算引擎**分离了。有向图可以作为**数据处理描述语言**和**运算引擎**的前后端分离协议。

* 比如一个网站的架构中，服务器和网页通过 HTTP 协议通信。服务器<——HTTP——>网页
* 比如在 TensorFlow 的设计中，客户端可以用任何语言（比如 Python 或者 C++）描述计算图，运行时引擎（runtime) 理论上却可以在任何地方具体运行，比如在本地，在 CPU，或者在 TPU。Runtime<——DAG——>Client
* 我的数据描述可以用 Python 描述，由业务团队使用；计算引擎用 C++ 实现，可以由数据底层架构团队维护并且高度优化；或者我的数据描述在本地写，计算引擎在云端执行。Data Engine<——DAG——>Client

有向图表达需要**数据处理描述语言**和**运算引擎**协商一致，其他的实现都是灵活可拓展的。

#### 我们要统一批处理和流处理的编程模型 ####

* 批处理处理的是有界离散的数据，比如处理一个文本文件；
* 流处理处理的是无界连续的数据，比如每时每刻的支付宝交易数据。

MapReduce的一个局限为了批处理而设计的，应对流处理的时候不再那么得心应手。（Apache Storm、Apache Flink也有类似问题，Flink里的批处理数据结构用DataSet，但是流处理用DataStream）。

真正的业务系统，批处理和流处理是常常混合共生，或者频繁变换的。

比如临时出现将批处理的数据更改成流处理，所以需要在设计的数据处理框架里，就得有更高级层的数据抽象。——>统一数据结构，无论业务需求什么样，开发者只需要学习一套API。

#### 我们要在架构层面提供异常处理和数据监控的能力 ####

对数据漏损的敏感性

### 小结 ###

![53aa1aad08b11e6c2db5cf8bb584572e.png](img/53aa1aad08b11e6c2db5cf8bb584572e.png)

Apache Spark 和 Apache Beam

### 思考题 ###

你现在在使用的数据处理技术有什么问题，你有怎样的改进设计？

### 精选留言 ###

#### 1 ####

Unify platform和批流统一已经是主要趋势了，而我个人目前只对spark、flink有一定的了解。对于spark来说，无疑是很优秀的一个引擎，包括它的all in one的组件栈，structured streaming出来后的批流api的统一，目前在做的continues Mode。而flink，的确因为阿里的运营，在国内火了。但也展现了它的独有优势，更加贴近dataflow model的思想。同时，基于社区以及阿里、华为小伙伴的努力，flink的table/sql 的api也得到的很大的增强，提供了流批统一的api。虽然底层然后需要分化为dataset和datastream以及runtime层的batchTask和StreamTask，但是现在也在rethink the stack，这个point在2019 SF 的大会也几乎吸引了所有人。但就现状而言，flink的确有着理念上的优势（流是批的超集），同时也有迅猛上升的趋势。

Spark的话虽然原生Spark Streaming Model和Dataflow Model不一样，但是Cloudera Labs也有根据Dataflow Model的原理实现了Spark Dataflow使得Beam可以跑Spark runner。

而对于Flink来说的话，在0.10版本以后它的DataStream API就已经是根据Dataflow Model的思想来重写了。现在Flink也支持两套API，分别是DataStream版本的和Beam版本的。其实data Artisans一直都有和Google保持交流，希望未来两套Beam和Flink的API能达到统一。

#### 2 ####



## 03 | 大规模数据处理初体验：怎样实现大型电商热销榜 ##

从1万用户到一亿用户，从GB数据到PB数据系统。

假设你的电商网站销售 10 亿件商品，已经跟踪了网站的销售记录：商品 id 和购买时间 {product_id, timestamp}，整个交易记录是 1000 亿行数据，TB 级。作为技术负责人，你会怎样设计一个系统，根据销售记录统计去年销量前 10 的商品呢？

个例子，假设我们的数据是：

|product_id|timestamp|
|--|--|
|1|1553721167|
|2|1553721199|
|3|1553721220|
|1|1553721241|

### 小规模的经典算法 ###

统计每个商品的销量——>找出销量前10

1. 统计每个商品的销量。可以用哈希表（hashtable）数据结构来解决，是一个 O(n) 的算法，这里 n 是 1000 亿。
2. 找出销量前十，可以用经典的 Top K 算法，也是 O(n) 的算法。

但在一切系统中，随着尺度的变大，很多方法就不再适用。具体在问题中，同样的Top K算法当数据规模变大会遇到什么问题呢？

1. 内存占用。对于 TB 级的交易记录数据，很难找到单台计算机容纳那么大的哈希表了。我把销量计数放在磁盘里完成好了。就用一个 1000 亿行的文件或者表，然后再把销量统计结果一行一行读进后面的堆树 / 优先级队列。
2. 磁盘I/O等延时问题。当数据规模变大，我们难以避免地需要把一些中间结果存进磁盘，以应对单步任务出错等问题。一次磁盘读取大概需要 10ms 的时间。如果按照上一点提到的文件替代方法，因为我们是一个 O(n * log k) 的算法，就需要 10ms * 10^9 = 10 ^ 7 s = 115 天的时间。

### 大规模分布式解决方案 ###

需要把每一步从简单的函数算法，升级为计算集群的分布式算法。

#### 统计每个商品的销量 ####

1000 台机器，每台机器一次可以处理 1 万条销售记录。对于每台机器而言，它的单次处理又回归到了我们熟悉的传统算法，数据规模大大缩小。

#### 找出销量前K ####

在上一个统计销量集群得到的数据输出，将会是我们这个处理流程的输入。所以这里需要把分布在各个机器分散的产品销量汇总出来。

#### 汇总最终结果 ####

到了最后一步，你需要把在“销量前 K 集群”中的结果汇总出来。也就是说，从所有排名前 K=1 的商品候选者中找出真正的销量前 K=1 的商品。

### 大规模数据处理框架的功能要求 ###

两个最基本的需求是：

1. 高度抽象的数据处理流程描述语言。作为用户，肯定不想配置分布式系统的每台机器了。作为框架使用者，我希望框架是非常简单的，能够用几行代码把业务逻辑描述清楚。
2. 根据描述的数据处理流程，自动化的任务分配优化。这个框架背后的引擎需要足够智能，简单地说，要把那些本来手动配置的系统，进行自动任务分配。

### 小结 ###

从 GB 数据到 TB 数据，我们从小规模算法升级到了分布式处理的设计方案；从单一 TB 数据场景到 1000 个应用场景，我们探索了大规模数据处理框架的设计。

### 思考题 ###

在你的工作中，有没有随着数据规模变大，系统出问题的情况，你又是怎么解决的？

### 精选留言 ###



# 模块二|实战学习大规模数据处理基本功 #

## 04 | 分布式系统（上）：学会用服务等级协议SLA来评估你的系统 ##

SLA（Service-Level Agreement），也就是服务等级协议，指的是系统服务提供者（Provider）对客户（Customer）的一个服务承若。这是衡量一个大型分布式系统是否“健康”的常见方法。

四个 SLA 指标，可用性、准确性、系统容量和延迟。

#### 1. 可用性（Avaiabity） ####

可用性指的是系统服务能正常运行所占的时间百分比。

对于许多系统而言，四个 9 的可用性（99.99％ Availability，或每年约 50 分钟的系统中断时间）即可以被认为是高可用性（High availability）。

#### 2. 准确性（Accracy） ####

	错误率 = 导致系统产生内部错误的有效请求数 / 期间的有效请求总数

#### 3. 系统容量（Capacity） ####

在数据处理中，系统容量通常指的是系统能够支持的预期负载量是多少，一般会以每秒的请求数为单位来表示。

1. 限流（Throtling）
2. 性能测试
3. 日志


#### 4. 延迟（Latency） ####

延迟指的是系统在收到用户的请求到相应这个请求之间的时间间隔。

### 小结 ###

## 05 | 分布式系统（下）：架构师不得不知的三大指标 ##

### 可扩展性 ###

分布式系统的核心是可扩展性（Scalability）

增加系统容量的模型有两种：水平扩展（Horizontal Scaling）和垂直扩展（Vertical Scaling）。

1. 水平扩展，在现有的系统中增加新的机器节点。
2. 垂直扩展，就是在不改变系统中机器数量的情况下，“升级”现有机器的性能。

### 一致性 ###

* 强一致性：系统中的某个数据被成功更新后，后续任何怼该数据的读取操作都将得到更新后的值。所以在任意时刻，同一系统所有节点的数据是一样的。
* 弱一致性：系统中的某个数据被更新后，后续对该数据的读取操作可能得到更新后的值，也可能是更改前的值，但经过“不一致时间窗口”这段时间后，后续怼该数据的读取都是更新后的值。
* 最终一致性：是弱一致性的特殊形式。存储系统保证，在没有新的更新的条件下，最终所有的访问都是最后更新的值。

### 持久性 ###

数据持久性（Data Durability）意味着数据一旦被成功存储就可以一直继续使用，即使系统中的节点下线、宕机或数据损坏也是如此。

在分布式数据处理系统中，还有一个持久性概念是消息持久性。在分布式系统中，节点之间需要经常相互发送消息去同步以保证一致性。

分布式系统中的消息通讯通常由分布式消息服务完成，比如RabiitMQ、Kafka。这些消息服务能支持（或配置后支持）不同级别的消息达可靠性。消息持久性包含两个方面：

1. 当消息服务的节点发生了错误，已经发送的消息仍然会在错误解决之后被处理；
2. 如果一个消息队列声明了持久性，那么即使队列在消息发送之后掉线，仍然会在重新上线之后收到这条消息。

### 小结 ###

### 思考题 ###

## 06 | 如何区分批处理还是流处理 ##

批处理（Batching Processing）和流处理（Stream Processing）

## 07 | Workflow设计模式：让你在大规模数据世界中君临天下 ##

批处理和流处理

有向无环图

### 复制模式（Copier Pattern） ###

复制模式通常是将单个数据处理模块中的数据，完整地复制到两个或更多的数据处理模块中，然后再由不同的数据处理模块进行处理。

### 过滤模式（Filter Pattern） ###

### 分离模式（Splitter Pattern） ###

### 合并模式（Joiner Pattern） ###

## 08 | 发布/订阅模式：流处理架构中的瑞士军刀 ##

处理大规模数据中十分流行的一种设计模式：发布 / 订阅模式（Publish/Subscribe Pattern），有些地方也称它为 Pub/Sub。

消息（Message）和消息队列（Message Queue）

### 消息 ###

在分布式架构里，架构中的各个组件（Component）需要相互联系沟通。组件可以是后台的数据库，可以是前端的浏览器，也可以是公司内部不同的服务终端（Service Endpoint）。

而各个组件间就是以靠通过发送消息互相通讯的。

Component A ——(Send Message)——> Component B

消息可以是任意格式的（JSON、XML或自定义的格式）。

### 消息队列 ###

消息队列在发布/订阅模式中起的是一个持久化缓冲（Durable Buffer）的作用。

消息的发送方可以发送任意消息至这个消息队列中，消息队列在接收到消息之后会将消息保存好，直到消息的接收方确认已经从这个队列拿到了这个消息，才会将这条消息从消息队列中删除。

Component A ——(Send Message)——> Message Queue  ——(Receive Message)——> Component B

### 发布/订阅模式 ###

*发布 / 订阅模式指的是消息的发送方可以将消息异步地发送给一个系统中不同组件，而无需知道接收方是谁。*在发布 / 订阅模式中，发送方被称为发布者（Publisher），接收方则被称作订阅者（Subscriber）。

发布者将消息发送到消息队列中，订阅者可以从消息队列里取出自己感兴趣的消息。在发布 / 订阅模式里，可以有任意多个发布者发送消息，也可以有任意多个订阅者接收消息。

### 发布/订阅模式的优缺点 ###

发布 / 订阅模式会有以下几个优点：

* 松耦合（Loose Coupling）：消息的发布者和消息的订阅者在开发的时候完全不需要事先知道对方的存在，可以独立地进行开发。
* 高伸缩性（High Scalability）：发布 / 订阅模式中的消息队列可以独立的作为一个数据存储中心存在。在分布式环境中，更是消息队列更是可以扩展至上千个服务器中。
* 系统组件间通信更加简洁：因为不需要为每一个消息的订阅者准备专门的消息格式，只要知道了消息队列中保存消息的格式，发布者就可以按照这个格式发送消息，订阅者也只需要按照这个格式接收消息。

但是还是存在着自身的缺点的:

* 不能保证发布者发送的数据一定会送达订阅者

在 Apache Kafka 中，消息的发送方被称为 Producer，消息的接收方被称为 Consumer，而消息队列被称为 Topic。

![b31f636250ed7e4ea9d20ef6bac3e90e.jpg](/img/b31f636250ed7e4ea9d20ef6bac3e90e.jpg)

Apache Kafka 在判断消息是否被接收方接收是利用了 Log offset 机制。

### 发布/订阅模式的适用场景 ###

如果你在处理数据的时候碰到以下场景，那么就可以考虑使用发布 / 订阅的数据处理模式。

* 系统的发送方需要向大量的接收方广播消息。
* 系统中某一个组件需要与多个独立开发的组件或服务进行通信，而这些独立开发的组件或服务可以使用不同的编程语言和通信协议。
* 系统的发送方在向接收方发送消息之后无需接收方进行实时响应。
* 系统的发送方在向接收方发送消息之后无需接收方进行实时响应。
* 系统中对数据一致性的要求只需要支持数据的最终一致性（Eventual Consistency）模型。

PS.如果系统的发送方在向接收方发送消息之后，需要接收方进行实时响应的话，那么绝大多数情况下，都不要考虑使用发布 / 订阅的数据处理模式。

### 小结 ###

发布 / 订阅模式：解耦（Decouple）系统中不同的组件，许多实时的流处理架构就是利用了这个数据处理的设计模式搭建起来的。因为发布 / 订阅模式同时具有很好的伸缩性。

### 思考题 ###

微信的朋友圈功能适合使用发布 / 订阅模式吗？为什么？

适合，如果微信的朋友圈是个大集合的话，那么发布一条朋友圈信息就相当于在这个大集合中发布一条信息，看到这个信息的人都在这个大集合中订阅一条信息。

## 09 | CAP定理：三选二，架构师必须学会的取舍 ##

## 10 | Lambda架构：Twitter亿级实时数据分析架构背后的倚天剑 ##

### Lambda架构 ###

Lambda 架构使开发人员能够构建大规模分布式数据处理系统。它具有很好的灵活性和可扩展性，也对硬件故障和人为失误有很好的容错性。

Lambda 架构总共由三层系统组成：*批处理层*（Batch Layer），*速度处理层*（Speed Layer），以及用于响应查询的*服务层*（Serving Layer）。

批处理层存储管理主数据集（不可变的数据集）和预先批处理计算好的视图。

批处理层使用可处理大量数据的分布式处理系统预先计算结果。它通过处理所有的已有历史数据来实现数据的准确性。这意味着它是基于完整的数据集来重新计算的，能够修复任何错误，然后更新现有的数据视图。输出通常存储在只读数据库中，更新则完全取代现有的预先计算好的视图。

速度处理层会实时处理新来的大数据。

速度层通过提供最新数据的实时视图来最小化延迟。速度层所生成的数据视图可能不如批处理层最终生成的视图那样准确或完整，但它们几乎在收到数据后立即可用。而当同样的数据在批处理层处理完成后，在速度层的数据就可以被替代掉了。

本质上，速度层弥补了批处理层所导致的数据视图滞后。比如说，批处理层的每个任务都需要 1 个小时才能完成，而在这 1 个小时里，我们是无法获取批处理层中最新任务给出的数据视图的。而速度层因为能够实时处理数据给出结果，就弥补了这 1 个小时的滞后。

所有在批处理层和速度层处理完的结果都输出存储在服务层中，服务层通过返回预先计算的数据视图或从速度层处理构建好数据视图来响应查询。

所有的新用户行为数据都可以同时流入批处理层和速度层。批处理层会永久保存数据并且对数据进行预处理，得到我们想要的用户行为模型并写入服务层。而速度层也同时对新用户行为数据进行处理，得到实时的用户行为模型。

一个查询即通过批处理层兼顾了数据的完整性，也可以通过速度层弥补批处理层的高延时性，让整个查询具有实时性。

### Twitter的数据分析案例 ###

因为 Apache Spark 平台中既有批处理架构也兼容了流处理架构，所以我们选择在批处理层和速度层都采用 Apache Spark 来读取来自 Apache Kafka 的数据。

批处理层和速度层在分析处理好数据后会将数据视图输出存储在服务层中，我们将使用 Apache Cassandra 平台来存储他们的数据视图。Apache Cassandra 将批处理层的视图数据和速度层的实时视图数据结合起来，就可以得到一系列有趣的数据。

### Smart Parking案例分析 ###

那速度层的数据呢？我们可以将所有用户的 GPS 数据聚集起来，这些需要每秒收集的 GPS 数据刚好又是速度层所擅长的实时流处理数据。从这些用户的实时 GPS 数据中，我们可以再建立一套预测模型来预测附近停车场位置的拥挤程度。

服务层将从批处理层和速度层得到的分数结合后将得到最高分数的停车场推荐给用户。这样利用了历史数据（停车场数据）和实时数据（用户 GPS 数据）能大大提升推荐的准确率。

### 小结 ###

### 思考题 ###

## 11 | Kappa架构：利用Kafka锻造的屠龙刀 ##

同样身为大规模数据处理架构，Kappa架构利用Kafka锻造的“屠龙刀”。

![8fe667211309978b2dd6cb6948939923.jpg](/img/8fe667211309978b2dd6cb6948939923.jpg)

Lambda架构结合了批处理和流处理的架构思想，将进入系统的大规模数据同时送入这两套架构层中，分别是批处理层（Batch Layer）和速度层（Speed Layer），同时产生两套数据结构并存入服务层。

批处理层有着很好的容错性，同时爷因为保存着所有的历史记录，使产生的数据集具有很好的准确性。速度层可以及时地处理流入的数据，因此具有低延迟性。最终服务层将这两套数据结合，并生成一个完整的数据视图提供给用户。

### Lambda架构的不足 ###

* 使用起来十分灵活，并且可以适用于很多的应用场景
* 存在着一些不足，主要表现在它的维护很复杂。

维护 Lambda 架构的复杂性在于我们要同时维护两套系统架构：批处理层和速度层。

### Kappa架构 ###

在前面 Publish–Subscribe 模式那一讲中，我讲到过像 Apache Kafka 这样的流处理平台是具有永久保存数据日志的功能的。通过平台的这一特性，我们可以重新处理部署于速度层架构中的历史数据。

Apache Kafka 为例来讲述整个全新架构的过程

1. 部署 Apache Kafka，并设置数据日志的保留期（Retention Period）。这里的保留期指的是你希望能够重新处理的历史数据的时间区间。
2. 需要改进现有的逻辑算法，那就表示我们需要对历史数据进行重新处理。
3. 当这个新的数据视图处理过的数据进度赶上了旧的数据视图时，我们的应用便可以切换到从新的数据视图中读取。
4. 停止旧版本的作业实例，并删除旧的数据视图。

#### 《纽约时报》内容管理系统架构实例 ####



### 小结 ###

学习到了 Lambda 架构和 Kappa 架构这两种大规模数据处理架构，它们都各自有着自身的优缺点。我们需要按照实际情况来权衡利弊，看看我们在业务中到底需要使用到哪种架构。

* 如果你所面对的业务逻辑是设计一种稳健的机器学习模型来预测即将发生的事情，那么你应该优先考虑使用 Lambda 架构，因为它拥有批处理层和速度层来确保更少的错误。
* 如果你所面对的业务逻辑是希望实时性比较高，而且客户端又是根据运行时发生的实时事件来做出回应的，那么你就应该优先考虑使用 Kappa 架构。

### 思考题 ###

在学习完 Lambda 架构和 Kappa 架构之后，你能说出 Kappa 架构相对 Lambda 架构的优势吗？

# 模块三|抽丝剥茧剖析 #

## 12 | 我们为什么需要Spark？ ##

### Spark的优势 ###

Spark最基本的数据抽象叫做弹性分布式数据集（Resilient Distributed Dataset，RDD），它代表一个可以被分区（partition）的只读数据集，
## 13 | 弹性分布式数据集：Spark大厦的地基（上） ##

### 为什么需要新的数据抽象模型？ ###

### RDD的定义 ###

#### RDD表示已被分区、不可变的，并能够被并行操作的数据集合 ####

#### 分区 ####

## 14 | 弹性分布式数据集：Spark大厦的地基（下） ##

### RDD的结构 ###

检查点（Checkpoint）、存储级别（Storage Level）和迭代函数（Iterator）

* MEMORY_ONLY
* MEMORY_AND_DISK
* DISK_ONLY
* MEMORY_ONLY_2

## 15 | Spark SQL:Spark数据查询的利器 ##

## 16 | Spark Streaming：Spark的实时流计算API ##

通过Spark SQL API像查询关系型数据库一样查询Spark的数据

### Spark Streaming的原理 ###

Spark Streaming 的原理与微积分的思想很类似。Spark Streaming 用时间片拆分了无限的数据流，然后对每一个数据片用类似于批处理的方法进行处理，输出的数据也是一块一块的。

SparK Streaming提供一个对于流数据的抽象DStream。DStream可以由Apache Kafka、Flume或者HDFS的流数据生成，也可以由别的DStream经过各种转换操作得来。

底层

对DStream的转换操作，意味着对它包含的每一个RDD进行同样的转换操作。



### DStream ###

DStream的内部形式，即一个连续的RDD序列，每一个RDD代表一个时间窗口的输入数据流。



## 17 | Structured Streaming：如何用DataFrame API进行实时数据分析 ##

## 18 | Word Count：从零开始运行你的第一个Spark应用 ##

1. 浅入深地学习了 Spark 的基本数据结构 RDD，了解了它这样设计的原因，以及它所支持的 API。
2. 学习了 Spark SQL 的 DataSet/DataFrame API，了解到它不仅提供类似于 SQL query 的接口，大大提高了开发者的工作效率，还集成了 Catalyst 优化器，可以提升程序的性能。
3. Spark Streaming 和 Structured Streaming。两者都是基于微批处理（Micro batch processing）的思想，将流数据按时间间隔分割成小的数据块进行批处理，实时更新计算结果。

### 安装Spark ###

#### 判断java版本 ####

java -version

#### 安装python3 ####

yum install python3

#### 安装scala ####

	$ tar -xzf ~/Dowmloads/scala-2.13.1.tgz
	$ mv scala-2.13.1 /usr/local/scala
	
	vim ~/.bash_profile
	
	export SCALA_HOME=/usr/local/scala
	export PATH=$PATH:$SCALA_HOME/bin
	
	source ~/.bash_profile

#### 安装spark ####

	$ tar -xzf ~/Dowmloads/spark-2.4.3-bin-hadoop2.7.tg
	$ mv spark-2.4.3-bin-hadoop2.7.tgz /usr/local/spark
	
	vim ~/.bash_profile
	
	export SPARK_HOME=/usr/local/spark
	export PATH=$PATH:$SPARK_HOME/bin
	
	source ~/.bash_profile

### 基于RDD API的Word Count程序 ###

对中间的先map再reduce的处理。

* Spark2.0之前
	* SparkContext 是所有 Spark 任务的入口，包含了 Spark 程序的基本设置，比如程序的名字、内存大小、并行处理的粒度等，Spark 的驱动程序需要利用它来连接到集群。
	* 无论Spark集群有多少个节点做并行处理，每个程序只可以有唯一的SparkContext，它可以被SparkConf对象初始化。
	* 
			conf = SparkConf().setAppName(appName).setMaster(master)
			sc = SparkContext(conf=conf)
* Spark2.0之后，随着新的DataFrame/DataSet API的普及化，Spark引入了新的SparkSession对象作为Spark任务的入口。
	* SparkSession不仅有SparkContext的所有功能，集成了所有Spark提供的API，比如 DataFrame、Spark Streaming 和 Structured Streaming。

--

SparkSession不仅有SparkContext的所有功能，集成了所有Spark提供的API，比如DataFrame、Spark Streaming和Structured Streaming，我们再也不用为不同的功能分别定义 Context。 

	spark = SparkSession
	       .builder
	       .appName(appName)
	       .getOrCreate()
	text_file = spark.read.text("file://….").rdd.map(lambda r: r[0])
--	

在创建好代表每一行文本的RDD之后，接下来需要两个步骤

1. 把每行的文本拆分成一个个词语；（使用flatMap去行转换成词语）
2. 统计每个词语的频率（把每个词语转换成（word, 1）的形式，然后用reduceByKey去相同词语的次数相加起来）

	counts = lines.flatMap(lambda x: x.split(' '))
		.map(lambda x: (x, 1))
        .reduceByKey(add)


### 基于DataFrame Api的Word Count程序 ###

在 DataFrame 的世界中，我们可以把所有的词语放入一张表，表中的每一行代表一个词语，当然这个表只有一列。我们可以对这个表用一个 groupBy() 操作把所有相同的词语聚合起来，然后用 count() 来统计出每个 group 的数量。

	from pyspark.sql import SparkSession
	from pyspark.sql.functions import *
	
	if __name__ == "__main__":
	   spark = SparkSession
	       .builder
	       .appName(‘WordCount’)
	       .getOrCreate()
	   lines = spark.read.text("sample.txt")
	   wordCounts = lines
	       .select(explode(split(lines.value, " "))
	       .alias("word"))
	       .groupBy("word")
	       .count()
	   wordCounts.show()
	   
	   spark.stop()

### 小结 ###

如何从零开始创建一个简单的 Spark 的应用程序，包括如何安装 Spark、如何配置环境、Spark 程序的基本结构等等。

## 19 | 综合案例实战：处理加州房屋信息，构建线性回归模型 ##

对 Spark 各种 API 的基本用法有了一定的了解，还通过统计词频的实例掌握了如何从零开始写一个 Spark 程序。让我们从一个真实的数据集出发，看看如何用 Spark 解决实际问题。

### 数据集介绍 ###

用已有的数据，构建一个**线性回归模型**。

类似 A=bB+cC+dD+…+iI 的公式，A代表房价，B到I分别代表另外八个属性。

### 进一步了解数据集 ###

每当我们需要对某个数据集进行处理时，了解它的特性，并尝试对它做一些简单的预处理，让数据的可读性更好。这些工作我们最好在 Spark 的交互式 Shell 上完成，而不是创建 python 的源文件并执行。

1. 让我们把数据集读入 Spark

	from pyspark.sql import SparkSession	

2. 把房屋信息数据和每个属性的定义读入了Spark，并创建了两个相应的RDD。RDD是有一个惰性求值的特性的，所以，可以用collect()函数来把数据输出到Shell上。

	header.collect()

	[u'longitude: continuous.', u'latitude: continuous.', u'housingMedianAge: continuous. ', u'totalRooms: continuous. ', u'totalBedrooms: continuous. ', u'population: continuous. ', u'households: continuous. ', u'medianIncome: continuous. ', u'medianHouseValue: continuous. ']


### 预处理 ###

### 创建模型 ###

### 模型评估 ###

### 小结 ###

这一讲我们通过一个真实的数据集，通过以下步骤解决了一个实际的数据处理问题：

1. 观察并了解数据集
2. 数据清洗
3. 数据的预处理
4. 训练模型
5. 评估模型

-- 
1. 熟悉与使用 Spark 各类 API
2. 了解数据处理的一般思路，并强化了对RDD、DataFrame和机器学习API的使用。

### 实践与思考题 ###

## 20 | 流处理案例实战：分析纽约市出租车载客信息 ##

### 数据集介绍 ###

### 流数据输入 ###

### 数据清洗 ###

## 21 | 深入对比Spark与Flink：帮你系统设计两开花 ##

1. 从MapReduce框架存在的问题入手，知道了Spark的主要优点，比如用内存运算来提高性能；提供很多High-level API；开发者无需用map和reduce两个操作实现复杂逻辑；支持流处理等等。
2. Spark的数据抽象——RDD。RDD是整个Spark的核心概念，所有的新API在底层都是基于RDD实现的。很底层，不方便开发者使用，而且用RDD API写的应用程序需要大量的人工调优来提高性能。
3. Spark SQL提供的DataFrame/DataSetAPI解决，使用类似SQL的查询接口，把数据看成关系型数据库的表，提升了熟悉关系型数据库的
4. Spark Streaming和Structured Streaming，这是Spark的流处理组件，其中Structured Streaming也可以使用DataSet/DataFrame API，这就实现了 Spark 批流处理的统一。

这是因为 Spark 的流处理是基于所谓微批处理（Micro-batch processing）的思想，即它把流处理看作是批处理的一种特殊形式，每次接收到一个时间间隔的数据才会去处理，所以天生很难在实时性上有所提升。

虽然在 Spark 2.3 中提出了连续处理模型（Continuous Processing Model），但是现在只支持很有限的功能，并不能在大的项目中使用。Spark 还需要做出很大的努力才能改进现有的流处理模型。

Apache Flink 就是其中的翘楚。它采用了基于操作符（Operator）的连续流模型，可以做到微秒级别的延迟。今天我就带你一起了解一下这个流行的数据处理平台，并将 Flink 与 Spark 做深入对比，方便你在今后的实际项目中做出选择。

### Flink 核心模型简介 ###

# 模块四 | Apache Beam为何能一统江湖 #

# 模块五 | 决战Apache Beam真实硅谷案例 #

# 模块六 | 大规模数据处理的挑战与未来 #

# 专栏加餐 | 特别福利 #

## FAQ第一期 | 学习大规模数据处理需要什么基础 ##

#### 问题一：学习大规模数据处理需要有什么基础？ ####

#### 问题二：小型公司程序员学习大规模数据处理的意义？ ####

1. 对于公司来讲，小型互联网公司或者传统企业，并不是不需要数据处理技能，而是他们还没有从数据中挖掘 business insight 的意识，没有数据驱动决策的意识，甚至没有收集数据的意识。
2. 对于个人来讲，你就一定要看长期的职业发展，公司会从小变大，职位会从低变高。当你需要影响决策的时候，当你面临的数据量变多的时候，当你准备跳槽的时候，数据的处理能力都是至关重要的。 

“第一讲”问题精选

“第二讲”问题精选

“第五讲”问题精选

## 结束语 | 世间所有的相遇，都是久别重逢 ##

专栏成功的指标 = 所有读者收获之和

万物皆数据


