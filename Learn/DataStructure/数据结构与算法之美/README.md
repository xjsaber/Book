# 数据结构与算法之美 #

开篇词

## 开篇词 | 从今天起，跨过“数据结构与算法”这道坎 ##

扎实的基础，是快速学习并且获得成功的秘诀。

基础知识就像是一座大楼的地基，它决定了我们的技术高度。而要想快速做出点事情，前提条件一定是基础能力过硬，“内功”要到位。

四个递进的模块：

#### 1. 入门篇 ####

时间、空间复杂度分析是数据结构和算法中非常重要的知识点，贯穿整个专栏的学习过程。

掌握时间、空间复杂度的概念，大O表示法的由来，各种复杂度分析技巧，以及最好、最坏、平均、均摊复杂度分析法。

#### 2. 基础篇 ####

针对每种数据结构和算法，我都会结合具体的软件开发实例，由浅入深进行讲解，并适时总结一些实用“宝典”。

#### 3. 高级篇 ####

讲一些不是那么常用的数据结构和算法。设置这一部分的目的，是为了让你开拓视野，强化训练算法思维、逻辑思维。

#### 4. 实战篇 ####

绕数据结构和算法在具体软件实践中的应用来讲的，所以最后我会通过实战部分串讲一下前面讲到的数据结构和算法。我会拿一些开源项目、框架或者系统设计问题，剖析它们背后的数据结构和算法，让你有一个更加直观的感受。

人生路上，我们会遇到很多的坎。跨过去，你就可以成长，跨不过去就是困难和停滞。而在后面很长的一段时间里，你都需要为这个困难买单。

## 01 | 为什么要学习数据结构和算法 ##

*想要通关大厂面试，千万别让数据结构和算法拖了后腿*

学任何知识都是为了“用”，是为了实际解决工作问题的。

*业务开发工程师，你真的愿意做一辈子 CRUD boy 吗？*

不需要自己实现，并不代表什么都不需要了解。

用到的各种框架、中间件和底层系统，比如Spring、RPC信息、消息中间件、Redis等等，在这些基础框架中，一般都揉和了很多基础数据结构和算法的设计思想。

掌握数据结构和算法，不管对于阅读框架源码，还是理解其背后的设计思想，都是非常有用的。

*基础架构研发工程师，写出达到开源水平的框架才是你的目标！*

*对编程还有追求？不想被行业淘汰？那就不要只会写凑合能用的代码！*

性能好坏起码是其中一个非常重要的判断标准。

### 内容小结 ###

学习数据结构和算法

* 建立时间复杂度、空间复杂度意识
* 写出高质量的代码
* 能够设计基础架构
* 提升编程技能
* 训练逻辑思维
* 积攒人生经验

*掌握了数据结构与算法，你看待问题的深度，解决问题的角度就会完全不一样。*

### 课后思考 ###

### 精选留言 ###

#### 1.  ####

为什么学习数据结构和算法？我认为有3点比较重要
1.直接好处是能够有写出性能更优的代码。
2.算法，是一种解决问题的思路和方法，有机会应用到生活和事业的其他方面。
3.长期来看，大脑思考能力是个人最重要的核心竞争力，而算法是为数不多的能够有效训练大脑思考能力的途径之一。

#### 2.  ####

一定要动手写

#### 3.  ####

总感觉学了就忘，忘了又学，如此反复，老师，这种到底是没了解算法的原理导致不会灵活应用，还是写的少导致的，感觉学习算法很少能应用起来

作者回复: 1. 客观的讲，有些项目确实涉及的数据结构和算法少一些，你可以再看下我文章里写的。
2. 你提到学了又忘，我觉得一方面你是没有掌握学习的方法，学习的重点，走马观花的看肯定比较容易忘；我们02节会具体讲；
3. 不会灵活应用？那估计还是没有好的教材教你如何应用，还有可能就是确实还没掌握太牢，只是懂点皮毛，很浅，灵活应用是一个比较的境界，需要一段时间的沉淀学习。
4. 学习算法并不是为了记住几个排序、二分查找、二叉树遍历，他还能锻炼你的逻辑思维、性能意识，而且，如果你写代码能力还有欠缺，你还可以通过把学到的数据结构和算法都实现一遍，这是一种很好很好的锻炼编程能力的方法。所以不要过度追求一定要在项目里手写快排、手写二叉树才能算是用上。

## 02 | 如何抓住重点，系统高效地学习数据结构与算法？ ##

看不懂数据结构和算法，*真正的原因是没有找到好的学习方法，没有抓住学习的重点。*

### 什么是数据结构？什么是算法 ###

虽然我们说没必要深挖严格的定义，但是这并不等于不需要理解概念。

广义和狭义的理解数据结构：

1. 从广义上讲，数据结构就是指一组数据的存储结构。算法就是操作数据的一组方法。
2. 从狭义上讲，是指某些著名的数据结构和算法，比如队列、栈、堆、二分查找、动态规划等。

数据结构和算法是相辅相成的，*数据结构是为算法服务的，算法要作用在特定的数据结构之上*。 

### 学习这个专栏需要什么基础 ###

是什么 => 为什么 => 怎么做

### 学习的重点再什么地方 ###

学习数据结构与算法

1. 掌握一个数据结构与算法中最重要的概念——复杂度分析
2. 最常用，最基础的20个数据结构与算法
	* 10 个数据结构：数组、链表、栈、队列、散列表、二叉树、堆、跳表、图、Trie 树
	* 10 个算法：递归、排序、二分查找、搜索、哈希算法、贪心算法、分治算法、回溯算法、动态规划、字符串匹配算法
3. 要学习它的“来历”“自身的特点”“适合解决的问题”以及“实际的应用场景”

### 一些可以让你事半功倍的学习技巧 ###

1. 边学边练、适度刷题：可以“适度”刷题，但一定不要浪费太多时间在刷题上。我们学习的目的还是掌握，然后应用
2. 多问、多思考、多互动：学习最好的方法是，找到几个人一起学习，一块儿讨论切磋，有问题及时寻求老师答疑。
3. 打怪升级学习法：学习的过程中，我们碰到最大的问题就是，坚持不下来；我们在枯燥的学习过程中，也可以给自己设立一个切实可行的目标。
4. 知识需要沉淀，不要想视图一下子掌握所有：学习知识的过程是反复迭代、不断沉淀的过程。

### 内容小结 ###

* 数据结构和算法的学习重点，复杂度分析，以及10个数据结构和10个算法。
* 学习技巧的总结

## 03 | 复杂度分析（上）：如何分析、统计算法的执行效率和资源消耗 ##

复杂度分析是整个算法学习的精髓，只要掌握了它，数据结构和算法的内容基本上就掌握了一半。

### 为什么需要复杂度分析 ###

事后统计法

1. 测试结构非常依赖测试环境
2. 测试结果受数据规模的影响很大

*我们需要一个不用具体的测试数据来测试，就可以粗略地估计算法的执行效率的方法*。

### 大O复杂度表示法 ###

	 int cal(int n) {
	   int sum = 0;
	   int i = 1;
	   for (; i <= n; ++i) {
	     sum = sum + i;
	   }
	   return sum;
	 }

假设每行代码执行的时间都一样，为 unit_time，第 2、3 行代码分别需要 1 个 unit_time 的执行时间，第 4、5 行都运行了 n 遍，所以需要 2n*unit_time 的执行时间，所以这段代码总的执行时间就是 (2n+2)*unit_time。*所有代码的执行时间 T(n) 与每行代码的执行次数成正比。*

	T(n) = O(f(n))

T(n) 它表示代码执行的时间；n 表示数据规模的大小；f(n) 表示每行代码执行的次数总和。

大 O 时间复杂度实际上并不具体表示代码真正的执行时间，而是表示代码执行时间随数据规模增长的变化趋势，所以，也叫作渐进时间复杂度（asymptotic time complexity），简称时间复杂度。

### 时间复杂度分析 ###

1. 只关注循环次数最多的一段代码:我们在分析一个算法、一段代码的时间复杂度的时候，也只关注循环执行次数最多的那一段代码就可以了
2. 加法法则：总复杂度等于量级最大的那段代码的复杂度
3. 乘法法则：嵌套代码的复杂度等于嵌套内外代码复杂度的乘积。

### 几种常见时间复杂度实例分析 ###

* 常量阶O(1)
* 对数阶O(log^n)
* 线性阶O(n)
* 线性对数阶O(nlog^n)
* 指数阶O(2^n)
* 阶乘阶O(n!)
* 平方阶O(n^2)、立方阶O(n^3)....K次方阶O(n^k)

*多项式量级*和*非多项式量级*

非多项式量级：O(2n) 和 O(n!)

#### 1. O(1) ####

	 int i = 8;
	 int j = 6;
	 int sum = i + j;

O(1) 只是常量级时间复杂度的一种表示方法，并不是指只执行了一行代码。

一般情况下，只要算法中不存在循环语句、递归语句，即使有成千上万行的代码，其时间复杂度也是Ο(1)。

#### 2. O(logn)、O(nlogn) ####

	 i=1;
	 while (i <= n)  {
	   i = i * 2;
	 }

*在采用大 O 标记复杂度的时候，可以忽略系数，即 O(Cf(n)) = O(f(n))*。所以，O(log2n) 就等于 O(log3n)。因此，在对数阶时间复杂度的表示方法里，我们忽略对数的“底”，统一表示为 O(logn)。

2^x=n，O(log~2n)

#### 3. O(m+n)、O(m*n) ####

代码的复杂度由*两个数据的规模*来决定

	int cal(int m, int n) {
	  int sum_1 = 0;
	  int i = 1;
	  for (; i < m; ++i) {
	    sum_1 = sum_1 + i;
	  }
	
	  int sum_2 = 0;
	  int j = 1;
	  for (; j < n; ++j) {
	    sum_2 = sum_2 + j;
	  }
	
	  return sum_1 + sum_2;
	}

从代码来看，m 和 n 是表示两个数据规模。我们无法事先评估 m 和 n 谁的量级大，所以我们在表示复杂度的时候，就不能简单地利用加法法则，省略掉其中一个。所以，上面代码的时间复杂度就是 O(m+n)。需要将加法规则改为：T1(m) + T2(n) = O(f(m) + g(n))。但是乘法法则继续有效：T1(m)*T2(n) = O(f(m) * f(n))

### 空间复杂度分析 ###

时间复杂度的全称是*渐进时间复杂度，表示算法的执行时间与数据规模之间的增长关系*。空间复杂度全称就是*渐进空间复杂度（asymptotic space complexity），表示算法的存储空间与数据规模之间的增长关系*。

	void print(int n) {
	  int i = 0;
	  int[] a = new int[n];
	  for (i; i <n; ++i) {
	    a[i] = i * i;
	  }
	
	  for (i = n-1; i >= 0; --i) {
	    print out a[i]
	  }
	}

* 第 2 行代码中，我们申请了一个空间存储变量 i，但是它是常量阶的，跟数据规模 n 没有关系，所以我们可以忽略。
* 第 3 行申请了一个大小为 n 的 int 类型数组

剩下的代码都没有占用更多的空间，所以整段代码的空间复杂度就是 O(n)。 

### 内容小结 ###

复杂度也叫渐进复杂度，包括时间复杂度和空间复杂度，用来分析算法执行效率与数据规模之间的增长关系。越高阶复杂度的算法，执行效率越低。常见的复杂度并不多，从低阶到高阶有：O(1)、O(log^n)、O(n)、O(nlog^n)、O(n^2)。

*复杂度分析并不难，关键在于多练*。

### 课后思考 ###

有人说，我们项目之前都会进行性能测试，再做代码的时间复杂度、空间复杂度分析，是不是多此一举呢？而且，每段代码都分析一下时间复杂度、空间复杂度，是不是很浪费时间呢？你怎么看待这个问题呢？

### 精选留言 ###

#### 1.  ####

我不认为是多此一举，渐进时间，空间复杂度分析为我们提供了一个很好的理论分析的方向，并且它是宿主平台无关的，能够让我们对我们的程序或算法有一个大致的认识，让我们知道，比如在最坏的情况下程序的执行效率如何，同时也为我们交流提供了一个不错的桥梁，我们可以说，算法1的时间复杂度是O(n)，算法2的时间复杂度是O(logN)，这样我们立刻就对不同的算法有了一个“效率”上的感性认识。

当然，渐进式时间，空间复杂度分析只是一个理论模型，只能提供给粗略的估计分析，我们不能直接断定就觉得O(logN)的算法一定优于O(n), 针对不同的宿主环境，不同的数据集，不同的数据量的大小，在实际应用上面可能真正的性能会不同，个人觉得，针对不同的实际情况，进而进行一定的性能基准测试是很有必要的，比如在统一一批手机上(同样的硬件，系统等等)进行横向基准测试，进而选择适合特定应用场景下的最有算法。

综上所述，渐进式时间，空间复杂度分析与性能基准测试并不冲突，而是相辅相成的，但是一个低阶的时间复杂度程序有极大的可能性会优于一个高阶的时间复杂度程序，所以在实际编程中，时刻关心理论时间，空间度模型是有助于产出效率高的程序的，同时，因为渐进式时间，空间复杂度分析只是提供一个粗略的分析模型，因此也不会浪费太多时间，重点在于在编程时，要具有这种复杂度分析的思维。

## 04 | 复杂度分析（下）：浅析最好、最坏、平均、均摊时间复杂度 ##

四个复杂度分析方面的知识点：最好情况时间复杂度（best case time complexity）、最坏情况时间复杂度（worst case time complexity）、平均情况时间复杂度（average case time complexity）、均摊时间复杂度（amortized time complexity）。

### 最好、最坏情况时间复杂度 ###

	// n表示数组array的长度
	int find(int[] array, int n, int x) {
	  int i = 0;
	  int pos = -1;
	  for (; i < n; ++i) {
	    if (array[i] == x) pos = i;
	  }
	  return pos;
	}

* 最好情况时间复杂度就是，在最理想的情况下，执行这段代码的时间复杂度。
* 最坏情况时间复杂度就是，在最糟糕的情况下，执行这段代码的时间复杂度。

### 平均情况时间复杂度 ###

要查找的变量 x 在数组中的位置，有 n+1 种情况：**在数组的 0～n-1 位置中**和**不在数组中**。我们把每种情况下，查找需要遍历的元素个数累加起来，然后再除以 n+1，就可以得到需要遍历的元素个数的平均值，即：(1+2+3+⋯+n+n)/(n+1)=n(n+3))/(2(n+1)

时间复杂度的大 O 标记法中，可以省略掉系数、低阶、常量，得到的平均时间复杂度就是 O(n)。

要查找的变量x在数组中的位置，有n+1种情况：在数组的0~n-1位置中和不在数组中。我们把每种情况下，查找需要遍历的元素个数累加起来，然后再除以n+1，就可以得到需要遍历的元素个数的平均值。

概率论中的*加权平均值*，也叫作*期望值*，所以平均时间复杂度的全称应该叫*加权平均时间复杂度*或者*期望时间复杂度*。

引入概率之后，前面那段代码的加权平均值为 (3n+1)/4。用大 O 表示法来表示，去掉系数和常量，这段代码的加权平均时间复杂度仍然是 O(n)。

### 均摊时间复杂度 ###

	 // array表示一个长度为n的数组
	 // 代码中的array.length就等于n
	 int[] array = new int[n];
	 int count = 0;
	 
	 void insert(int val) {
	    if (count == array.length) {
	       int sum = 0;
	       for (int i = 0; i < array.length; ++i) {
	          sum = sum + array[i];
	       }
	       array[0] = sum;
	       count = 1;
	    }
	
	    array[count] = val;
	    ++count;
	 }

1. find（）函数在极端情况下，复杂度才为O（1）；insert() 在大部分情况下，时间复杂度都为 O(1)。只有个别情况下，复杂度才比较高，为 O(n)。这是 insert()第一个区别于 find() 的地方。
2. 对于 insert() 函数来说，O(1) 时间复杂度的插入和 O(n) 时间复杂度的插入，出现的频率是非常有规律的，而且有一定的前后时序关系，一般都是一个 O(n) 插入之后，紧跟着 n-1 个 O(1) 的插入操作，循环往复。

针对这样一种特殊场景的复杂度分析，我们并不需要像之前讲平均复杂度分析方法那样，找出所有的输入情况及相应的发生概率，然后再计算加权平均值。

摊还分析法，通过摊还分析得到的时间复杂度我们起了一个名字，叫均摊时间复杂度。

均摊时间复杂度就是一种特殊的平均时间复杂度

### 内容小结 ###

复杂度分析方法分别有：最好情况时间复杂度、最坏情况时间复杂度、平均情况时间复杂度、均摊时间复杂度。

### 课后思考 ###

分析一下下面这个 add() 函数的时间复杂度

	// 全局变量，大小为10的数组array，长度len，下标i。
	int array[] = new int[10]; 
	int len = 10;
	int i = 0;
	
	// 往数组中添加一个元素
	void add(int element) {
	   if (i >= len) { // 数组空间不够了
	     // 重新申请一个2倍大小的数组空间
	     int new_array[] = new int[len*2];
	     // 把原来array数组中的数据依次copy到new_array
	     for (int j = 0; j < len; ++j) {
	       new_array[j] = array[j];
	     }
	     // new_array复制给array，array现在大小就是2倍len了
	     array = new_array;
	     len = 2 * len;
	   }
	   // 将element放到下标为i的位置，下标i加一
	   array[i] = element;
	   ++i;
	}

基础篇

## 05 | 数组：为什么很多编程语言中数组都从0开始编号？ ##

*为什么数组要从 0 开始编号，而不是从 1 开始呢？*

### 如何实现随机访问？ ###

数组（Array）是一种线性表数据结构。它用一组连续的内存空间，来存储一组具有相同类型的数据。

1.* 线性表（Linear List）*：线性表就是数据排成像一条线一样的结构。每个线性表上的数据最多只有前和后两个方向。其实除了数组，链表、队列、栈等也是线性表结构。*非线性表*，比如二叉树、堆、图等。在非线性表中，数据之间并不是简单的前后关系。
2. *连续的内存空间和相同类型的数据*。堪称“杀手锏”的特性：**“随机访问”**，但随之带来让数组的很多操作变得非常低效。

### 低效的“插入”和删除 ###

#### 插入操作 ####

* 如果插入数组末尾的数据，则最好情况时间复杂度为 O(1)；
* 如果插入开头的数据，则最坏情况时间复杂度为 O(n)；
* 平均情况时间复杂度也为 O(n)。

如果是有序数组，在某个位置插入一个新的元素，必需按照搬移k之后的数据进行操作；但如果是无序数组，直接将第 k 位的数据搬移到数组元素的最后，把新的元素直接放入第 k 个位置（避免出现大规模数据迁移）。

	假设数组 a[10]中存储了如下 5 个元素：a，b，c，d，e。我们现在需要将元素 x 插入到第 3 个位置。我们只需要将 c 放入到 a[5]，将 a[2]赋值为 x 即可。最后，数组中的元素如下： a，b，x，d，e，c。

![3f70b4ad9069ec568a2caaddc231b7dc.jpg](img/3f70b4ad9069ec568a2caaddc231b7dc.jpg)

	利用这种处理技巧，在特定场景下，在第 k 个位置插入一个元素的时间复杂度就会降为 O(1)。

#### 删除操作 ####

* 如果删除数组末尾的数据，则最好情况时间复杂度为 O(1)；
* 如果删除开头的数据，则最坏情况时间复杂度为 O(n)；
* 平均情况时间复杂度也为 O(n)。

在某些特殊的场景下，不一定非得追求数组中数据的连续性。如果我们将多次删除操作集中在一起执行，删除的效率是不是会提高很多呢？

	数组 a[10]中存储了 8 个元素：a，b，c，d，e，f，g，h。现在，我们要依次删除 a，b，c 三个元素。

![b69b8c5dbf6248649ddab7d3e7cfd7e5.jpg](img/b69b8c5dbf6248649ddab7d3e7cfd7e5.jpg)

	为了避免d，e，f，g，h这几个数据会被搬移三次，先记录下已经删除的数据。每次的删除操作并不是真正地搬移数据，只是记录数据已经被删除。。当数组没有更多空间存储数据时，我们再触发执行一次真正的删除操作。（JVM标记清除垃圾回收算法的核心思想）

很多时候我们并不是要去死记硬背某个数据结构或者算法，而是要学习它背后的思想和处理技巧，这些东西才是最有价值的。

### 警惕数组的访问越界问题 ###

	int main(int argc, char* argv[]){
	    int i = 0;
	    int arr[3] = {0};
	    for(; i<=3; i++){
	        arr[i] = 0;
	        printf("hello world\n");
	    }
	    return 0;
	}

### 容器能否完全替代数组？ ###

容器的优点：

1. 封装细节
2. 支持动态扩容

虽然容器支持动态扩容，但涉及内存申请和数据搬移（比较耗时）。以，如果事先能确定需要存储的数据大小，最好在创建 ArrayList 的时候事先指定数据大小。

1. Java ArrayList 无法存储基本类型，比如 int、long，需要封装为 Integer、Long 类，而 Autoboxing、Unboxing 则有一定的性能消耗，所以如果特别关注性能，或者希望使用基本类型，就可以选用数组。
2. 如果数据大小事先已知，并且对数据的操作非常简单，用不到 ArrayList 提供的大部分方法，也可以直接使用数组。
3. 还有一个是我个人的喜好，当要表示多维数组时，用数组往往会更加直观。比如 `Object[][] array`；而用容器的话则需要这样定义：`ArrayList<ArrayList<object>> array。`

### 解答开题 ###

为什么大多数编程语言中，数组要从 0 开始编号，而不是从 1 开始呢？

1. 从数组存储的内存模型上来看，“下标”最确切的定义应该是“偏移（offset）”如果用 a 来表示数组的首地址，a[0]就是偏移为 0 的位置，也就是首地址，a[k]就表示偏移 k 个 type_size 的位置，所以计算 a[k]的内存地址只需要用这个公式：`a[k]_address = base_address + k * type_size`。但是，如果数组从 1 开始计数，那我们计算数组元素 a[k]的内存地址就会变为：`a[k]_address = base_address + (k-1)*type_size`（对CPU来说，多一个减法指令）
2. 历史原因

### 内容小结 ###

数组：最基础、最简单的数据结构；数组用一块连续的内存空间，来存储相同类型的一组数据，最大的特点就是支持随机访问，但插入、删除操作也因此变得比较低效，平均情况时间复杂度为 O(n)。

1. 在平时的业务开发中，我们可以直接使用编程语言提供的容器类
2. 如果是特别底层的开发，直接使用数组可能会更合适

### 课后思考 ###

1. 前面我基于数组的原理引出 JVM 的标记清除垃圾回收算法的核心理念。我不知道你是否使用 Java 语言，理解 JVM，如果你熟悉，可以在评论区回顾下你理解的标记清除垃圾回收算法。
2. 前面我们讲到一维数组的内存寻址公式，那你可以思考一下，类比一下，二维数组的内存寻址公式是怎样的呢？

## 06 | 链表（上）：如何实现LRU缓存淘汰算法？ ##

如何用链表来实现 LRU 缓存淘汰策略呢？

### 五花八门的链表结构 ###

#### 底层的存储结构 ####

* 数组：需要**一块连续的内存空间**
* 链表：通过“指针”将一组**零散的内存块**串联起来使用，其中，我们把内存块称为链表的“结点”。为了将所有的结点串起来，每个链表的结点除了存储数据之外，还需要记录链上的下一个结点的地址（后继指针next）。

两个特殊结点：

* 头结点：第一个结点，记录链表的基地址
* 尾结点：最后一个结点，不是指向下一个结点，而是指向一个空地址 NULL

优点和缺点

* 链表的存储空间本身不是连续的，所以插入和删除一个数据是非常快速的。
* 链表要想随机访问第 k 个元素，就没有数组那么高效了。因为链表中的数据并非连续存储的，（根据首地址和下标，通过寻址公式就能直接计算出对应的内存地址——数组的优点），而是需要根据指针一个结点一个结点地依次遍历，直到找到相应的结点。

#### 循环链表 ####

循环链表是一种特殊的单链表

优点：从链尾到链头比较方便：当要处理的数据具有环型结构特点时，就特别适合采用循环链表

#### 双向链表 ####

双向链表，顾名思义，它支持两个方向，每个结点不止有一个后继指针 next 指向后面的结点，还有一个前驱指针 prev 指向前面的结点。

从链表中删除一个数据：

* 删除结点中“值等于某个给定值”的结点
* 删除给定指针指向的结点：

**删除结点中“值等于某个给定值”的结点**

需要一个结点一个结点遍历，尽管单纯的删除操作时间复杂度是 O(1)，但遍历查找的时间是主要的耗时点，对应的时间复杂度为 O(n)。根据时间复杂度分析中的加法法则，删除值等于给定值的结点对应的链表操作的总时间复杂度为 O(n)。

**删除给定指针指向的结点**

因为双向链表中的结点已经保存了前驱结点的指针，不需要像单链表那样遍历。

Java的实现，LinkedHashMap


**空间换时间**的设计思想，当内存空间充足的时候，如果我们更加追求代码的执行速度，我们就可以选择空间复杂度相对较高、但时间复杂度相对很低的算法或者数据结构。相反，如果内存比较紧缺，比如代码跑在手机或者单片机上，这个时候，就要反过来用时间换空间的设计思路。

扩展：双向循环链表

### 链表 VS 数组性能大比拼 ###

* 数组：简单易用，在实现上使用的是连续的内存空间，可以借助 CPU 的缓存机制，预读数组中的数据，所以访问效率更高。
* 链表：链表在内存中并不是连续存储，所以对 CPU 缓存不友好，没办法有效预读。
* 数组：大小固定，如果声明的数组过大，系统可能没有足够的连续内存空间分配给它，导致“内存不足（out of memory）”。如果声明的数组过小，则可能出现不够用的情况。这时只能再申请一个更大的内存空间，把原数组拷贝进去，非常费时。
* 链表：链表本身没有大小的限制，天然地支持动态扩容

### 解答开篇 ###



## 07 | 链表（下）：如何轻松写出正确的链表代码？ ##

### 技巧一：理解指针或引用的含义 ###

*将某个变量赋值给指针，实际上就是将这个变量的地址赋值给指针，或者反过来说，指针中存储了这个变量的内存地址，指向了这个变量，通过指针就能找到这个变量。*

### 技巧二：警惕指针丢失和内存泄漏 ###

* 插入结点时，一定要注意操作的顺序
* 删除链表结点时，也一定要记得手动释放内存空间

### 技巧三：利用哨兵简化实现难度 ###

head 指针都会一直指向这个哨兵结点。我们也把这种有哨兵结点的链表叫带头链表。相反，没有哨兵结点的链表就叫作不带头链表。

### 技巧四：重点留意边界条件处理 ###

要实现没有 Bug 的链表代码，一定要在编写的过程中以及编写完成之后，检查边界条件是否考虑全面，以及代码在边界条件下是否能正确运行。

检查链表代码是否正确的边界条件有这样几个：

* 如果链表为空时，代码是否能正常工作？
* 如果链表只包含一个结点时，代码是否能正常工作？
* 如果链表只包含两个结点时，代码是否能正常工作？
* 代码逻辑在处理头结点和尾结点的时候，是否能正常工作？

不光光是写链表代码，你在写任何代码时，也千万不要只是实现业务正常情况下的功能就好了，一定要多想想，你的代码在运行的时候，可能会遇到哪些边界情况或者异常情况。遇到了应该如何应对，这样写出来的代码才够健壮！

### 技巧五：举例画图，辅助思考 ###

“举例法”和“画图法”

### 技巧六：多写多练，没有捷径 ###

* 单链表反转
* 链表中环的检测
* 两个有序的链表合并
* 删除链表倒数第 n 个结点
* 求链表的中间结点

### 内容小结 ###

* 理解指针或引用的含义
* 警惕指针丢失和内存泄漏
* 利用哨兵简化实现难度
* 重点留意边界条件处理
* 以及举例画图、辅助思考
* 多写多练。

写链表代码是最考验逻辑思维能力的。

### 课后思考 ###

今天我们讲到用哨兵来简化编码实现，你是否还能够想到其他场景，利用哨兵可以大大地简化编码难度？

## 08 | 栈：如何实现浏览器的前进和后退功能？ ##

### 如何理解“栈”？ ###

后进者先出，先进者后出，这就是典型的“栈”结构。

* 操作特性：栈是一种“操作受限”的线性表
* 功能：数组和链表都可以替代栈，但操作上灵活自由，使用时比较不可控

当某个数据集合只涉及在一端插入和删除数据，并且满足后进先出、先进后出的特性，我们就应该首选“栈”这种数据结构。

### 如何实现一个“栈”？ ###

* 顺序栈
* 链式栈

### 支持动态扩容的顺序栈 ###

### 栈在函数调用中的应用 ###

11 | 排序（上）：为什么插入排序比冒泡排序更受欢迎？

14 | 排序优化

## 15 | 二分查找（上） ##

假设我们有 1000 万个整数数据，每个数据占 8 个字节，如何设计数据结构和算法，快速判断某个整数是否出现在这 1000 万数据中？ 

### 无处不在的二分查找 ###

二分查找针对的是一个有序的数据集合，查找思想有点类似分治思想。每次都通过跟区间的中间元素对比，将待查找的区间缩小为之前的一半，直到找到要查找的元素，或者区间被缩小为 0。

### O（logn）惊人的查找速度 ###

时间复杂度是O(logn)的算法，可能比常量级O(1)的算法还要高效。因为O(1)有可能表示的十一个非常大的常量值。

另外，指数时间复杂度的算法在大规模数据面前是无效的。

### 二分查找的递归与非递归实现 ###

最简单的情况就是有序数组中不存在重复元素

容易出错的3个地方

#### 1. 循环退出条件 ####

low<=high

#### 2. mid的取值 ####

`mid =（low+high/2`，low和high比较大的话，两者之和有可能会溢出。 可以改进为`low+(high-low)/2`甚至 `low+((high-low)>>1)`

#### 3. low和high的更新 ####

low=mid+1，high=mid-1。注意这里的 +1 和 -1，如果直接写成 low=mid 或者 high=mid，就可能会发生死循环。比如，当 high=3，low=3 时，如果 a[3]不等于 value，就会导致一直循环不退出。

实际上，二分查找除了用循环来实现，还可以用递归来实现

### 二分查找应用场景的局限性 ###

二分查找的时间复杂度是 O(logn)，查找数据的效率非常高。

#### 首先，二分查找依赖的是顺序表结构，简单点说就是数组 ####

二分查找无法依赖于链表，如果使用链表来查找时间复杂度会十分高。

二分查找只能用在数据是通过顺序表来存储的数据结构上。如果你的数据是通过其他数据结构存储的，则无法应用二分查找。

#### 其次，二分查找针对的是有序数据。 ####

1. 数据必须是有序的。如果数据没有序，我们需要先排序。
2. 如果数据集合有频繁的插入和删除操作，针对这种动态数据集合，维护有序的成本很高
3. 二分查找只能用在插入、删除操作不频繁，一次排序多次查找的场景中。

#### 再次，数据量太小不适合二分查找。 ####

* 要处理得数据量很小，没有必要使用二分查找，使用顺序遍历便可
* 但如果数据之间的比较操作非常耗时，不管数据量大小，都推荐使用二分查找。

#### 最后，数据量太大也不适合二分查找。 ####

二分查找的底层需要依赖数组这种数据结构，而数组为了支持随机访问的特性，要求内存空间连续，对内存的要求比较苛刻。比如，我们有 1GB 大小的数据，如果希望用数组来存储，那就需要 1GB 的连续内存空间。

### 解答开题 ###

### 内容小结 ###

针对*有序*数据的高效查找算法，二分查找，它的时间复杂度是* O(logn)*。

二分查找的核心思想理解起来非常简单，有点类似分治思想。即每次都通过跟区间中的中间元素对比，将待查找的区间缩小为一半，直到找到要查找的元素，或者区间被缩小为 0。但是二分查找的代码实现比较容易写错。你需要着重掌握它的三个容易出错的地方：

1. 循环退出条件
2. mid 的取值
3. low 和 high 的更新。

二分查找虽然性能比较优秀，但应用场景也比较有限。

1. 底层必须依赖数组，并且还要求数据是有序的。
2. 对于较小规模的数据查找，我们直接使用顺序遍历就可以了，二分查找的优势并不明显。
3. 二分查找更适合处理静态数据，也就是没有频繁的数据插入、删除操作。

### 课后思考 ###

15 | 二分查找（下）	

高级篇

