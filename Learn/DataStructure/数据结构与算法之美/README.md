# 数据结构与算法之美 #

开篇词

## 开篇词 | 从今天起，跨过“数据结构与算法”这道坎 ##

扎实的基础，是快速学习并且获得成功的秘诀。

基础知识就像是一座大楼的地基，它决定了我们的技术高度。而要想快速做出点事情，前提条件一定是基础能力过硬，“内功”要到位。

四个递进的模块：

#### 1. 入门篇 ####

时间、空间复杂度分析是数据结构和算法中非常重要的知识点，贯穿整个专栏的学习过程。

掌握时间、空间复杂度的概念，大O表示法的由来，各种复杂度分析技巧，以及最好、最坏、平均、均摊复杂度分析法。

#### 2. 基础篇 ####

针对每种数据结构和算法，我都会结合具体的软件开发实例，由浅入深进行讲解，并适时总结一些实用“宝典”。

#### 3. 高级篇 ####

讲一些不是那么常用的数据结构和算法。设置这一部分的目的，是为了让你开拓视野，强化训练算法思维、逻辑思维。

#### 4. 实战篇 ####

绕数据结构和算法在具体软件实践中的应用来讲的，所以最后我会通过实战部分串讲一下前面讲到的数据结构和算法。我会拿一些开源项目、框架或者系统设计问题，剖析它们背后的数据结构和算法，让你有一个更加直观的感受。

人生路上，我们会遇到很多的坎。跨过去，你就可以成长，跨不过去就是困难和停滞。而在后面很长的一段时间里，你都需要为这个困难买单。

## 01 | 为什么要学习数据结构和算法 ##

*想要通关大厂面试，千万别让数据结构和算法拖了后腿*

学任何知识都是为了“用”，是为了实际解决工作问题的。

*业务开发工程师，你真的愿意做一辈子 CRUD boy 吗？*

不需要自己实现，并不代表什么都不需要了解。

用到的各种框架、中间件和底层系统，比如Spring、RPC信息、消息中间件、Redis等等，在这些基础框架中，一般都揉和了很多基础数据结构和算法的设计思想。

掌握数据结构和算法，不管对于阅读框架源码，还是理解其背后的设计思想，都是非常有用的。

*基础架构研发工程师，写出达到开源水平的框架才是你的目标！*

*对编程还有追求？不想被行业淘汰？那就不要只会写凑合能用的代码！*

性能好坏起码是其中一个非常重要的判断标准。

### 内容小结 ###

学习数据结构和算法

* 建立时间复杂度、空间复杂度意识
* 写出高质量的代码
* 能够设计基础架构
* 提升编程技能
* 训练逻辑思维
* 积攒人生经验

*掌握了数据结构与算法，你看待问题的深度，解决问题的角度就会完全不一样。*

### 课后思考 ###

### 精选留言 ###

#### 1.  ####

为什么学习数据结构和算法？我认为有3点比较重要
1.直接好处是能够有写出性能更优的代码。
2.算法，是一种解决问题的思路和方法，有机会应用到生活和事业的其他方面。
3.长期来看，大脑思考能力是个人最重要的核心竞争力，而算法是为数不多的能够有效训练大脑思考能力的途径之一。

#### 2.  ####

一定要动手写

#### 3.  ####

总感觉学了就忘，忘了又学，如此反复，老师，这种到底是没了解算法的原理导致不会灵活应用，还是写的少导致的，感觉学习算法很少能应用起来

作者回复: 1. 客观的讲，有些项目确实涉及的数据结构和算法少一些，你可以再看下我文章里写的。
2. 你提到学了又忘，我觉得一方面你是没有掌握学习的方法，学习的重点，走马观花的看肯定比较容易忘；我们02节会具体讲；
3. 不会灵活应用？那估计还是没有好的教材教你如何应用，还有可能就是确实还没掌握太牢，只是懂点皮毛，很浅，灵活应用是一个比较的境界，需要一段时间的沉淀学习。
4. 学习算法并不是为了记住几个排序、二分查找、二叉树遍历，他还能锻炼你的逻辑思维、性能意识，而且，如果你写代码能力还有欠缺，你还可以通过把学到的数据结构和算法都实现一遍，这是一种很好很好的锻炼编程能力的方法。所以不要过度追求一定要在项目里手写快排、手写二叉树才能算是用上。

## 02 | 如何抓住重点，系统高效地学习数据结构与算法？ ##

看不懂数据结构和算法，*真正的原因是没有找到好的学习方法，没有抓住学习的重点。*

### 什么是数据结构？什么是算法 ###

虽然我们说没必要深挖严格的定义，但是这并不等于不需要理解概念。

广义和狭义的理解数据结构：

1. 从广义上讲，数据结构就是指一组数据的存储结构。算法就是操作数据的一组方法。
2. 从狭义上讲，是指某些著名的数据结构和算法，比如队列、栈、堆、二分查找、动态规划等。

数据结构和算法是相辅相成的，*数据结构是为算法服务的，算法要作用在特定的数据结构之上*。 

### 学习这个专栏需要什么基础 ###

是什么 => 为什么 => 怎么做

### 学习的重点再什么地方 ###

学习数据结构与算法

1. 掌握一个数据结构与算法中最重要的概念——复杂度分析
2. 最常用，最基础的20个数据结构与算法
	* 10 个数据结构：数组、链表、栈、队列、散列表、二叉树、堆、跳表、图、Trie 树
	* 10 个算法：递归、排序、二分查找、搜索、哈希算法、贪心算法、分治算法、回溯算法、动态规划、字符串匹配算法
3. 要学习它的“来历”“自身的特点”“适合解决的问题”以及“实际的应用场景”

### 一些可以让你事半功倍的学习技巧 ###

1. 边学边练、适度刷题：可以“适度”刷题，但一定不要浪费太多时间在刷题上。我们学习的目的还是掌握，然后应用
2. 多问、多思考、多互动：学习最好的方法是，找到几个人一起学习，一块儿讨论切磋，有问题及时寻求老师答疑。
3. 打怪升级学习法：学习的过程中，我们碰到最大的问题就是，坚持不下来；我们在枯燥的学习过程中，也可以给自己设立一个切实可行的目标。
4. 知识需要沉淀，不要想视图一下子掌握所有：学习知识的过程是反复迭代、不断沉淀的过程。

### 内容小结 ###

* 数据结构和算法的学习重点，复杂度分析，以及10个数据结构和10个算法。
* 学习技巧的总结

## 03 | 复杂度分析（上）：如何分析、统计算法的执行效率和资源消耗 ##

复杂度分析是整个算法学习的精髓，只要掌握了它，数据结构和算法的内容基本上就掌握了一半。

### 为什么需要复杂度分析 ###

事后统计法

1. 测试结构非常依赖测试环境
2. 测试结果受数据规模的影响很大

*我们需要一个不用具体的测试数据来测试，就可以粗略地估计算法的执行效率的方法*。

### 大O复杂度表示法 ###

	 int cal(int n) {
	   int sum = 0;
	   int i = 1;
	   for (; i <= n; ++i) {
	     sum = sum + i;
	   }
	   return sum;
	 }

假设每行代码执行的时间都一样，为 unit_time，第 2、3 行代码分别需要 1 个 unit_time 的执行时间，第 4、5 行都运行了 n 遍，所以需要 2n*unit_time 的执行时间，所以这段代码总的执行时间就是 (2n+2)*unit_time。*所有代码的执行时间 T(n) 与每行代码的执行次数成正比。*

	T(n) = O(f(n))

T(n) 它表示代码执行的时间；n 表示数据规模的大小；f(n) 表示每行代码执行的次数总和。

大 O 时间复杂度实际上并不具体表示代码真正的执行时间，而是表示代码执行时间随数据规模增长的变化趋势，所以，也叫作渐进时间复杂度（asymptotic time complexity），简称时间复杂度。

### 时间复杂度分析 ###

1. 只关注循环次数最多的一段代码:我们在分析一个算法、一段代码的时间复杂度的时候，也只关注循环执行次数最多的那一段代码就可以了
2. 加法法则：总复杂度等于量级最大的那段代码的复杂度
3. 乘法法则：嵌套代码的复杂度等于嵌套内外代码复杂度的乘积。

### 几种常见时间复杂度实例分析 ###

* 常量阶O(1)
* 对数阶O(log^n)
* 线性阶O(n)
* 线性对数阶O(nlog^n)
* 指数阶O(2^n)
* 阶乘阶O(n!)
* 平方阶O(n^2)、立方阶O(n^3)....K次方阶O(n^k)

*多项式量级*和*非多项式量级*

非多项式量级：O(2n) 和 O(n!)

#### 1. O(1) ####

	 int i = 8;
	 int j = 6;
	 int sum = i + j;

O(1) 只是常量级时间复杂度的一种表示方法，并不是指只执行了一行代码。

一般情况下，只要算法中不存在循环语句、递归语句，即使有成千上万行的代码，其时间复杂度也是Ο(1)。

#### 2. O(logn)、O(nlogn) ####

	 i=1;
	 while (i <= n)  {
	   i = i * 2;
	 }

*在采用大 O 标记复杂度的时候，可以忽略系数，即 O(Cf(n)) = O(f(n))*。所以，O(log2n) 就等于 O(log3n)。因此，在对数阶时间复杂度的表示方法里，我们忽略对数的“底”，统一表示为 O(logn)。

2^x=n，O(log~2n)

#### 3. O(m+n)、O(m*n) ####

代码的复杂度由*两个数据的规模*来决定

	int cal(int m, int n) {
	  int sum_1 = 0;
	  int i = 1;
	  for (; i < m; ++i) {
	    sum_1 = sum_1 + i;
	  }
	
	  int sum_2 = 0;
	  int j = 1;
	  for (; j < n; ++j) {
	    sum_2 = sum_2 + j;
	  }
	
	  return sum_1 + sum_2;
	}

从代码来看，m 和 n 是表示两个数据规模。我们无法事先评估 m 和 n 谁的量级大，所以我们在表示复杂度的时候，就不能简单地利用加法法则，省略掉其中一个。所以，上面代码的时间复杂度就是 O(m+n)。需要将加法规则改为：T1(m) + T2(n) = O(f(m) + g(n))。但是乘法法则继续有效：T1(m)*T2(n) = O(f(m) * f(n))

### 空间复杂度分析 ###

时间复杂度的全称是*渐进时间复杂度，表示算法的执行时间与数据规模之间的增长关系*。空间复杂度全称就是*渐进空间复杂度（asymptotic space complexity），表示算法的存储空间与数据规模之间的增长关系*。

	void print(int n) {
	  int i = 0;
	  int[] a = new int[n];
	  for (i; i <n; ++i) {
	    a[i] = i * i;
	  }
	
	  for (i = n-1; i >= 0; --i) {
	    print out a[i]
	  }
	}

* 第 2 行代码中，我们申请了一个空间存储变量 i，但是它是常量阶的，跟数据规模 n 没有关系，所以我们可以忽略。
* 第 3 行申请了一个大小为 n 的 int 类型数组

剩下的代码都没有占用更多的空间，所以整段代码的空间复杂度就是 O(n)。 

### 内容小结 ###

复杂度也叫渐进复杂度，包括时间复杂度和空间复杂度，用来分析算法执行效率与数据规模之间的增长关系。越高阶复杂度的算法，执行效率越低。常见的复杂度并不多，从低阶到高阶有：O(1)、O(log^n)、O(n)、O(nlog^n)、O(n^2)。

*复杂度分析并不难，关键在于多练*。

### 课后思考 ###

有人说，我们项目之前都会进行性能测试，再做代码的时间复杂度、空间复杂度分析，是不是多此一举呢？而且，每段代码都分析一下时间复杂度、空间复杂度，是不是很浪费时间呢？你怎么看待这个问题呢？

### 精选留言 ###

#### 1.  ####

我不认为是多此一举，渐进时间，空间复杂度分析为我们提供了一个很好的理论分析的方向，并且它是宿主平台无关的，能够让我们对我们的程序或算法有一个大致的认识，让我们知道，比如在最坏的情况下程序的执行效率如何，同时也为我们交流提供了一个不错的桥梁，我们可以说，算法1的时间复杂度是O(n)，算法2的时间复杂度是O(logN)，这样我们立刻就对不同的算法有了一个“效率”上的感性认识。

当然，渐进式时间，空间复杂度分析只是一个理论模型，只能提供给粗略的估计分析，我们不能直接断定就觉得O(logN)的算法一定优于O(n), 针对不同的宿主环境，不同的数据集，不同的数据量的大小，在实际应用上面可能真正的性能会不同，个人觉得，针对不同的实际情况，进而进行一定的性能基准测试是很有必要的，比如在统一一批手机上(同样的硬件，系统等等)进行横向基准测试，进而选择适合特定应用场景下的最有算法。

综上所述，渐进式时间，空间复杂度分析与性能基准测试并不冲突，而是相辅相成的，但是一个低阶的时间复杂度程序有极大的可能性会优于一个高阶的时间复杂度程序，所以在实际编程中，时刻关心理论时间，空间度模型是有助于产出效率高的程序的，同时，因为渐进式时间，空间复杂度分析只是提供一个粗略的分析模型，因此也不会浪费太多时间，重点在于在编程时，要具有这种复杂度分析的思维。

## 04 | 复杂度分析（下）：浅析最好、最坏、平均、均摊时间复杂度 ##

四个复杂度分析方面的知识点：最好情况时间复杂度（best case time complexity）、最坏情况时间复杂度（worst case time complexity）、平均情况时间复杂度（average case time complexity）、均摊时间复杂度（amortized time complexity）。

### 最好、最坏情况时间复杂度 ###

	// n表示数组array的长度
	int find(int[] array, int n, int x) {
	  int i = 0;
	  int pos = -1;
	  for (; i < n; ++i) {
	    if (array[i] == x) pos = i;
	  }
	  return pos;
	}

* 最好情况时间复杂度就是，在最理想的情况下，执行这段代码的时间复杂度。
* 最坏情况时间复杂度就是，在最糟糕的情况下，执行这段代码的时间复杂度。

### 平均情况时间复杂度 ###

要查找的变量 x 在数组中的位置，有 n+1 种情况：**在数组的 0～n-1 位置中**和**不在数组中**。我们把每种情况下，查找需要遍历的元素个数累加起来，然后再除以 n+1，就可以得到需要遍历的元素个数的平均值，即：(1+2+3+⋯+n+n)/(n+1)=n(n+3))/(2(n+1)

时间复杂度的大 O 标记法中，可以省略掉系数、低阶、常量，得到的平均时间复杂度就是 O(n)。

要查找的变量x在数组中的位置，有n+1种情况：在数组的0~n-1位置中和不在数组中。我们把每种情况下，查找需要遍历的元素个数累加起来，然后再除以n+1，就可以得到需要遍历的元素个数的平均值。

概率论中的*加权平均值*，也叫作*期望值*，所以平均时间复杂度的全称应该叫*加权平均时间复杂度*或者*期望时间复杂度*。

引入概率之后，前面那段代码的加权平均值为 (3n+1)/4。用大 O 表示法来表示，去掉系数和常量，这段代码的加权平均时间复杂度仍然是 O(n)。

### 均摊时间复杂度 ###

	 // array表示一个长度为n的数组
	 // 代码中的array.length就等于n
	 int[] array = new int[n];
	 int count = 0;
	 
	 void insert(int val) {
	    if (count == array.length) {
	       int sum = 0;
	       for (int i = 0; i < array.length; ++i) {
	          sum = sum + array[i];
	       }
	       array[0] = sum;
	       count = 1;
	    }
	
	    array[count] = val;
	    ++count;
	 }

1. find（）函数在极端情况下，复杂度才为O（1）；insert() 在大部分情况下，时间复杂度都为 O(1)。只有个别情况下，复杂度才比较高，为 O(n)。这是 insert()第一个区别于 find() 的地方。
2. 对于 insert() 函数来说，O(1) 时间复杂度的插入和 O(n) 时间复杂度的插入，出现的频率是非常有规律的，而且有一定的前后时序关系，一般都是一个 O(n) 插入之后，紧跟着 n-1 个 O(1) 的插入操作，循环往复。

针对这样一种特殊场景的复杂度分析，我们并不需要像之前讲平均复杂度分析方法那样，找出所有的输入情况及相应的发生概率，然后再计算加权平均值。

摊还分析法，通过摊还分析得到的时间复杂度我们起了一个名字，叫均摊时间复杂度。

均摊时间复杂度就是一种特殊的平均时间复杂度

### 内容小结 ###

复杂度分析方法分别有：最好情况时间复杂度、最坏情况时间复杂度、平均情况时间复杂度、均摊时间复杂度。

### 课后思考 ###

分析一下下面这个 add() 函数的时间复杂度

	// 全局变量，大小为10的数组array，长度len，下标i。
	int array[] = new int[10]; 
	int len = 10;
	int i = 0;
	
	// 往数组中添加一个元素
	void add(int element) {
	   if (i >= len) { // 数组空间不够了
	     // 重新申请一个2倍大小的数组空间
	     int new_array[] = new int[len*2];
	     // 把原来array数组中的数据依次copy到new_array
	     for (int j = 0; j < len; ++j) {
	       new_array[j] = array[j];
	     }
	     // new_array复制给array，array现在大小就是2倍len了
	     array = new_array;
	     len = 2 * len;
	   }
	   // 将element放到下标为i的位置，下标i加一
	   array[i] = element;
	   ++i;
	}

基础篇

## 05 | 数组：为什么很多编程语言中数组都从0开始编号？ ##

*为什么数组要从 0 开始编号，而不是从 1 开始呢？*

### 如何实现随机访问？ ###

数组（Array）是一种线性表数据结构。它用一组连续的内存空间，来存储一组具有相同类型的数据。

1.* 线性表（Linear List）*：线性表就是数据排成像一条线一样的结构。每个线性表上的数据最多只有前和后两个方向。其实除了数组，链表、队列、栈等也是线性表结构。*非线性表*，比如二叉树、堆、图等。在非线性表中，数据之间并不是简单的前后关系。
2. *连续的内存空间和相同类型的数据*。堪称“杀手锏”的特性：**“随机访问”**，但随之带来让数组的很多操作变得非常低效。

### 低效的“插入”和删除 ###

#### 插入操作 ####

* 如果插入数组末尾的数据，则最好情况时间复杂度为 O(1)；
* 如果插入开头的数据，则最坏情况时间复杂度为 O(n)；
* 平均情况时间复杂度也为 O(n)。

如果是有序数组，在某个位置插入一个新的元素，必需按照搬移k之后的数据进行操作；但如果是无序数组，直接将第 k 位的数据搬移到数组元素的最后，把新的元素直接放入第 k 个位置（避免出现大规模数据迁移）。

	假设数组 a[10]中存储了如下 5 个元素：a，b，c，d，e。我们现在需要将元素 x 插入到第 3 个位置。我们只需要将 c 放入到 a[5]，将 a[2]赋值为 x 即可。最后，数组中的元素如下： a，b，x，d，e，c。

![3f70b4ad9069ec568a2caaddc231b7dc.jpg](img/3f70b4ad9069ec568a2caaddc231b7dc.jpg)

	利用这种处理技巧，在特定场景下，在第 k 个位置插入一个元素的时间复杂度就会降为 O(1)。

#### 删除操作 ####

* 如果删除数组末尾的数据，则最好情况时间复杂度为 O(1)；
* 如果删除开头的数据，则最坏情况时间复杂度为 O(n)；
* 平均情况时间复杂度也为 O(n)。

在某些特殊的场景下，不一定非得追求数组中数据的连续性。如果我们将多次删除操作集中在一起执行，删除的效率是不是会提高很多呢？

	数组 a[10]中存储了 8 个元素：a，b，c，d，e，f，g，h。现在，我们要依次删除 a，b，c 三个元素。

![b69b8c5dbf6248649ddab7d3e7cfd7e5.jpg](img/b69b8c5dbf6248649ddab7d3e7cfd7e5.jpg)

	为了避免d，e，f，g，h这几个数据会被搬移三次，先记录下已经删除的数据。每次的删除操作并不是真正地搬移数据，只是记录数据已经被删除。。当数组没有更多空间存储数据时，我们再触发执行一次真正的删除操作。（JVM标记清除垃圾回收算法的核心思想）

很多时候我们并不是要去死记硬背某个数据结构或者算法，而是要学习它背后的思想和处理技巧，这些东西才是最有价值的。

### 警惕数组的访问越界问题 ###

	int main(int argc, char* argv[]){
	    int i = 0;
	    int arr[3] = {0};
	    for(; i<=3; i++){
	        arr[i] = 0;
	        printf("hello world\n");
	    }
	    return 0;
	}

### 容器能否完全替代数组？ ###

容器的优点：

1. 封装细节
2. 支持动态扩容

虽然容器支持动态扩容，但涉及内存申请和数据搬移（比较耗时）。以，如果事先能确定需要存储的数据大小，最好在创建 ArrayList 的时候事先指定数据大小。

1. Java ArrayList 无法存储基本类型，比如 int、long，需要封装为 Integer、Long 类，而 Autoboxing、Unboxing 则有一定的性能消耗，所以如果特别关注性能，或者希望使用基本类型，就可以选用数组。
2. 如果数据大小事先已知，并且对数据的操作非常简单，用不到 ArrayList 提供的大部分方法，也可以直接使用数组。
3. 还有一个是我个人的喜好，当要表示多维数组时，用数组往往会更加直观。比如 `Object[][] array`；而用容器的话则需要这样定义：`ArrayList<ArrayList<object>> array。`

### 解答开题 ###

为什么大多数编程语言中，数组要从 0 开始编号，而不是从 1 开始呢？

1. 从数组存储的内存模型上来看，“下标”最确切的定义应该是“偏移（offset）”如果用 a 来表示数组的首地址，a[0]就是偏移为 0 的位置，也就是首地址，a[k]就表示偏移 k 个 type_size 的位置，所以计算 a[k]的内存地址只需要用这个公式：`a[k]_address = base_address + k * type_size`。但是，如果数组从 1 开始计数，那我们计算数组元素 a[k]的内存地址就会变为：`a[k]_address = base_address + (k-1)*type_size`（对CPU来说，多一个减法指令）
2. 历史原因

### 内容小结 ###

数组：最基础、最简单的数据结构；数组用一块连续的内存空间，来存储相同类型的一组数据，最大的特点就是支持随机访问，但插入、删除操作也因此变得比较低效，平均情况时间复杂度为 O(n)。

1. 在平时的业务开发中，我们可以直接使用编程语言提供的容器类
2. 如果是特别底层的开发，直接使用数组可能会更合适

### 课后思考 ###

1. 前面我基于数组的原理引出 JVM 的标记清除垃圾回收算法的核心理念。我不知道你是否使用 Java 语言，理解 JVM，如果你熟悉，可以在评论区回顾下你理解的标记清除垃圾回收算法。
2. 前面我们讲到一维数组的内存寻址公式，那你可以思考一下，类比一下，二维数组的内存寻址公式是怎样的呢？

## 06 | 链表（上）：如何实现LRU缓存淘汰算法？ ##

如何用链表来实现 LRU 缓存淘汰策略呢？

### 五花八门的链表结构 ###

#### 底层的存储结构 ####

* 数组：需要**一块连续的内存空间**
* 链表：通过“指针”将一组**零散的内存块**串联起来使用，其中，我们把内存块称为链表的“结点”。为了将所有的结点串起来，每个链表的结点除了存储数据之外，还需要记录链上的下一个结点的地址（后继指针next）。

两个特殊结点：

* 头结点：第一个结点，记录链表的基地址
* 尾结点：最后一个结点，不是指向下一个结点，而是指向一个空地址 NULL

优点和缺点

* 链表的存储空间本身不是连续的，所以插入和删除一个数据是非常快速的。
* 链表要想随机访问第 k 个元素，就没有数组那么高效了。因为链表中的数据并非连续存储的，（根据首地址和下标，通过寻址公式就能直接计算出对应的内存地址——数组的优点），而是需要根据指针一个结点一个结点地依次遍历，直到找到相应的结点。

#### 循环链表 ####

循环链表是一种特殊的单链表

优点：从链尾到链头比较方便：当要处理的数据具有环型结构特点时，就特别适合采用循环链表

#### 双向链表 ####

双向链表，顾名思义，它支持两个方向，每个结点不止有一个后继指针 next 指向后面的结点，还有一个前驱指针 prev 指向前面的结点。

从链表中删除一个数据：

* 删除结点中“值等于某个给定值”的结点
* 删除给定指针指向的结点：

**删除结点中“值等于某个给定值”的结点**

需要一个结点一个结点遍历，尽管单纯的删除操作时间复杂度是 O(1)，但遍历查找的时间是主要的耗时点，对应的时间复杂度为 O(n)。根据时间复杂度分析中的加法法则，删除值等于给定值的结点对应的链表操作的总时间复杂度为 O(n)。

**删除给定指针指向的结点**

因为双向链表中的结点已经保存了前驱结点的指针，不需要像单链表那样遍历。

Java的实现，LinkedHashMap


**空间换时间**的设计思想，当内存空间充足的时候，如果我们更加追求代码的执行速度，我们就可以选择空间复杂度相对较高、但时间复杂度相对很低的算法或者数据结构。相反，如果内存比较紧缺，比如代码跑在手机或者单片机上，这个时候，就要反过来用时间换空间的设计思路。

扩展：双向循环链表

### 链表 VS 数组性能大比拼 ###

* 数组：简单易用，在实现上使用的是连续的内存空间，可以借助 CPU 的缓存机制，预读数组中的数据，所以访问效率更高。
* 链表：链表在内存中并不是连续存储，所以对 CPU 缓存不友好，没办法有效预读。
* 数组：大小固定，如果声明的数组过大，系统可能没有足够的连续内存空间分配给它，导致“内存不足（out of memory）”。如果声明的数组过小，则可能出现不够用的情况。这时只能再申请一个更大的内存空间，把原数组拷贝进去，非常费时。
* 链表：链表本身没有大小的限制，天然地支持动态扩容

### 解答开篇 ###



## 07 | 链表（下）：如何轻松写出正确的链表代码？ ##

### 技巧一：理解指针或引用的含义 ###

*将某个变量赋值给指针，实际上就是将这个变量的地址赋值给指针，或者反过来说，指针中存储了这个变量的内存地址，指向了这个变量，通过指针就能找到这个变量。*

### 技巧二：警惕指针丢失和内存泄漏 ###

* 插入结点时，一定要注意操作的顺序
* 删除链表结点时，也一定要记得手动释放内存空间

### 技巧三：利用哨兵简化实现难度 ###

head 指针都会一直指向这个哨兵结点。我们也把这种有哨兵结点的链表叫带头链表。相反，没有哨兵结点的链表就叫作不带头链表。

### 技巧四：重点留意边界条件处理 ###

要实现没有 Bug 的链表代码，一定要在编写的过程中以及编写完成之后，检查边界条件是否考虑全面，以及代码在边界条件下是否能正确运行。

检查链表代码是否正确的边界条件有这样几个：

* 如果链表为空时，代码是否能正常工作？
* 如果链表只包含一个结点时，代码是否能正常工作？
* 如果链表只包含两个结点时，代码是否能正常工作？
* 代码逻辑在处理头结点和尾结点的时候，是否能正常工作？

不光光是写链表代码，你在写任何代码时，也千万不要只是实现业务正常情况下的功能就好了，一定要多想想，你的代码在运行的时候，可能会遇到哪些边界情况或者异常情况。遇到了应该如何应对，这样写出来的代码才够健壮！

### 技巧五：举例画图，辅助思考 ###

“举例法”和“画图法”

### 技巧六：多写多练，没有捷径 ###

* 单链表反转
* 链表中环的检测
* 两个有序的链表合并
* 删除链表倒数第 n 个结点
* 求链表的中间结点

### 内容小结 ###

* 理解指针或引用的含义
* 警惕指针丢失和内存泄漏
* 利用哨兵简化实现难度
* 重点留意边界条件处理
* 以及举例画图、辅助思考
* 多写多练。

写链表代码是最考验逻辑思维能力的。

### 课后思考 ###

今天我们讲到用哨兵来简化编码实现，你是否还能够想到其他场景，利用哨兵可以大大地简化编码难度？

## 08 | 栈：如何实现浏览器的前进和后退功能？ ##

### 如何理解“栈”？ ###

后进者先出，先进者后出，这就是典型的“栈”结构。

* 操作特性：栈是一种“操作受限”的线性表
* 功能：数组和链表都可以替代栈，但操作上灵活自由，使用时比较不可控

当某个数据集合只涉及在一端插入和删除数据，并且满足后进先出、先进后出的特性，我们就应该首选“栈”这种数据结构。

### 如何实现一个“栈”？ ###

* 顺序栈
* 链式栈

不管是顺序栈还是链式栈，我们存储数据只需要一个大小为 n 的数组就够了。在入栈和出栈过程中，只需要一两个临时变量存储空间，所以空间复杂度是 O(1)。

注意，这里存储数据需要一个大小为 n 的数组，并不是说空间复杂度就是 O(n)。

### 支持动态扩容的顺序栈 ###

对于出栈操作来说，我们不会涉及内存的重新申请和数据的搬移，所以出栈的时间复杂度仍然是 O(1)。

对于入栈操作来说，情况就不一样了。当栈中有空闲空间时，入栈操作的时间复杂度为 O(1)。但当空间不够时，就需要重新申请内存和数据搬移，所以时间复杂度就变成了 O(n)。

* 栈空间不够时，我们重新申请一个是原来大小两倍的数组；
* 为了简化分析，假设只有入栈操作没有出栈操作；
* 定义不涉及内存搬移的入栈操作为 simple-push 操作，时间复杂度为 O(1)。

这 K 次入栈操作，总共涉及了 K 个数据的搬移，以及 K 次 simple-push 操作。将 K 个数据搬移均摊到 K 次入栈操作，那每个入栈操作只需要一个数据搬移和一个 simple-push 操作。以此类推，入栈操作的均摊时间复杂度就为 O(1)。

均摊时间复杂度一般都等于最好情况时间复杂度。入栈操作的时间复杂度 O 都是 O(1)，只有在个别时刻才会退化为 O(n)，所以把耗时多的入栈操作的时间均摊到其他入栈操作上，平均情况下的耗时就接近 O(1)。

### 栈在函数调用中的应用 ###

函数调用栈

操作系统给每个线程分配了一块独立的内存空间，这块内存被组织成“栈”这种结构, 用来存储函数调用时的临时变量。每进入一个函数，就会将临时变量作为一个栈帧入栈，当被调用函数执行完成，返回之后，将这个函数对应的栈帧出栈。

### 栈在表达式求值中的应用 ###

表达式求值。

1. 实际上，编译器就是通过两个栈来实现的。其中一个保存操作数的栈，另一个是保存运算符的栈。我们从左向右遍历表达式，当遇到数字，我们就直接压入操作数栈；当遇到运算符，就与运算符栈的栈顶元素进行比较。
2. 如果比运算符栈顶元素的优先级高，就将当前运算符压入栈；如果比运算符栈顶元素的优先级低或者相同，从运算符栈中取栈顶运算符，从操作数栈的栈顶取 2 个操作数，然后进行计算，再把计算完的结果压入操作数栈，继续比较。

### 栈在括号匹配中的应用 ###

借助栈来检查表达式中的括号是否匹配。

* 用栈来保存未匹配的左括号，从左到右依次扫描字符串。当扫描到左括号时，则将其压入栈中；当扫描到右括号时，从栈顶取出一个左括号。
* 如果能够匹配，比如“(”跟“)”匹配，“[”跟“]”匹配，“{”跟“}”匹配，则继续扫描剩下的字符串。
* 如果扫描的过程中，遇到不能配对的右括号，或者栈中没有数据，则说明为非法格式。

### 解答开题 ###

### 内容小结 ###

栈是一种操作受限的数据结构，只支持入栈和出栈操作。后进先出是它最大的特点。栈既可以通过数组实现，也可以通过链表来实现。不管基于数组还是链表，入栈、出栈的时间复杂度都为 O(1)。

一种支持动态扩容的顺序栈，需要重点掌握它的均摊时间复杂度分析方法。

### 课后思考 ###

1. 我们在讲栈的应用时，讲到用函数调用栈来保存临时变量，为什么函数调用要用“栈”来保存临时变量呢？用其他数据结构不行吗？
2. 我们都知道，JVM 内存管理中有个“
3. 栈”的概念。栈内存用来存储局部变量和方法调用，堆内存用来存储 Java 中的对象。那 JVM 里面的“栈”跟我们这里说的“栈”是不是一回事呢？如果不是，那它为什么又叫作“栈”呢？

答：

1. 栈更符合计算机逻辑，跟~~命名空间~~作用域有关，~~越早入栈的变量，范围越大，越迟出栈~~栈的特性正好跟作用域的特性接近
2. ~~不是，只是JVM的栈，应该更类似数组。相当于值类型，模仿计算机的栈。~~

### 精选留言 ###

#### 1 ####

为什么函数调用要用“栈”来保存临时变量呢？用其他数据结构不行吗？

其实，我们不一定非要用栈来保存临时变量，只不过如果这个函数调用符合后进先出的特性，用栈这种数据结构来实现，是最顺理成章的选择。

从调用函数进入被调用函数，对于数据来说，变化的是什么呢？是作用域。所以根本上，只要能保证每进入一个新的函数，都是一个新的作用域就可以。而要实现这个，用栈就非常方便。在进入被调用函数的时候，分配一段栈空间给这个函数的变量，在函数结束的时候，将栈顶复位，正好回到调用函数的作用域内。

#### 2 ####



## 09 | 队列：队列在线程池等有限资源池中的应用 ##

当我们向固定大小的线程池中请求一个线程时，如果线程池中没有空闲资源了，这个时候线程池如何处理这个请求？是拒绝请求还是排队请求？各种处理策略又是怎么实现的呢？

### 如何理解“队列”？ ###

先进者先出，这就是典型的“队列”

* 栈只支持两个基本操作：入栈 push()和出栈 pop()
* 队列只支持两个基本操作：入队 enqueue()和出队 dequeue()

队列和栈一样，也是一种操作受限的线性表数据结构

高性能队列 Disruptor、Linux 环形缓存，都用到了循环并发队列；
Java concurrent 并发包利用 ArrayBlockingQueue 来实现公平锁等

### 顺序队列和链式队列 ###

* 对于栈来说，只需要一个栈顶指针就可以了
* 队列需要两个指针：一个是head指针，指向队头；一个是tail指针，指向队尾。

### 循环队列 ###

确定好队空和队满的判定条件。

* 队列为空的判断条件仍然是 head == tail
* 当队满时，(tail+1)%n=head

### 阻塞队列和并发队列 ###

阻塞队列其实就是在队列基础上增加了阻塞操作。你应该已经发现了，上述的定义就是一个“生产者 - 消费者模型”！是的，我们可以使用阻塞队列，轻松实现一个“生产者 - 消费者模型”！

线程安全的队列我们叫作并发队列。最简单直接的实现方式是直接在 enqueue()、dequeue() 方法上加锁，但是锁粒度大并发度会比较低，同一时刻仅允许一个存或者取操作。实际上，基于数组的循环队列，利用 CAS 原子操作，可以实现非常高效的并发队列。这也是循环队列比链式队列应用更加广泛的原因。在实战篇讲 Disruptor 的时候，我会再详细讲并发队列的应用。

### 解答开篇 ###

实际上，对于大部分资源有限的场景，当没有空闲资源时，基本上都可以通过“队列”这种数据结构来实现请求排队。

### 内容小结 ###

队列最大的特点就是先进先出，主要的两个操作是入队和出队。跟栈一样，它既可以用数组来实现，也可以用链表来实现。用数组实现的叫顺序队列，用链表实现的叫链式队列。

循环队列是我们这节的重点。要想写出没有 bug 的循环队列实现代码，关键要确定好队空和队满的判定条件，具体的代码你要能写出来。

还讲了几种高级的队列结构，阻塞队列、并发队列，底层都还是队列这种数据结构，只不过在之上附加了很多其他功能。阻塞队列就是入队、出队操作可以阻塞，并发队列就是队列的操作多线程安全。

### 课后思考 ###

1. 除了线程池这种池结构会用到队列排队请求，你还知道有哪些类似的池结构或者场景中会用到队列的排队请求呢？
2. 今天讲到并发队列，关于如何实现无锁并发队列，网上有非常多的讨论。对这个问题，你怎么看呢？

## 10 |递归：如何用三行代码找到“最终推荐人” ##

给定一个用户 ID，如何查找这个用户的“最终推荐人”

### 如何理解“递归”？ ###

最难理解的点

1. 动态规划
2. 递归

递归是一种应用非常广泛的算法（或者编程技巧）。很多数据结构和算法的编码实现都要用到递归，比如 DFS 深度优先搜索、前中后序二叉树遍历等等。

	f(n)=f(n-1)+1 其中，f(1)=1

### 递归需要满足的三个条件 ###

1. 一个问题的解可以分解为几个子问题的解

2. 这个问题与分解之后的子问题，除了数据规模之外，求解思路完全一样

3. 存在递归终止条件

### 如何编写递归代码？ ###

写出递推公式，找到终止条件

写递归代码的关键就是找到如何将大问题分解为小问题的规律，并且基于此写出递推公式，然后再推敲终止条件，最后将递推公式和终止条件翻译成代码。

如果一个问题 A 可以分解为若干子问题 B、C、D，你可以假设子问题 B、C、D 已经解决，在此基础上思考如何解决问题 A。而且，你只需要思考问题 A 与子问题 B、C、D 两层之间的关系即可，不需要一层一层往下思考子问题与子子问题，子子问题与子子子问题之间的关系。屏蔽掉递归细节，这样子理解起来就简单多了。

编写递归代码的关键是，只要遇到递归，我们就把它抽象成一个递推公式，不用想一层层的调用关系，不要试图用人脑去分解递归的每个步骤。

### 递归代码要警惕堆栈溢出 ###

函数调用会使用栈来保存临时变量。每调用一个函数，都会将临时变量封装为栈帧压入内存栈，等函数执行完成返回时，才出栈。系统栈或者虚拟机栈空间一般都不大。如果递归求解的数据规模很大，调用层次很深，一直压入栈，就会有堆栈溢出的风险。

如何避免出现堆栈溢出

1. 通过在代码中限制递归调用的最大深度的方式来解决这个问题

### 递归代码要警惕重复计算 ###

为了避免重复计算，我们可以通过一个数据结构（比如散列表）来保存已经求解过的 f(k)。当递归调用到 f(k) 时，先看下是否已经求解过了。如果是，则直接从散列表中取值返回。

在时间效率上，递归代码里多了很多函数调用，当这些函数调用的数量较大时，就会积聚成一个可观的时间成本。在空间复杂度上，因为递归调用一次就会在内存栈中保存一次现场数据，所以在分析递归代码空间复杂度时，需要额外考虑这部分的开销，比如我们前面讲到的电影院递归代码，空间复杂度并不是 O(1)，而是 O(n)。

### 怎么将递归代码改写为非递归代码？ ###

递归有利有弊：

* 利是递归代码的表达力很强，写起来非常简洁；
* 弊就是空间复杂度高、有堆栈溢出的风险、存在重复计算、过多的函数调用会耗时较多等问题。

因为递归本身就是借助栈来实现的，只不过我们使用的栈是系统或者虚拟机本身提供的，我们没有感知罢了。如果我们自己在内存堆上实现栈，手动模拟入栈、出栈过程，这样任何递归代码都可以改写成看上去不是递归代码的样子。

即：*所有的递归代码都可以改为这种迭代循环的非递归写法*

### 解答开篇 ###

1. 如果递归很深，可能会有堆栈溢出的问题。
2. 如果数据库里存在脏数据，我们还需要处理由此产生的无限递归问题。比如 demo 环境下数据库中，测试工程师为了方便测试，会人为地插入一些数据，就会出现脏数据。如果 A 的推荐人是 B，B 的推荐人是 C，C 的推荐人是 A，这样就会发生死循环。

第一个问题，我前面已经解答过了，可以用限制递归深度来解决。第二个问题，也可以用限制递归深度来解决。不过，还有一个更高级的处理方法，就是自动检测 A-B-C-A 这种“环”的存在。

### 内容小结 ###

递归是一种非常高效、简洁的编码技巧。只要是满足“三个条件”的问题就可以通过递归代码来解决。

1. 写出递推公式
2. 找出终止条件
3. 然后再翻译成递归代码。

递归代码虽然简洁高效，但是，递归代码也有很多弊端。比如，堆栈溢出、重复计算、函数调用耗时多、空间复杂度高等，所以，在编写递归代码的时候，一定要控制好这些副作用。

### 课后思考 ###

我们平时调试代码喜欢使用 IDE 的单步跟踪功能，像规模比较大、递归层次很深的递归代码，几乎无法使用这种调试方式。对于递归代码，你有什么好的调试方法呢？

## 11 | 排序（上）：为什么插入排序比冒泡排序更受欢迎？ ##

最常用的：冒泡排序、插入排序、选择排序、归并排序、快速排序、计数排序、基数排序、桶排序

插入排序和冒泡排序的时间复杂度相同，都是 O(n2)，在实际的软件开发里，为什么我们更倾向于使用插入排序算法而不是冒泡排序算法呢？

### 如何分析一个“排序算法”？ ###

#### 排序算法的执行效率 ####

**1. 最好情况、最坏情况、平均情况时间复杂度**

在分析排序算法的时间复杂度时，要分别给出最好情况、最坏情况、平均情况下的时间复杂度。除此之外，你还要说出最好、最坏时间复杂度对应的要排序的原始数据是什么样的。

* 有些排序算法会区分，为了好对比，所以我们最好都做一下区分。
* 对于要排序的数据，有的接近有序，有的完全无序。有序度不同的数据，对于排序的执行时间肯定是有影响的，

**2. 时间复杂度的系数、常数 、低阶**

时间复杂度反应的是数据规模 n 很大的时候的一个增长趋势，所以它表示的时候会忽略系数、常数、低阶。

**3. 比较次数和交换（或移动）次数**

如果我们在分析排序算法的执行效率的时候，应该把比较次数和交换（或移动）次数也考虑进去。

### 排序算法的内存消耗 ###

原地排序（Sorted in place）。原地排序算法，就是特指空间复杂度是 O(1) 的排序算法。

### 排序算法的稳定性 ###

仅仅用执行效率和内存消耗来衡量排序算法的好坏是不够的。针对排序算法，我们还有一个重要的度量指标，稳定性。如果待排序的序列中存在值相等的元素，经过排序之后，相等元素之间原有的先后顺序不变。

稳定排序算法可以保持金额相同的两个对象，在排序之后的前后顺序不变。

### 冒泡算法 ###

**第一，冒泡排序是原地排序算法吗？**

空间复杂度为 O(1)，是一个原地排序算法。

**第二，冒泡排序是稳定的排序算法吗？**

稳定的排序算法

**第三，冒泡排序的时间复杂度是多少**

* 最好情况时间复杂度是 O(n)
* 最坏情况时间复杂度是 O(n^2)

平均时间复杂度就是加权平均期望时间复杂度，如果用概率论方法定量分析平均时间复杂度，涉及的数学推理和计算就会很复杂。

“有序度”和“逆序度”

**有序度**是数组中具有有序关系的元素对的个数。有序元素对用数学表达式表示就是这样：

	有序元素对：a[i] <= a[j], 如果i < j。

逆序度的定义正好跟有序度相反（默认从小到大为有序）
	
	逆序元素对：a[i] > a[j], 如果i < j。

**逆序度 = 满有序度 - 有序度。**

冒泡排序包含两个操作原子，比较和交换。每交换一次，有序度就加 1。不管算法怎么改进，交换次数总是确定的，即为**逆序度，也就是n*(n-1)/2–初始有序度**。

### 插入排序（Insertion Sort） ###

这是一个动态排序的过程，即动态地往有序集合中添加数据，我们可以通过这种方法保持集合中的数据一直有序。

已排序区间和未排序区间，初始已排序区间只有一个元素，就是数组的第一个元素。插入算法的核心思想是取未排序区间中的元素，在已排序区间中找到合适的插入位置将其插入，并保证已排序区间数据一直有序。重复这个过程，直到未排序区间中元素为空，算法结束。

**第一，插入排序是原地排序算法吗？**

原地排序算法

**第二，插入排序是稳定的排序算法吗？**

稳定的排序算法

**第三，插入排序的时间复杂度是多少？**

* 最好是时间复杂度为 O(n)，**从尾到头遍历已经有序的数据**
* 

### 选择排序（Selection Sort） ###

选择排序算法的实现思路有点类似插入排序，也分已排序区间和未排序区间。但是选择排序每次会从未排序区间中找到最小的元素，将其放到已排序区间的末尾。

1. 选择排序空间复杂度为 O(1)，原地排序算法
2. 选择排序的最好情况时间复杂度、最坏情况和平均情况时间复杂度都为 O(n2)。
3. 选择排序是一种不稳定的排序算法

### 解答开篇 ###

* 冒泡排序不管怎么优化，元素交换的次数是一个固定值，是原始数据的逆序度。
* 插入排序是同样的，不管怎么优化，元素移动的次数也等于原始数据的逆序度。

冒泡排序的数据交换要比插入排序的数据移动要复杂

所以，虽然冒泡排序和插入排序在时间复杂度上是一样的，都是 O(n2)，但是如果我们希望把性能优化做到极致，那肯定首选插入排序。

### 内容小结 ###

![348604caaf0a1b1d7fee0512822f0e50.jpg](/img/348604caaf0a1b1d7fee0512822f0e50.jpg)

这三种排序算法，实现代码都非常简单，对于小规模数据的排序，用起来非常高效。但是在大规模数据排序的时候，这个时间复杂度还是稍微有点高，所以更倾向于用时间复杂度为 O(nlogn) 的排序算法。

### 课后思考 ###

我们讲过，特定算法是依赖特定的数据结构的。我们今天讲的几种排序算法，都是基于数组实现的。如果数据存储在链表中，这三种排序算法还能工作吗？如果能，那相应的时间、空间复杂度又是多少呢？

## 12 | 排序（下）：如何用快排思想在O(n)内查找第K大元素？ ##

两种时间复杂度为 O(nlogn) 的排序算法，**归并排序和快速排序**。

如何在 O(n) 的时间复杂度内查找一个无序数组中的第 K 大元素？

### 归并排序的原理 ###

**归并排序**（Merge Sort）

归并排序的核心思想还是蛮简单的。如果要排序一个数组，我们先把数组从中间分成前后两部分，然后对前后两部分分别排序，再将排好序的两部分合并在一起，这样整个数组就都有序了。

归并排序使用的是分治思想。

分治是一种解决问题的处理思想，递归是一种编程技巧。归并排序用的是分治思想，可以用递归来实现。**我们现在就来看看如何用递归代码来实现归并排序**。

	递推公式：
	merge_sort(p…r) = merge(merge_sort(p…q), merge_sort(q+1…r))
	
	终止条件：
	p >= r 不用再继续分解

merge_sort(p…r) 表示，给下标从 p 到 r 之间的数组排序。我们将这个排序问题转化为了两个子问题，merge_sort(p…q) 和 merge_sort(q+1…r)，其中下标 q 等于 p 和 r 的中间位置，也就是 (p+r)/2。当下标从 p 到 q 和从 q+1 到 r 这两个子数组都排好序之后，我们再将两个有序的子数组合并在一起，这样下标从 p 到 r 之间的数据就也排好序了。

### 归并排序的性能分析 ###

#### 第一，归并排序是稳定的排序算法吗？ ####

归并排序稳不稳定关键要看 merge() 函数，也就是两个有序子数组合并成一个有序数组的那部分代码。

归并排序是一个稳定的排序算法。

#### 第二，归并排序的时间复杂度是多少？ ####

不仅递归求解的问题可以写成递推公式，递归代码的时间复杂度也可以写成递推公式。

T(n) 就等于 O(nlogn)。所以归并排序的时间复杂度是 O(nlogn)。从我们的原理分析和伪代码可以看出，归并排序的执行效率与要排序的原始数组的有序程度无关，所以其时间复杂度是非常稳定的，不管是最好情况、最坏情况，还是平均情况，时间复杂度都是 O(nlogn)。

#### 第三，归并排序的空间复杂度是多少？ ####

归并排序的时间复杂度任何情况下都是 O(nlogn)。归并排序并没有像快排那样，应用广泛，这是为什么呢？因为它有一个致命的“弱点”，那就是归并排序不是原地排序算法。

这是因为归并排序的合并函数，在合并两个有序数组为一个有序数组时，需要借助额外的存储空间。

尽管每次合并操作都需要申请额外的内存空间，但在合并完成之后，临时开辟的内存空间就被释放掉了。在任意时刻，CPU 只会有一个函数在执行，也就只会有一个临时的内存空间在使用。临时内存空间最大也不会超过 n 个数据的大小，所以空间复杂度是 O(n)。

### 快速排序的原理 ###

快速排序算法（Quicksort），快排利用的也是分治思想。

快排的思想是这样的：如果要排序数组中下标从 p 到 r 之间的一组数据，我们选择 p 到 r 之间的任意一个数据作为 pivot（分区点）。

归并排序的处理过程是由下到上的，先处理子问题，然后再合并。而快排正好相反，它的处理过程是由上到下的，先分区，然后再处理子问题。

### 快速排序的性能分析 ###

快排是一种原地、不稳定的排序算法。

快排也是用递归来实现的。如果每次分区操作，都能正好把数组分成大小接近相等的两个小区间，那快排的时间复杂度递推求解公式跟归并是相同的。所以，快排的时间复杂度也是 O(nlogn)。

T(n) 在大部分情况下的时间复杂度都可以做到 O(nlogn)，只有在极端情况下，才会退化到 O(n2)。

### 解答开篇 ###

快排核心思想就是**分治**和**分区**

### 内容小结 ###

归并排序和快速排序是两种稍微复杂的排序算法，它们用的都是分治的思想，代码都通过递归来实现，过程非常相似。理解归并排序的重点是理解递推公式和 merge() 合并函数。同理，理解快排的重点也是理解递推公式，还有 partition() 分区函数。

归并排序和快速排序是两种稍微复杂的排序算法，它们用的都是分治的思想，代码都通过递归来实现，过程非常相似。理解归并排序的重点是理解递推公式和 merge() 合并函数。同理，理解快排的重点也是理解递推公式，还有 partition() 分区函数。

快速排序算法虽然最坏情况下的时间复杂度是 O(n2)，但是平均情况下时间复杂度都是 O(nlogn)。不仅如此，快速排序算法时间复杂度退化到 O(n2) 的概率非常小，我们可以通过合理地选择 pivot 来避免这种情况。

### 课后思考 ###

## 13 | 线性排序：如何根据年龄给100万用户数据排序？ ##

桶排序、计数排序、基数排序

* 时间复杂度是线性的，线性排序
* 不涉及元素之间的比较操作

如何根据年龄给 100 万用户排序？ 

### 桶排序 ###

会用到“桶”，核心思想是将要排序的数据分到几个有序的桶里，每个桶里的数据再单独进行排序。桶内排完序之后，再把每个桶里的数据按照顺序依次取出，组成的序列就是有序的了。

如果要排序的数据有 n 个，我们把它们均匀地划分到 m 个桶内，每个桶里就有 k=n/m 个元素。每个桶内部使用快速排序，时间复杂度为 O(k * logk)。m 个桶排序的时间复杂度就是 O(m * k * logk)，因为 k=n/m，所以整个桶排序的时间复杂度就是 O(n*log(n/m))。当桶的个数 m 接近数据个数 n 时，log(n/m) 就是一个非常小的常量，这个时候桶排序的时间复杂度接近 O(n)。

**桶排序看起来很优秀，那它是不是可以替代我们之前讲的排序算法呢？**

不能

1. 需要划分成m个桶
2. 数据在各个桶之间的分布是比较均匀的，在极端情况下，如果数据都被划分到一个桶里，那就退化为 O(nlogn) 的排序算法了。

*桶排序比较适合用在外部排序中。*

### 计数排序（Counting sort） ###

*计数排序其实是桶排序的一种特殊情况。*

**不过，为什么这个排序算法叫“计数”排序呢？“计数”的含义来自哪里呢？**

*计数排序只能用在数据范围不大的场景中，如果数据范围 k 比要排序的数据 n 大很多，就不适合用计数排序了。而且，计数排序只能给非负整数排序，如果要排序的数据是其他类型的，要将其在不改变相对大小的情况下，转化为非负整数。* 如果将所有的分数都先乘以 10转化成整数；如果要排序的数据中有负数，数据的范围是[-1000, 1000]，那我们就需要先对每个数据都加 1000，转化成非负整数。

### 基数排序（Radix sort） ###

这里按照每位来排序的排序算法要是稳定的。先按照最后一位来排序手机号码，然后，再按照倒数第二位重新排序，以此类推，最后按照第一位重新排序。经过 11 次排序之后，手机号码就都有序了。

我们可以把所有的单词补齐到相同长度，位数不够的可以在后面补“0”

基数排序对要排序的数据是有要求的，需要可以分割出独立的“位”来比较，而且位之间有递进的关系，如果 a 数据的高位比 b 数据大，那剩下的低位就不用比较了。除此之外，每一位的数据范围不能太大，要可以用线性排序算法来排序，否则，基数排序的时间复杂度就无法做到 O(n) 了。

### 解答开题 ###

### 内容小结 ###

 3 种排序算法，有桶排序、计数排序、基数排序

1. 线性时间复杂度
2. 对要排序的数据都有比较苛刻的要求，如果数据特征比较符合这些排序算法的要求，应用这些算法，会非常高效，线性时间复杂度可以达到 O(n)。
3. 桶排序和计数排序的排序思想是非常相似的，都是针对范围不大的数据，将数据划分成不同的桶来实现排序；基数排序要求数据可以划分成高低位，位之间有递进关系。比较两个数，我们只需要比较高位，高位相同的再比较低位。而且每一位的数据范围不能太大，因为基数排序算法需要借助桶排序或者计数排序来完成每一个位的排序工作。

### 课后思考 ###

## 14 | 排序优化 ##

如何实现一个通用的、高性能的排序函数？

### 如何选择合适的排序算法？ ###

### 如何优化快速排序？ ###

这种 O(n2) 时间复杂度出现的主要原因还是因为我们分区点选的不够合理。

被分区点分开的两个分区中，数据的数量差不多。

#### 1. 三数取中法 ####

从区间的首、尾、中间，分别取出一个数，然后对比大小，取这 3 个数的中间值作为分区点。

#### 2. 随机法 ####

随机法就是每次从要排序的区间中，随机选择一个元素作为分区点。这种方法并不能保证每次分区点都选的比较好，但是从概率的角度来看，也不大可能会出现每次分区点都选的很差的情况，所以平均情况下，这样选的分区点是比较好的。

### 举例分析排序函数 ###

qsort() 会优先使用归并排序来排序输入数据，因为归并排序的空间复杂度是 O(n)

但如果数据量太大，*要排序的数据量比较大的时候，qsort() 会改为用快速排序算法来排序*。



### 内容小结 ###

我们大部分排序函数都是采用 O(nlogn) 排序算法来实现，但是为了尽可能地提高性能，会做很多优化。

快排的优化策略：

1. 合理选择分区点
2. 避免递归太深
3. 例举了C语言qsort()的底层实现原理

### 课后思考 ###

我分析了 C 语言的中的 qsort() 的底层排序算法，你能否分析一下你所熟悉的语言中的排序函数都是用什么排序算法实现的呢？都有哪些优化技巧？

**希望你把思考的过程看得比标准答案更重要**

### 精选留言 ###



## 15 | 二分查找（上） ##

假设我们有 1000 万个整数数据，每个数据占 8 个字节，如何设计数据结构和算法，快速判断某个整数是否出现在这 1000 万数据中？ 

### 无处不在的二分查找 ###

二分查找针对的是一个有序的数据集合，查找思想有点类似分治思想。每次都通过跟区间的中间元素对比，将待查找的区间缩小为之前的一半，直到找到要查找的元素，或者区间被缩小为 0。

### O（logn）惊人的查找速度 ###

时间复杂度是O(logn)的算法，可能比常量级O(1)的算法还要高效。因为O(1)有可能表示的十一个非常大的常量值。

另外，指数时间复杂度的算法在大规模数据面前是无效的。

### 二分查找的递归与非递归实现 ###

最简单的情况就是有序数组中不存在重复元素

容易出错的3个地方

#### 1. 循环退出条件 ####

low<=high

#### 2. mid的取值 ####

`mid =（low+high/2`，low和high比较大的话，两者之和有可能会溢出。 可以改进为`low+(high-low)/2`甚至 `low+((high-low)>>1)`

#### 3. low和high的更新 ####

low=mid+1，high=mid-1。注意这里的 +1 和 -1，如果直接写成 low=mid 或者 high=mid，就可能会发生死循环。比如，当 high=3，low=3 时，如果 a[3]不等于 value，就会导致一直循环不退出。

实际上，二分查找除了用循环来实现，还可以用递归来实现

### 二分查找应用场景的局限性 ###

二分查找的时间复杂度是 O(logn)，查找数据的效率非常高。

#### 首先，二分查找依赖的是顺序表结构，简单点说就是数组 ####

二分查找无法依赖于链表，如果使用链表来查找时间复杂度会十分高。

二分查找只能用在数据是通过顺序表来存储的数据结构上。如果你的数据是通过其他数据结构存储的，则无法应用二分查找。

#### 其次，二分查找针对的是有序数据。 ####

1. 数据必须是有序的。如果数据没有序，我们需要先排序。
2. 如果数据集合有频繁的插入和删除操作，针对这种动态数据集合，维护有序的成本很高
3. 二分查找只能用在插入、删除操作不频繁，一次排序多次查找的场景中。

#### 再次，数据量太小不适合二分查找。 ####

* 要处理得数据量很小，没有必要使用二分查找，使用顺序遍历便可
* 但如果数据之间的比较操作非常耗时，不管数据量大小，都推荐使用二分查找。

#### 最后，数据量太大也不适合二分查找。 ####

二分查找的底层需要依赖数组这种数据结构，而数组为了支持随机访问的特性，要求内存空间连续，对内存的要求比较苛刻。比如，我们有 1GB 大小的数据，如果希望用数组来存储，那就需要 1GB 的连续内存空间。

### 解答开题 ###

### 内容小结 ###

针对*有序*数据的高效查找算法，二分查找，它的时间复杂度是* O(logn)*。

二分查找的核心思想理解起来非常简单，有点类似分治思想。即每次都通过跟区间中的中间元素对比，将待查找的区间缩小为一半，直到找到要查找的元素，或者区间被缩小为 0。但是二分查找的代码实现比较容易写错。你需要着重掌握它的三个容易出错的地方：

1. 循环退出条件
2. mid 的取值
3. low 和 high 的更新。

二分查找虽然性能比较优秀，但应用场景也比较有限。

1. 底层必须依赖数组，并且还要求数据是有序的。
2. 对于较小规模的数据查找，我们直接使用顺序遍历就可以了，二分查找的优势并不明显。
3. 二分查找更适合处理静态数据，也就是没有频繁的数据插入、删除操作。

### 课后思考 ###

## 16 | 二分查找（下）	 ##

假设我们有 12 万条这样的 IP 区间与归属地的对应关系，如何快速定位出一个 IP 地址的归属地呢？

4种常见的二分查找变形问题

* 查找第一个值等于给定值的元素
* 查找最后一个值等于给定值的元素
* 查找第一个大于等于给定值的元素
* 查找最后一个小于等于给定值的元素

### 变体一：查找第一个值等于给定值的元素 ###

很多人都觉得变形的二分查找很难写，主要原因是太追求第一种那样完美、简洁的写法。

### 变体二：查找最后一个值等于给定值的元素 ###

### 变体三：查找第一个大于等于给定值的元素 ###

### 变体四：查找最后一个小于等于给定值的元素 ###

### 解答开篇 ###

如何快速定位出一个 IP 地址的归属地？

1. 如果 IP 区间与归属地的对应关系不经常更新，我们可以先预处理这 12 万条数据，让其按照起始 IP 从小到大排序。(IP 地址可以转化为 32 位的整型数。所以，我们可以将起始地址，按照对应的整型值的大小关系，从小到大进行排序)
2. 转化：第四种变形问题“在有序数组中，查找最后一个小于等于某个给定值的元素”了
3. 先通过二分查找，找到最后一个起始 IP 小于等于这个 IP 的 IP 区间
	* 检查IP是否在这个IP区间内，如果在，取出对应的归属地显示
	* 如果不在，就返回未查找到

### 内容小结 ###

凡是用二分查找能解觉得，绝大部分倾向于用散列表或者二叉查找树。

求“值等于给定值”的二分查找确实不怎么会被用到，二分查找更适合用在“近似”查找问题，在这类问题上，二分查找的优势更加明显。比如今天讲的这几种变体问题，用其他数据结构，比如散列表、二叉树，就比较难实现了。

变体的二分查找算法写起来非常烧脑，很容易因为细节处理不好而产生 Bug，这些容易出错的细节有：*终止条件、区间上下界更新方法、返回值选择*。所以今天的内容你最好能用自己实现一遍，对锻炼编码能力、逻辑思维、写出 Bug free 代码，会很有帮助。

### 课后思考 ###

如果有序数组是一个循环有序数组，比如 4，5，6，1，2，3。针对这种情况，如何实现一个求“值等于给定值”的二分查找算法呢？

## 17 | 跳表：为什么Redis一定要用跳表来实现有序集合 ##


## 18 | 散列表（上）：Word文档中的单词拼写检查功能是如何实现的？ ##

### 散列思想 ###

散列表用的是数组支持按照下标随机访问数据的特性，所以散列表其实就是数组的一种扩展，由数组演化而来。可以说，如果没有数组，就没有散列表。

**键（key）**或者**关键字**，把参数编号转化为数组下标的映射方法就叫做**散列函数**，而散列函数计算得到的值就叫做**散列值**（或“Hash值” “哈希值”）

散列表用的就是数组支持按照下标随机访问的时候，时间复杂度是 O(1) 的特性。我们通过散列函数把元素的键值映射为下标，然后将数据存储在数组中对应下标的位置。当我们按照键值查询元素时，我们用同样的散列函数，将键值转化数组下标，从对应的数组下标的位置取数据。

该如何构造散列函数呢？我总结了三点散列函数设计的基本要求：

1. 散列函数计算得到的散列值是一个非负整数；
2. 如果 key1 = key2，那 hash(key1) == hash(key2)；
3. 如果 key1 ≠ key2，那 hash(key1) ≠ hash(key2)。在真实的情况下，要想找到一个不同的key对应的散列值都不一样的散列函数，几乎是不可能的。即使像MD5、SHA、CRC等哈希算法，也无法完全避免这种*散列冲突*。

### 散列冲突 ###

常用的散列冲突解决方法有两类，开放寻址法（open addressing）和链表法（chaining）。

#### 1. 开放寻址法 ####

开放寻址法的核心思想是，如果出现了散列冲突，我们就重新探测一个空闲位置，将其插入。

**线性探测**（Linear Probing）

**二次探测**（Quadratic probing）

线性探测每次探测的步长是 1，那它探测的下标序列就是 hash(key)+0，hash(key)+1，hash(key)+2，而二次探测探测的步长就变成了原来的“二次方”，也就是说，它探测的下标序列就是 hash(key)+0，hash(key)+1^2^，hash(key)+2^2^。

**双重散列**（Double hashing）

意思就是不仅要使用一个散列函数。我们使用一组散列函数 hash1(key)，hash2(key)，hash3(key)……我们先用第一个散列函数，如果计算得到的存储位置已经被占用，再用第二个散列函数，依次类推，直到找到空闲的存储位置。

装载因子（load factor：来表示空位的多少）的计算公式是：

	散列表的装载因子=填入表中的元素个数/散列表的长度

装载因子越大，说明空闲位置越少，冲突越多，散列表的性能会下降。

#### 2. 链表法 ####

链表法是一种更加常用的散列冲突解决办法，相比开放寻址法，它要简单很多。在散列表中，每个“桶（bucket）”或者“槽（slot）”会对应一条链表，所有散列值相同的元素我们都放到相同槽位对应的链表中。

对于散列比较均匀的散列函数来说，理论上讲，k=n/m，其中 n 表示散列中数据的个数，m 表示散列表中“槽”的个数。

### 解答开篇 ###

### 内容小结 ###

散列表两个核心问题：

1. 散列函数设计
2. 散列冲突解决
	* 开放寻址法
	* 链表法

### 课后思考 ###

## 19 | 散列表（中）：如何打造一个工业级水平的散列表 ##

如何设计一个可以应对各种异常情况的工业级散列表，来避免在散列冲突的情况下，散列表性能的急剧下降，并且能抵抗散列碰撞攻击？

### 如何设计散列函数？ ###

1. 散列函数的设计不能太复杂
2. 散列函数生成的值要尽可能随机并且均匀分布

#### 设计方法 ####

1. 数据分析法
2. ASCII码值“进位”相加，然后再跟散列表的大小求余、取模

### 装载因子过大了怎么办？ ###

装载因子越大，说明散列表中的元素越多，空闲位置越少，散列冲突的概率就越大。

插入一个数据，最好情况下，不需要扩容，最好时间复杂度是 O(1)。最坏情况下，散列表装载因子过高，启动扩容，我们需要重新申请内存空间，重新计算哈希位置，并且搬移数据，所以时间复杂度是 O(n)。用摊还分析法，均摊情况下，时间复杂度接近最好情况，就是 O(1)。

实际上，对于动态散列表，随着数据的删除，散列表中的数据会越来越少，空闲空间会越来越多。如果我们对空间消耗非常敏感，我们可以在装载因子小于某个值之后，启动动态缩容。当然，如果我们更加在意执行效率，能够容忍多消耗一点内存空间，那就可以不用费劲来缩容了。

装载因子阈值的设置要权衡时间、空间复杂度。如果内存空间不紧张，对执行效率要求很高，可以降低负载因子的阈值；相反，如果内存空间紧张，对执行效率要求又不高，可以增加负载因子的值，甚至可以大于 1。

### 如何避免低效地扩容？ ###

为了解决一次性扩容耗时过多的情况，我们可以将扩容操作穿插在插入操作的过程中，分批完成。当装载因子触达阈值之后，我们只申请新空间，但并不将老的数据搬移到新散列表中。

当有新数据要插入时，我们将新数据插入新散列表中，并且从老的散列表中拿出一个数据放入到新散列表。每次插入一个数据到散列表，我们都重复上面的过程。经过多次插入操作之后，老的散列表中的数据就一点一点全部搬移到新散列表中了。这样没有了集中的一次性数据搬移，插入操作就都变得很快了。

这期间的查询操作怎么来做呢？对于查询操作，为了兼容了新、老散列表中的数据，我们先从新散列表中查找，如果没有找到，再去老的散列表中查找。

通过这样均摊的方法，将一次性扩容的代价，均摊到多次插入操作中，就避免了一次性扩容耗时过多的情况。这种实现方式，任何情况下，插入一个数据的时间复杂度都是 O(1)。

### 如何选择冲突解决方法？ ###

比如，Java 中 LinkedHashMap 就采用了链表法解决冲突，ThreadLocalMap 是通过线性探测的开放寻址法来解决冲突。

#### 1. 开放寻址法 ####

开放寻址法不像链表法，需要拉很多链表。散列表中的数据都存储在数组中，可以有效地利用 CPU 缓存加快查询速度。而且，这种方法实现的散列表，序列化起来比较简单。链表法包含指针，序列化起来就没那么容易。

用开放寻址法解决冲突的散列表，删除数据的时候比较麻烦，需要特殊标记已经删除掉的数据。而且，在开放寻址法中，所有的数据都存储在一个数组中，比起链表法来说，冲突的代价更高。所以，使用开放寻址法解决冲突的散列表，装载因子的上限不能太大。这也导致这种方法比链表法更浪费内存空间。

*我总结一下，当数据量比较小、装载因子小的时候，适合采用开放寻址法。这也是 Java 中的ThreadLocalMap使用开放寻址法解决散列冲突的原因。*

#### 2. 链表法 ####

1. 表法对内存的利用率比开放寻址法要高。因为链表结点可以在需要的时候再创建，并不需要像开放寻址法那样事先申请好。实际上，这一点也是我们前面讲过的链表优于数组的地方。
2. 对于链表法来说，只要散列函数的值随机均匀，即便装载因子变成 10，也就是链表的长度变长了而已，虽然查找效率有所下降，但是比起顺序查找还是快很多。
3. 链表因为要存储指针，所以对于比较小的对象的存储，是比较消耗内存的，还有可能会让内存的消耗翻倍。而且，因为链表中的结点是零散分布在内存中的，不是连续的，所以对 CPU 缓存是不友好的，这方面对于执行效率也有一定的影响。

对链表法稍加改造，可以实现一个更加高效的散列表。那就是，我们将链表法中的链表改造为其他高效的动态数据结构，比如跳表、红黑树。这样，

*我总结一下，基于链表的散列冲突处理方法比较适合存储大对象、大数据量的散列表，而且，比起开放寻址法，它更加灵活，支持更多的优化策略，比如用红黑树代替链表。*

### 工业级散列表举例分析 ###

Java 中的 HashMap 这样一个工业级的散列表

#### 1.  初始大小 ####

HashMap 默认的初始大小是 16，当然这个默认值是可以设置的，如果事先知道大概的数据量有多大，可以通过修改默认初始大小，减少动态扩容的次数，这样会大大提高 HashMap 的性能。

#### 2.  装载因子和动态扩容 ####

最大装载因子默认是 0.75，当 HashMap 中元素个数超过 0.75*capacity（capacity 表示散列表的容量）的时候，就会启动扩容，每次扩容都会扩容为原来的两倍大小。

#### 3.  散列冲突解决方法 ####

HashMap 底层采用链表法来解决冲突。即使负载因子和散列函数设计得再合理，也免不了会出现拉链过长的情况，一旦出现拉链过长，则会严重影响 HashMap 的性能。

JDK1.8版本中，为了对 HashMap 做进一步优化，引入了红黑树。当链表长度太长（默认超过 8）时，链表就转换为红黑树。我们可以利用红黑树快速增删改查的特点，提高 HashMap 的性能。当红黑树结点个数少于 8 个的时候，又会将红黑树转化为链表。因为在数据量较小的情况下，红黑树要维护平衡，比起链表来，性能上的优势并不明显。

#### 4. 散列函数 ####

散列函数的设计并不复杂，追求的是简单高效、分布均匀。
	
	int hash(Object key) {
	    int h = key.hashCode()；
	    return (h ^ (h >>> 16)) & (capicity -1); //capicity表示散列表的大小
	}

其中，hashCode() 返回的是 Java 对象的 hash code。

### 解答开篇 ###

*何为一个工业级的散列表？工业级的散列表应该具有哪些特性？*

1. 支持快速的查询、插入、删除操作；
2. 内存占用合理，不能浪费过多的内存空间；
3. 性能稳定，极端情况下，散列表的性能也不会退化到无法接受的情况。

*如何实现这样一个散列表呢*

1. 设计一个合适的散列函数；
2. 定义装载因子阈值，并且设计动态扩容策略；
3. 选择合适的散列冲突解决方法。

### 内容小结 ###

如何设计一个工业级的散列表，以及如何应对各种异常情况，防止在极端情况下，散列表的性能退化过于严重。应对策略：

1. 如何设计散列函数
2. 如何根据装载因子动态扩容
3. 如何选择散列冲突解决方法。

关于散列函数的设计，我们要尽可能让散列后的值随机且均匀分布，

关于散列冲突解决方法的选择，我对比了开放寻址法和链表法两种方法的优劣和适应的场景。大部分情况下，链表法更加普适。而且，我们还可以通过将链表法中的链表改造成其他动态查找数据结构，比如红黑树，来避免散列表时间复杂度退化成 O(n)，抵御散列碰撞攻击。但是，对于小规模数据、装载因子不高的散列表，比较适合用开放寻址法。

对于动态散列表来说，不管我们如何设计散列函数，选择什么样的散列冲突解决方法。随着数据的不断增加，散列表总会出现装载因子过高的情况。这个时候，我们就需要启动动态扩容。

## 20 | 散列表（下）：为什么散列表和链表经常会一起使用？ ##

如何用链表来实现 LRU 缓存淘汰算法，但是链表实现的 LRU 缓存淘汰算法的时间复杂度是 O(n)

工业实现：

* Redis 的有序集合是使用跳表来实现的，跳表可以看作一种改进版的链表。Redis 有序集合不仅使用了跳表，还用到了散列表。
* Java的LinkedHashMap 这样一个常用的容器，也用到了散列表和链表两种数据结构。

### LRU缓存淘汰算法 ###

在链表那一节中，我提到，借助散列表，我们可以把 LRU 缓存淘汰算法的时间复杂度降低为 O(1)。现在，我们就来看看它是如何做到的。

1. 维护一个按照访问时间从大到小有序排列的链表结构。因为缓存大小有限，当缓存空间不够，需要淘汰一个数据的时候，我们就直接将链表头部的结点删除。
2. 当要缓存某个数据的时候，先在链表中查找这个数据。如果没有找到，则直接将数据放到链表的尾部；如果找到了，我们就把它移动到链表的尾部。因为查找数据需要遍历链表，所以单纯用链表实现的 LRU 缓存淘汰算法的时间复杂很高，是 O(n)。

一个缓存（cache）系统主要包含下面这几个操作：

1. 往缓存中添加一个数据；
2. 从缓存中删除一个数据；
3. 在缓存中查找一个数据。

这三个操作都要涉及“查找”操作，如果单纯地采用链表的话，时间复杂度只能是 O(n)。如果我们将散列表和链表两种数据结构组合使用，可以将这三个操作的时间复杂度都降低到 O(1)。

![eaefd5f4028cc7d4cfbb56b24ce8ae6e.jpg](img/eaefd5f4028cc7d4cfbb56b24ce8ae6e.jpg)

使用双向链表存储数据，链表中的每个结点处理存储数据（data）、前驱指针（prev）、后继指针（next）之外，还新增了一个特殊的字段 hnext。这个 hnext 有什么作用呢？

因为我们的散列表是通过链表法解决散列冲突的，所以每个结点会在两条链中。一个链是刚刚我们提到的*双向链表*，另一个链是散列表中的*拉链*。*前驱和后继指针是为了将结点串在双向链表中，hnext 指针是为了将结点串在散列表的拉链中。*如何做到时间复杂度是O（1）的？

#### 1. 如何查找一个数据 ####

散列表中查找数据的时间复杂度接近 O(1)，所以通过散列表，我们可以很快地在缓存中找到一个数据。当找到数据之后，我们还需要将它移动到双向链表的尾部。

#### 2. 如何删除一个数据 ####

我们需要找到数据所在的结点，然后将结点删除。借助散列表，我们可以在 O(1) 时间复杂度里找到要删除的结点。因为我们的链表是双向链表，双向链表可以通过前驱指针 O(1) 时间复杂度获取前驱结点，所以在双向链表中，删除结点只需要 O(1) 的时间复杂度。

#### 3. 如何添加一个数据 ####

添加数据到缓存稍微有点麻烦，我们需要先看这个数据是否已经在缓存中。如果已经在其中，需要将其移动到双向链表的尾部；如果不在其中，还要看缓存有没有满。如果满了，则将双向链表头部的结点删除，然后再将数据放到链表的尾部；如果没有满，就直接将数据放到链表的尾部。

这整个过程涉及的查找操作都可以通过散列表来完成。其他的操作，比如删除头结点、链表尾部插入数据等，都可以在 O(1) 的时间复杂度内完成。所以，这三个操作的时间复杂度都是 O(1)。至此，我们就通过散列表和双向链表的组合使用，实现了一个高效的、支持 LRU 缓存淘汰算法的缓存系统原型。

### Redis 有序集合 ###

在有序集合中，每个成员对象有两个重要的属性，key（键值）和 score（分值）。我们不仅会通过 score 来查找数据，还会通过 key 来查找数据。

### Java LinkedHashMap ###

LinkedHashMap 也是通过散列表和链表组合在一起实现的。实际上，它不仅支持按照插入顺序遍历数据，还支持按照访问顺序来遍历数据。按照访问时间排序的 LinkedHashMap 本身就是一个支持 LRU 缓存淘汰策略的缓存系统。

*LinkedHashMap 是通过双向链表和散列表这两种数据结构组合实现的。LinkedHashMap 中的“Linked”实际上是指的是双向链表，并非指用链表法解决散列冲突。*

### 解答开篇 & 内容小结 ###

散列表这种数据结构虽然支持非常高效的数据插入、删除、查找操作，但是散列表中的数据都是通过散列函数打乱之后无规律存储的。也就说，它无法支持按照某种顺序快速地遍历数据。如果希望按照顺序遍历散列表中的数据，那我们需要将散列表中的数据拷贝到数组中，然后排序，再遍历。

因为散列表是动态数据结构，不停地有数据的插入、删除，所以每当我们希望按顺序遍历散列表中的数据的时候，都需要先排序，那效率势必会很低。为了解决这个问题，我们将散列表和链表（或者跳表）结合在一起使用。

### 课后思考 ###

## 21 | 哈希算法（上）：如何防止数据库中的用户信息被脱库 ##

### 什么是哈希算法？ ###

Hash，将任意长度的二进制值串映射为固定长度的二进制值串，这个映射的规则就是*哈希算法*，而通过原始数据映射之后得到的二进制值串就是*哈希值*。

需要满足的要求：

* 从哈希值不能反向推导出原始数据（所以哈希算法也叫单向哈希算法）；
* 对输入数据非常敏感，哪怕原始数据只修改了一个 Bit，最后得到的哈希值也大不相同；
* 散列冲突的概率要很小，对于不同的原始数据，哈希值相同的概率非常小；
* 哈希算法的执行效率要尽量高效，针对较长的文本，也能快速地计算出哈希值。

哈希算法的应用非常非常多，我选了最常见的七个，分别是安全加密、唯一标识、数据校验、散列函数、负载均衡、数据分片、分布式存储。

### 应用一：安全加密 ###

最常用于加密的哈希算法是 

* *MD5*（MD5 Message-Digest Algorithm，MD5 消息摘要算法）
* *SHA*（Secure Hash Algorithm，安全散列算法）

其他加密算法：

* DES（Data Encryption Standard，数据加密标准）
* AES（Advanced Encryption Standard，高级加密标准）

对用于加密的哈希算法来说，有两点格外重要。

1. 很难根据哈希值反向推导出原始数据
2. 散列冲突的概率要很小。

#### 为什么哈希算法无法做到零冲突？ ####

一般情况下，哈希值越长的哈希算法，散列冲突的概率越低。

除此之外，没有绝对安全的加密。越复杂、越难破解的加密算法，需要的计算时间也越长。

### 应用二：唯一标识 ###

给每一个图片取一个唯一标识，或者说信息摘要。比如，我们可以从图片的二进制码串开头取 100 个字节，从中间取 100 个字节，从最后再取 100 个字节，然后将这 300 个字节放到一块，通过哈希算法（比如 MD5），得到一个哈希字符串，用它作为图片的唯一标识。通过这个唯一标识来判定图片是否在图库中，这样就可以减少很多工作量。

可以把每个图片的唯一标识，和相应的图片文件在图库中的路径信息，都存储在散列表中。当要查看某个图片是不是在图库中的时候，我们先通过哈希算法对这个图片取唯一标识，然后在散列表中查找是否存在这个唯一标识。

如果不存在，那就说明这个图片不在图库中；如果存在，我们再通过散列表中存储的文件路径，获取到这个已经存在的图片，跟现在要插入的图片做全量的比对，看是否完全一样。如果一样，就说明已经存在；如果不一样，说明两张图片尽管唯一标识相同，但是并不是相同的图片。

### 应用三：数据校验 ###

们通过哈希算法，对 100 个文件块分别取哈希值，并且保存在种子文件中。对数据很敏感。只要文件块的内容有一丁点儿的改变，最后计算出的哈希值就会完全不同。所以，当文件块下载完成之后，我们可以通过相同的哈希算法，对下载好的文件块逐一求哈希值，然后跟种子文件中保存的哈希值比对。如果不同，说明这个文件块不完整或者被篡改了，需要再重新从其他宿主机器上下载这个文件块。

### 应用四：散列函数 ###

散列函数是设计一个散列表的关键。它直接决定了散列冲突的概率和散列表的性能。不过，相对哈希算法的其他应用，散列函数对于散列算法冲突的要求要低很多。即便出现个别散列冲突，只要不是过于严重，我们都可以通过开放寻址法或者链表法解决。

散列函数用的散列算法一般都比较简单，比较追求效率。

### 解答开篇 ###

针对字典攻击，我们可以引入一个盐（salt），跟用户的密码组合在一起，增加密码的复杂度。我们拿组合之后的字符串来做哈希算法加密，将它存储到数据库中，进一步增加破解的难度。不过我这里想多说一句，我认为安全和攻击是一种博弈关系，不存在绝对的安全。所有的安全措施，只是增加攻击的成本而已。

### 内容小结 ###

哈希算法的四个应用场景

1. 唯一标识，哈希算法可以对大数据做信息摘要，通过一个较短的二进制编码来表示很大的数据。
2. 校验数据的完整性和正确性
3. 安全加密，我们讲到任何哈希算法都会出现散列冲突，但是这个冲突概率非常小。越是复杂哈希算法越难破解，但同样计算时间也就越长。所以，选择哈希算法的时候，要权衡安全性和计算时间来决定用哪种哈希算法。
4. 散列函数，这个我们前面讲散列表的时候已经详细地讲过，它对哈希算法的要求非常特别，更加看重的是散列的平均性和哈希算法的执行效率。


### 课后思考 ###

区块链

## 22 | 哈希算法（下）：哈希算法在分布式系统中有哪些应用 ##

分布式系统相关：

* 负载均衡
* 数据分片
* 分布式存储

### 应用五：负载均衡 ###

负载均衡算法有很多，比如轮询、随机、加权轮询等。

最直接的方法就是，维护一张映射关系表，这张表的内容是客户端 IP 地址或者会话 ID 与服务器编号的映射关系。客户端发出的每次请求，都要先在映射表中查找应该路由到的服务器编号，然后再请求编号对应的服务器。这种方法简单直观，但也有几个弊端：

* 如果客户端很多，映射表可能会很大，比较浪费内存空间；
* 客户端下线、上线，服务器扩容、缩容都会导致映射失效，这样维护映射表的成本就会很大；

我们可以通过哈希算法，对客户端 IP 地址或者会话 ID 计算哈希值，将取得的哈希值与服务器列表的大小进行取模运算，最终得到的值就是应该被路由到的服务器编号。

### 应用六：数据分片 ###

#### 1. 如何统计“搜索关键词”出现的次数？ ####

假如我们有 1T 的日志文件，这里面记录了用户的搜索关键词，我们想要快速统计出每个关键词被搜索的次数，该怎么做呢？

有两个难点：

* 搜索日志很大，没办法放到一台机器的内存中。
* 如果只用一台机器来处理这么巨大的数据，处理时间会很长。

针对这两个难点，采取以下对策：

1. 对数据进行分片
2. 采用多台机器处理的方法，来提高处理速度

具体的方式：

1. 为了提高处理的速度，我们用 n 台机器并行处理。
2. 从搜索记录的日志文件中，依次读出每个搜索关键词
3. 通过哈希函数计算哈希值
4. 再跟 n 取模，最终得到的值，就是应该被分配到的机器编号。

处理过程也是 MapReduce 的基本设计思想

#### 2. 如何快速判断图片是否在图库中？ ####

设现在我们的图库中有 1 亿张图片，很显然，在单台机器上构建散列表是行不通的。因为单台机器的内存有限，而 1 亿张图片构建散列表显然远远超过了单台机器的内存上限。

1. 可以对数据分片，采用多机处理，准备n台机器，让每台机器只维护某一部分图片对应的散列表
2. 每次从图库中读取一个图片，计算唯一标识
3. 然后与机器个数 n 求余取模，得到的值就对应要分配的机器编号
4. 然后将这个图片的唯一标识和图片路径发往对应的机器构建散列表。

针对这种海量数据的处理问题，我们都可以采用多机分布式处理。借助这种分片的思路，可以突破单机内存、CPU 等资源的限制。

### 应用七：分布式存储 ###

该如何决定将哪个数据放到哪个机器上呢？我们可以借用前面数据分片的思想，即通过哈希算法对数据取哈希值，然后对机器个数取模，这个最终值就是应该存储的缓存机器编号。

使得在新加入一个机器后，并不需要做大量的数据搬移。这时候，一致性哈希算法就要登场了

假设我们有 k 个机器，数据的哈希值的范围是[0, MAX]。我们将整个范围划分成 m 个小区间（m 远大于 k），每个机器负责 m/k 个小区间。当有新机器加入的时候，我们就将某几个小区间的数据，从原来的机器中搬移到新的机器中。这样，既不用全部重新哈希、搬移数据，也保持了各个机器上数据数量的均衡。

除了我们上面讲到的分布式缓存，实际上，一致性哈希算法的应用非常广泛，在很多分布式存储系统中，都可以见到一致性哈希算法的影子。

### 解答开篇 & 内容小结 ###

三种哈希算法在分布式系统中的应用，它们分别是：负载均衡、数据分片、分布式存储。

* 在负载均衡应用中，利用哈希算法替代映射表，可以实现一个会话粘滞的负载均衡策略。
* 在数据分片应用中，通过哈希算法对处理的海量数据进行分片，多机分布式处理，可以突破单机资源的限制。
* 在分布式存储应用中，利用一致性哈希算法，可以解决缓存等分布式系统的扩容、缩容导致数据大量搬移的难题。

## 23 | 二叉树基础（上）：什么样的二叉树适合用数组来存储？ ##

二叉树有哪几种存储方式？什么样的二叉树适合用数组来存储？

### 树（Tree） ###

![220043e683ea33b9912425ef759556ae.jpg](img/220043e683ea33b9912425ef759556ae.jpg)

A 节点就是 B 节点的父节点，B 节点是 A 节点的子节点。B、C、D 这三个节点的父节点是同一个节点，所以它们之间互称为兄弟节点。我们把没有父节点的节点叫作根节点，也就是图中的节点 E。我们把没有子节点的节点叫作叶子节点或者叶节点，比如图中的 G、H、I、J、K、L 都是叶子节点。

高度（Height）、深度（Depth）、层（Level）

### 二叉树（Binary Tree） ###

![09c2972d56eb0cf67e727deda0e9412b.jpg](img/09c2972d56eb0cf67e727deda0e9412b.jpg)

* 满二叉树：编号 2 的二叉树中，叶子节点全都在最底层，除了叶子节点之外，每个节点都有左右两个子节点
* 完全二叉树：编号 3 的二叉树中，叶子节点都在最底下两层，最后一层的叶子节点都靠左排列，并且除了最后一层，其他层的节点个数都要达到最大

#### 如何表示（或者存储）一棵二叉树？ ####

需要利用完全二叉树

有两种方法：

* 一种是基于指针或者引用的二叉链式存储法
* 一种是基于数组的顺序存储法。

* 链式存储法，每个节点有三个字段，其中一个存储数据，另外两个是指向左右子节点的指针。
* 顺序存储法，我们把根节点存储在下标 i = 1 的位置，那左子节点存储在下标 2 * i = 2 的位置，右子节点存储在 2 * i + 1 = 3 的位置。以此类推，B 节点的左子节点存储在 2 * i = 2 * 2 = 4 的位置，右子节点存储在 2 * i + 1 = 2 * 2 + 1 = 5 的位置

![14eaa820cb89a17a7303e8847a412330.jpg](img/14eaa820cb89a17a7303e8847a412330.jpg)

举的例子是一棵完全二叉树，所以仅仅“浪费”了一个下标为 0 的存储位置。如果是非完全二叉树，其实会浪费比较多的数组存储空间。

![08bd43991561ceeb76679fbb77071223.jpg](img/08bd43991561ceeb76679fbb77071223.jpg)

如果某棵二叉树是一棵完全二叉树，那用数组存储无疑是最节省内存的一种方式。因为数组的存储方式并不需要像链式存储法那样，要存储额外的左右子节点的指针。这也是为什么完全二叉树会单独拎出来的原因，也是为什么完全二叉树要求最后一层的子节点都靠左的原因。
当我们讲到堆和堆排序的时候，你会发现，堆其实就是一种完全二叉树，最常用的存储方式就是数组。

### 二叉树的遍历 ###

经典的方法有三种，*前序遍历、中序遍历和后序遍历*。其中，前、中、后序，表示的是节点与它的左右子树节点遍历打印的先后顺序。

* 前序遍历是指，对于树中的任意节点来说，先打印这个节点，然后再打印它的左子树，最后打印它的右子树。
* 中序遍历是指，对于树中的任意节点来说，先打印它的左子树，然后再打印它本身，最后打印它的右子树。
* 后序遍历是指，对于树中的任意节点来说，先打印它的左子树，然后再打印它的右子树，最后打印这个节点本身。

*实际上，二叉树的前、中、后序遍历就是一个递归的过程。*

二叉树遍历的时间复杂度是多少

### 解答开篇 & 内容小结 ###

一种非线性表数据结构，树。关于树，有几个比较常用的概念：根节点、叶子节点、父节点、子节点、兄弟节点，还有节点的高度、深度、层数，以及树的高度。

平时最常用的树就是二叉树。二叉树的每个节点最多有两个子节点，分别是左子节点和右子节点。二叉树中，有两种比较特殊的树，分别是满二叉树和完全二叉树。满二叉树又是完全二叉树的一种特殊情况。

二叉树既可以用链式存储，也可以用数组顺序存储。数组顺序存储的方式比较适合完全二叉树，其他类型的二叉树用数组存储会比较浪费存储空间。除此之外，二叉树里非常重要的操作就是前、中、后序遍历操作，遍历的时间复杂度是 O(n)，你需要理解并能用递归代码来实现。

### 课后思考 ###

## 24 | 二叉树基础（下）：有了如此高效的散列表，为什么还需要二叉树？ ##

既然有了这么高效的散列表，使用二叉树的地方是不是都可以替换成散列表呢？有没有哪些地方是散列表做不了，必须要用二叉树来做的呢？

### 二叉查找树（Binary Search Tree） ###

二叉查找树是二叉树中最常用的一种类型，也叫二叉搜索树。二叉查找树是为了实现快速查找而生的。不过，它不仅仅支持快速查找一个数据，还支持快速插入、删除一个数据。

二叉查找树要求，在树中的任意一个节点，其左子树中的每个节点的值，都要小于这个节点的值，而右子树节点的值都大于这个节点的值。

#### 1. 二叉查找树的查找操作 ####

1. 在二叉查找树中查找一个节点

#### 2. 二叉查找树的插入操作 ####

二叉查找树的插入过程有点类似查找操作。新插入的数据一般都是在叶子节点上，所以我们只需要从根节点开始，依次比较要插入的数据和节点的大小关系。

* 如果要插入的数据比节点的数据大，并且节点的右子树为空，就将新数据直接插到右子节点的位置；
* 如果不为空，就再递归遍历右子树，查找插入位置。
* 同理，如果要插入的数据比节点数值小，并且节点的左子树为空，就将新数据插入到左子节点的位置；
* 如果不为空，就再递归遍历左子树，查找插入位置。

#### 3. 二叉查找树的删除操作 ####

1. 如果要删除的节点没有子节点，我们只需要直接将父节点中，指向要删除节点的指针置为 null
2. 如果要删除的节点只有一个子节点（只有左子节点或者右子节点）只需要更新父节点中，指向要删除节点的指针，让它指向要删除节点的子节点就可以了。
3. 如果要删除的节点有两个子节点，们需要找到这个节点的右子树中的最小节点，把它替换到要删除的节点上。然后再删除掉这个最小节点，因为最小节点肯定没有左子节点（如果有左子结点，那就不是最小节点了）。

#### 4. 二叉查找树的其他操作 ####

除了插入、删除、查找操作之外，二叉查找树中还可以支持快速地查找最大节点和最小节点、前驱节点和后继节点。

二叉查找树除了支持上面几个操作之外，还有一个重要的特性：就是*中序遍历二叉查找树，可以输出有序的数据序列，时间复杂度是 O(n)，非常高效*。因此，二叉查找树也叫作二叉排序树。

### 支持重复数据的二叉查找树 ###

1. 二叉查找树中每一个节点不仅会存储一个数据，因此我们通过链表和支持动态扩容的数组等数据结构，把值相同的数据都存储在同一个节点上。
2. 每个节点仍然只存储一个数据。在查找插入位置的过程中，如果碰到一个节点的值，与要插入数据的值相同，我们就将这个要插入的数据放到这个节点的右子树，也就是说，把这个新插入的数据当作大于这个节点的值来处理。
	* 查找当要查找数据的时候，遇到值相同的节点，我们并不停止查找操作，而是继续在右子树中查找，直到遇到叶子节点，才停止。这样就可以把键值等于要查找值的所有节点都找出来。
	* 删除，需要先查找到每个要删除的节点，然后再按前面讲的删除操作的方法，依次删除。

### 二叉查找树的时间复杂度分析 ###

二叉查找树的插入、删除、查找操作的时间复杂度

不管操作是插入、删除还是查找，*时间复杂度其实都跟树的高度成正比，也就是 O(height)*。

树的高度就等于最大层数减一，

极度不平衡的二叉查找树，它的查找性能肯定不能满足我们的需求。我们需要构建一种不管怎么删除、插入数据，在任何时候，都能保持任意节点左右子树都比较平衡的二叉查找树， =》 这就是我们下一节课要详细讲的，一种特殊的二叉查找树，平衡二叉查找树。平衡二叉查找树的高度接近 logn，所以插入、删除、查找操作的时间复杂度也比较稳定，是 O(logn)。

### 解答开篇 ###

散列表的插入、删除、查找操作的时间复杂度可以做到常量级的 O(1)，而二叉查找树在比较平衡的情况下，插入、删除、查找操作时间复杂度才是 O(logn)。

有下面几个原因：

1. 散列表中的数据是无序存储的，如果要输出有序的数据，需要先进行排序。而对于二叉查找树来说，我们只需要中序遍历，就可以在 O(n) 的时间复杂度内，输出有序的数据序列。
2. 散列表扩容耗时很多，而且当遇到散列冲突时，性能不稳定，尽管二叉查找树的性能不稳定，但是在工程中，我们最常用的平衡二叉查找树的性能非常稳定，时间复杂度稳定在 O(logn)。
3. 尽管散列表的查找等操作的时间复杂度是常量级的，但因为哈希冲突的存在，这个常量不一定比 logn 小，所以实际的查找速度可能不一定比 O(logn) 快。加上哈希函数的耗时，也不一定就比平衡二叉查找树的效率高。
4. 散列表的构造比二叉查找树要复杂，需要考虑的东西很多。比如散列函数的设计、冲突解决办法、扩容、缩容等。平衡二叉查找树只需要考虑平衡性这一个问题，而且这个问题的解决方案比较成熟、固定。
5. 为了避免过多的散列冲突，散列表装载因子不能太大，特别是基于开放寻址法解决冲突的散列表，不然会浪费一定的存储空间。

综合这几点，平衡二叉查找树在某些方面还是优于散列表的，所以，这两者的存在并不冲突。我们在实际的开发过程中，需要结合具体的需求来选择使用哪一个。

### 内容小结 ###

特殊的二叉树，二叉查找树，支持快速地查找、插入、删除操作。

* 没有重复值：二叉查找树中，每个节点的值都大于左子树节点的值，小于右子树节点的值。
* 有重复值：
	* 让每个节点存储多个值相同的数据
	* 每个节点中存储一个数据

在二叉查找树中，查找、插入、删除等很多操作的时间复杂度都跟树的高度成正比。两个极端情况的时间复杂度分别是 O(n) 和 O(logn)，分别对应二叉树退化成链表的情况和完全二叉树。  

为了避免时间复杂度的退化，针对二叉查找树，我们又设计了一种更加复杂的树，平衡二叉查找树，时间复杂度可以做到稳定的 O(logn)

## 25 | 红黑树（上）：为什么工程中都用红黑树这种二叉树？ ##

## 26 | 红黑树（下）：掌握这些技巧，你也可以实现一个红黑树 ##

## 27 | 递归树：如何借助树来求解递归算法的时间复杂度？ ##

## 28 | 堆：堆和堆排序：为什么堆排序没有快速排序快 ##

堆排序是一种原地的、时间复杂度为 O(nlogn) 的排序算法。平均情况下，它的时间复杂度为 O(nlogn)。尽管这两种排序算法的时间复杂度都是 O(nlogn)，甚至堆排序比快速排序的时间复杂度还要稳定，但是，*在实际的软件开发中，快速排序的性能要比堆排序好，这是为什么呢？*

### 如何理解“堆”？ ###

堆是一种特殊的树。只要满足这两点，它就是一个堆：

* 堆是一个完全二叉树；
	* 堆必须是一个完全二叉树
	* 完全二叉树要求，除了最后一层，其他层的节点个数都是满的，最后一层的节点都靠左排列。
* 堆中没一个节点的值都必须大于等于（或小于等于）其子树中每个节点的值。
	* 堆中每个节点的值都大于等于（或者小于等于）其左右子节点的值。
	* 对于每个节点的值都大于等于子树中每个节点值的堆，我们叫作“大顶堆”。
	* 对于每个节点的值都小于等于子树中每个节点值的堆，我们叫作“小顶堆”。

### 如何实现一个堆？ ###

*堆都支持哪些操作*以及*如何存储一个堆*。

完全二叉树比较适合用数组来存储。用数组来存储完全二叉树是非常节省存储空间的。因为我们不需要存储左右子节点的指针，单纯地通过数组的下标，就可以找到一个节点的左右子节点和父节点。

![4d349f57947df6590a2dd1364c3b0b1e.jpg](img/4d349f57947df6590a2dd1364c3b0b1e.jpg)

数组中下标为 i 的节点的左子节点，就是下标为 i∗2 的节点，右子节点就是下标为 i∗2+1 的节点，父节点就是下标为 2i​ 的节点。

#### 1. 往堆中插入一个元素 ####

堆化（heapify）

1. 从下往上
2. 从上往下

堆化非常简单，就是顺着节点所在的路径，向上或者向下，对比，然后交换。

#### 2.删除堆顶元素 ####

从堆的定义的第二条中，任何节点的值都大于等于（或小于等于）子树节点的值，我们可以发现，堆顶元素存储的就是堆中数据的最大值或者最小值。

假设我们构造的是大顶堆，堆顶元素就是最大的元素。当我们删除堆顶元素之后，就需要把第二大的元素放到堆顶，那第二大元素肯定会出现在左右子节点中。然后我们再迭代地删除第二大节点，以此类推，直到叶子节点被删除。=》 会造成数据空洞

把最后一个节点放到堆顶，然后利用同样的父子节点对比方法。对于不满足父子节点大小关系的，互换两个节点，并且重复进行这个过程，直到父子节点之间满足大小关系为止。这就是*从上往下的堆化方法*。

因为我们移除的是数组中的最后一个元素，而在堆化的过程中，都是交换操作，不会出现数组中的“空洞”，所以这种方法堆化之后的结果，肯定满足完全二叉树的特性。

一个包含 n 个节点的完全二叉树，树的高度不会超过 log2​n。堆化的过程是顺着节点所在路径比较交换的，所以堆化的时间复杂度跟树的高度成正比，也就是 O(logn)。插入数据和删除堆顶元素的主要逻辑就是堆化，所以，往堆中插入一个元素和删除堆顶元素的时间复杂度都是 O(logn)。

### 如何基于堆实现排序？ ###

堆排序 O(nlogn)，稳定，原地。

#### 1. 建堆 ####

将数组原地建成一个堆，所谓“原地”就是，不借助另一个数组，就在原数组上操作。
1. 在堆中插入一个元素的思路。
	* 尽管数组中包含 n 个数据
	* 起初堆中只包含一个数据，就是下标为 1 的数据
	* 将下标从 2 到 n 的数据依次插入到堆中
	* 将包含 n 个数据的数组，组织成了堆 
2. 从后往前处理数组，并且每个数据都是从上往下堆化

在这段代码中，我们对下标从 2n​ 开始到 1 的数据进行堆化，下标是 2n​+1 到 n 的节点是叶子节点，我们不需要堆化。实际上，对于完全二叉树来说，下标从 2n​+1 到 n 的节点都是叶子节点。

我把每一层的节点个数和对应的高度画了出来，你可以看看。我们只需要将每个节点的高度求和，得出的就是建堆的时间复杂度。

建堆的时间复杂度就是 O(n)。

#### 2. 排序 ####

数组中的第一个元素就是堆顶，也就是最大的元素。我们把它跟最后一个元素交换，那最大元素就放到了下标为 n 的位置。

当堆顶元素移除之后，我们把下标为 n 的元素放到堆顶，然后再通过堆化的方法，将剩下的 n−1 个元素重新构建成堆。堆化完成之后，我们再取堆顶的元素，放到下标是 n−1 的位置，一直重复这个过程，直到最后堆中只剩下标为 1 的一个元素，排序工作就完成了。

整个堆排序的过程，都只需要极个别临时存储空间，所以堆排序是原地排序算法。堆排序包括建堆和排序两个操作，建堆过程的时间复杂度是 O(n)，排序过程的时间复杂度是 O(nlogn)，所以，堆排序整体的时间复杂度是 O(nlogn)。

### 解答开篇 ###

在实际开发中，为什么快速排序要比堆排序性能好？主要有两个原因：

*第一点，堆排序数据访问的方式没有快速排序友好。*

对于快速排序来说，数据是顺序访问的。而对于堆排序来说，数据是跳着访问的。 比如，堆排序中，最重要的一个操作就是数据的堆化。比如下面这个例子，对堆顶节点进行堆化，会依次访问数组下标是 1，2，4，8 的元素，而不是像快速排序那样，局部顺序访问，所以，这样对 CPU 缓存是不友好的。

*第二点，对于同样的数据，在排序过程中，堆排序算法的数据交换次数要多于快速排序。*

排序的时候，提出了两个概念，有序度和逆序度。对于基于比较的排序算法来说，整个排序过程就是由两个基本的操作组成的，比较和交换（或移动）。快速排序数据交换的次数不会比逆序度多。

### 内容小结 ###

堆是一种完全二叉树。它最大的特性是：每个节点的值都大于等于（或小于等于）其子树节点的值。因此，堆被分成了两类，大顶堆和小顶堆。

堆中比较重要的两个操作是插入一个数据和删除堆顶元素。
* 这两个操作都要用到堆化。
* 插入一个数据的时候，我们把新插入的数据放到数组的最后，然后从下往上堆化
* 删除堆顶数据的时候，我们把数组中的最后一个元素放到堆顶，然后从上往下堆化。这两个操作时间复杂度都是 O(logn)。

堆的经典应用，堆排序。

### 课后思考 ###

## 29 | 堆的应用：如何快速获取到top10最热门的搜索关键词？ ##

*假设现在我们有一个包含 10 亿个搜索关键词的日志文件，如何能快速获取到热门榜 Top 10 的搜索关键词呢？*

堆这种数据结构几个非常重要的应用：

* 优先级队列
* 求 Top K 
* 求中位数

### 堆的应用一：优先级队列 ###

优先级队列：

1. 首先是一个队列，先进先出，在优先级队列中，数据的出队顺序不是先进先出，而是按照优先级来（优先级最高的，最先出队）
2. 实现优先级队列的方法很多，但堆是最直接、最高效的。（高度相似）
3. 应用场景：赫夫曼编码、图的最短路径、最小生成树算法等等。
4. 语言实现：Java 的 PriorityQueue，C++ 的 priority_queue 等。

#### 1. 合并有序小文件 ####

将从小文件中取出来的字符串放入到小顶堆中，那堆顶的元素，也就是优先级队列队首的元素，就是最小的字符串。我们将这个字符串放入到大文件中，并将其从堆中删除。然后再从小文件中取出下一个字符串，放入到堆中。循环这个过程，就可以将 100 个小文件中的数据依次放入到大文件中。

删除堆顶数据和往堆中插入数据的时间复杂度都是 O(logn)，n 表示堆中的数据个数，这里就是 100。

#### 2. 高性能定时器 ####

假设我们有一个定时器，定时器中维护了很多定时任务，每个任务都设定了一个要触发执行的时间点。定时器每过一个很小的单位时间（比如 1 秒），就扫描一遍任务，看是否有任务到达设定的执行时间。如果到达了，就拿出来执行。

样每过 1 秒就扫描一遍任务列表的做法比较低效，主要原因：

1. 任务的约定执行时间离当前时间可能还有很久，这样前面很多次扫描其实都是徒劳的
2. 每次都要扫描整个任务列表，如果任务列表很大的话，势必会比较耗时

当 T 秒时间过去之后，定时器取优先级队列中队首的任务执行。然后再计算新的队首任务的执行时间点与当前时间点的差值，把这个值作为定时器执行下一个任务需要等待的时间

定时器既不用间隔 1 秒就轮询一次，也不用遍历整个任务列表，性能也就提高了。

### 堆的应用二：利用堆求Top K ###

求Top K的问题抽象成两类。

* 一类是针对静态数据集合，数据集合事先确定，不会再变。
* 一类是针对动态数据集合，事先并不确定，有数据动态地加入到集合中。

#### 静态数据 ####

维护一个大小为K的小顶堆，顺序遍历数组，从数组中取出数据与堆顶元素比较。如果比堆顶元素大，我们就把堆顶元素删除，并且将这个元素插入到堆中；如果比堆顶元素小，则不做处理，继续遍历数组。

#### 动态数据 ####

针对动态数据求得 Top K 就是实时 Top K

如果每次询问前 K 大数据，我们都基于当前的数据重新计算的话，那时间复杂度就是 O(nlogK)，n 表示当前的数据的大小。实际上，我们可以一直都维护一个 K 大小的小顶堆，当有数据被添加到集合中时，我们就拿它与堆顶的元素对比。如果比堆顶元素大，我们就把堆顶元素删除，并且将这个元素插入到堆中；如果比堆顶元素小，则不做处理。这样，无论任何时候需要查询当前的前 K 大数据，我们都可以立刻返回给他。

### 堆的应用三：利用堆求中位数 ###

中位数，就是处在中间位置的那个数。

* 静态数据：中位数是固定的
* 动态数据：中位数在不停地变动，如果再用先排序的方法，每次询问中位数的时候，都要先进行排序，那效率就不高了。

借助堆这种数据结构，我们不用排序，就可以非常高效地实现求中位数操作。我们来看看，它是如何做到的？

通过这样的方法，每次插入数据，可能会涉及几个数据的堆化操作，所以时间复杂度是 O(logn)。每次求 99% 响应时间的时候，直接返回大顶堆中的堆顶数据即可，时间复杂度是 O(1)。

### 内容小结 ###

堆的几个重要的应用，它们分别是：优先级队列、求 Top K 问题和求中位数问题。

* 优先级队列是一种特殊的队列，优先级高的数据先出队，而不再像普通的队列那样，先进先出。
* 求中位数实际上还有很多变形，比如求 99 百分位数据、90 百分位数据等，处理的思路都是一样的，即利用两个堆，一个大顶堆，一个小顶堆，随着数据的动态添加，动态调整两个堆中的数据，最后大顶堆的堆顶元素就是要求的数据。

### 课后思考 ###

有一个访问量非常大的新闻网站，我们希望将点击量排名 Top 10 的新闻摘要，滚动显示在网站首页 banner 上，并且每隔 1 小时更新一次。如果你是负责开发这个功能的工程师，你会如何来实现呢？


## 30 | 图的表示：如何存储微博、微信等社交网络中的好友关系？ ##

### 如何理解“图”？ ###

另一种非线性数据结构，图。

图中的元素我们就叫作顶点（vertex）。图中的一个顶点可以与任意其他顶点建立连接关系。我们把这种建立的关系叫作边（edge）。

* 微信：把每个用户看作一个顶点，如果两个用户之间互加好友，那就在两者之间建立一条边。所以，整个微信的好友关系就可以用一张图来表示。其中，每个用户有多少个好友，对应到图中，就叫作顶点的*度（degree）*，就是跟顶点相连接的边的条数。边没有方向的图就叫作“无向图”。
* 微博：微博允许单向关注，用户 A 关注了用户 B，但用户 B 可以不关注用户 A。这种边有方向的图叫作“有向图”。在有向图中，我们把度分为入度（In-degree）和出度（Out-degree）：
	* 顶点的入度，表示有多少条边指向这个顶点
	* 顶点的出度，表示有多少条边是以这个顶点为起点指向其他顶点
	* 对应到微博的例子，入度就表示有多少粉丝，出度就表示关注了多少人
* QQ：不仅记录了用户之间的好友关系，还记录了两个用户之间的亲密度，如果两个用户经常往来，那亲密度就比较高；如果不经常往来，亲密度就比较低。可以用*带权图（weighted graph）*表示，在带权图中，每条边都有一个权重（weight），我们可以通过这个权重来表示 QQ 好友间的亲密度。 

### 邻接矩阵存储方法 ###

图最直观的一种存储方法就是，邻接矩阵（Adjacency Matrix）。

邻接矩阵的底层依赖一个二维数组。

* 对于无向图来说，如果顶点 i 与顶点 j 之间有边，我们就将 A[i][j]和 A[j][i]标记为 1；
* 对于有向图来说，如果顶点 i 到顶点 j 之间，有一条箭头从顶点 i 指向顶点 j 的边，那我们就将 A[i][j]标记为 1。同理，如果有一条箭头从顶点 j 指向顶点 i 的边，我们就将 A[j][i]标记为 1。
* 对于带权图，数组中就存储相应的权重。

对于无向图来说，如果 A[i][j]等于 1，那 A[j][i]也肯定等于 1。实际上，我们只需要存储一个就可以了。也就是说，无向图的二维数组中，如果我们将其用对角线划分为上下两部分，那我们只需要利用上面或者下面这样一半的空间就足够了，另外一半白白浪费掉了。

* 邻接矩阵的缺点是浪费空间，如果存储的是稀疏图（Sparse Matrix）——顶点很多，但每个顶点的边并不多。
* 邻接矩阵的存储方法的优点：
	1. 邻接矩阵的存储方式简单、直接，因为基于数组，所以在获取两个顶点的关系时，非常高效。
	2. 用邻接矩阵存储图的另外一个好处是方便计算。

### 邻接表存储方法 ###

邻接表（Adjacency List）

### 解答开篇 ###

数据结构是为算法服务的，所以具体选择哪种存储方法，与期望支持的操作有关系。针对微博用户关系，假设我们需要支持下面这样几个操作：

* 判断用户 A 是否关注了用户 B；
* 判断用户 A 是否是用户 B 的粉丝；
* 用户 A 关注用户 B；
* 用户 A 取消关注用户 B；
* 根据用户名称的首字母排序，分页获取用户的粉丝列表；
* 根据用户名称的首字母排序，分页获取用户的关注列表。

如何存储一个图，邻接矩阵和邻接表。因为社交网络是一张稀疏图，使用邻接矩阵存储比较浪费存储空间。所以，这里我们采用邻接表来存储。

* 邻接表：存储了用户的关注关系
	* 邻接表中，每个顶点的链表中，存储的就是这个顶点指向的顶点
	* 如果要查找某个用户关注了哪些用户，我们可以在邻接表中查找
* 逆邻接表：存储的是用户的被关注关系
	*  逆邻接表中，每个顶点的链表中，存储的是指向这个顶点的顶点
	*  如果要查找某个用户被哪些用户关注了，我们从逆邻接表中查找。

基础的邻接表不适合快速判断两个用户之间是否是关注与被关注的关系，所以我们选择改进版本，将邻接表中的链表改为支持快速查找的动态数据结构。选择哪种动态数据结构呢？红黑树、跳表、有序动态数组还是散列表呢？（跳表）
* 如果对于小规模的数据，比如社交网络中只有几万、几十万个用户，我们可以将整个社交关系存储在内存中。
* 数据规模太大，可以通过哈希算法等数据分片方式，将邻接表存储在不同的机器上。

![08e4f4330a1d88e9fec94b0f2d1bbe2f.jpg](img/08e4f4330a1d88e9fec94b0f2d1bbe2f.jpg)

* 在机器 1 上存储顶点 1，2，3 的邻接表
* 在机器 2 上，存储顶点 4，5 的邻接表
* 逆邻接表的处理方式也一样，当要查询顶点与顶点关系的时候，我们就利用同样的哈希算法，先定位顶点所在的机器，然后再在相应的机器上查找

还可以利用外部存储（比如硬盘）数据库是我们经常用来持久化存储关系数据的，所以我这里介绍一种数据库的存储方式。

![7339595c631660dc87559bec2ddf928f.jpg](img/7339595c631660dc87559bec2ddf928f.jpg)

### 内容小结 ###

* 非线性表数据结构，图。
* 关于图，需要理解这样几个概念：无向图、有向图、带权图、顶点、边、度、入度、出度。
* 两个主要的存储方式：邻接矩阵和邻接表。

* 邻接矩阵存储方法的缺点是比较浪费空间，但是优点是查询效率高，而且方便矩阵运算。
* 邻接表存储方法中每个顶点都对应一个链表，存储与其相连接的其他顶点。尽管邻接表的存储方式比较节省存储空间，但链表不方便查找，所以查询效率没有邻接矩阵存储方式高。
* 针对这个问题，邻接表还有改进升级版，即将链表换成更加高效的动态数据结构，比如平衡二叉查找树、跳表、散列表等。

### 课后思考 ###

1. 关于开篇思考题，我们只讲了微博这种有向图的解决思路，那像微信这种无向图，应该怎么存储呢？你可以照着我的思路，自己做一下练习。
2. 除了我今天举的社交网络可以用图来表示之外，符合图这种结构特点的例子还有很多，比如知识图谱（Knowledge Graph）。关于图这种数据结构，你还能想到其他生活或者工作中的例子吗？

### 精选留言 ###

## 31 | 深度和广度优先搜索：如何找出社交网络中的三度好友关系？ ##

今天的开篇问题就是，*给你一个用户，如何找出这个用户的所有三度（其中包含一度、二度和三度）好友关系？*

### 什么是“搜索”算法？ ###

算法是作用于具体数据结构之上的，深度优先搜索算法和广度优先搜索算法都是基于“图”这种数据结构的。图这种数据结构的表达能力很强，大部分涉及搜索的场景都可以抽象成“图”。

图上的搜索算法，最直接的理解就是，在图中找出从一个顶点出发，到另一个顶点的路径。具体方法有很多，两种最简单、最“暴力”的深度优先、广度优先搜索，还有 A*、IDA* 等启发式搜索算法。

### 广度优先搜索（BFS） ###

广度优先搜索（Breadth-First-Search），BFS。一种“地毯式”层层推进的搜索策略，即先查找离起始顶点最近的，然后是次近的，依次往外搜索。bfs() 函数就是基于之前定义的，图的广度优先搜索的代码实现。其中 s 表示起始顶点，t 表示终止顶点。我们搜索一条从 s 到 t 的路径。实际上，这样求得的路径就是从 s 到 t 的最短路径。

里面有三个重要的辅助变量 visited、queue、prev。

* visited 是用来记录已经被访问的顶点，用来避免顶点被重复访问。如果顶点 q 被访问，那相应的 visited[q]会被设置为 true。
* queue 是一个队列，用来存储已经被访问、但相连的顶点还没有被访问的顶点。因为广度优先搜索是逐层访问的，也就是说，我们只有把第 k 层的顶点都访问完成之后，才能访问第 k+1 层的顶点。当我们访问到第 k 层的顶点的时候，我们需要把第 k 层的顶点记录下来，稍后才能通过第 k 层的顶点来找第 k+1 层的顶点。所以，我们用这个队列来实现记录的功能。
* prev 用来记录搜索路径。从顶点 s 开始，广度优先搜索到顶点 t 后，prev 数组中存储的就是搜索的路径。

时间复杂度和空间复杂度

* 广度优先搜索的空间消耗主要在几个辅助变量 visited 数组、queue 队列、prev 数组上。这三个存储空间的大小都不会超过顶点的个数，所以空间复杂度是 O(V)。

### 深度优先搜索（DFS） ###

深度优先搜索（Depth-First-Search），简称DFS。

深度优先搜索用的是一种比较著名的算法思想，回溯思想。深度优先搜索代码实现用到了prev、visited变量以及print()函数。

![8778201ce6ff7037c0b3f26b83efba85.jpg](img/8778201ce6ff7037c0b3f26b83efba85.jpg)

* 每条边最多会被访问两次，一次是遍历，一次是回退。所以，图上的深度优先搜索算法的时间复杂度是 O(E)，E 表示边的个数。
* 深度优先搜索算法的消耗内存主要是 visited、prev 数组和递归调用栈。visited、prev 数组的大小跟顶点的个数 V 成正比，递归调用栈的最大深度不会超过顶点的个数，所以总的空间复杂度就是 O(V)。

### 解答开题 ###

社交网络可以用图来表示。这个问题就非常适合用图的广度优先搜索算法来解决，因为广度优先搜索是层层往外推进的。

1. 遍历与起始顶点最近的一层顶点，也就是用户的一度好友
2. 再遍历与用户距离的边数为 2 的顶点，也就是二度好友关系
3. 以及与用户距离的边数为 3 的顶点，也就是三度好友关系

### 内容小结 ###

广度优先搜索和深度优先搜索是图上的两种最常用、最基本的搜索算法，比起其他高级的搜索算法，也被叫作暴力搜索算法。

1. 广度优先搜索，从起始顶点开始，依次往外遍历。广度优先搜索需要借助队列来实现，遍历得到的路径就是，起始顶点到终止顶点的最短路径。
2. 深度优先搜索用的是回溯思想，非常适合用递归实现。换种说法，深度优先搜索是借助栈来实现的。
3. 在执行效率方面，深度优先和广度优先搜索的时间复杂度都是 O(E)，空间复杂度是 O(V)。

### 课后思考 ###

1. 我们通过广度优先搜索算法解决了开篇的问题，你可以思考一下，能否用深度优先搜索来解决呢？
2. 学习数据结构最难的不是理解和掌握原理，而是能灵活地将各种场景和问题抽象成对应的数据结构和算法。今天的内容中提到，迷宫可以抽象成图，走迷宫可以抽象成搜索算法，你能具体讲讲，如何将迷宫抽象成一个图吗？或者换个说法，如何在计算机中存储一个迷宫？

## 32 | 字符串匹配基础（上）：如何借助哈希算法实现高效字符串匹配？ ##

* BF 算法和 RK 算法
* BM 算法和 KMP 算法

*RK 算法是如何借助哈希算法来实现高效字符串匹配的呢？*

### BF算法 ###

BF 算法中的 BF 是 Brute Force 的缩写，中文叫作暴力匹配算法，也叫朴素匹配算法。

1. 算法执行效率要比最差情况的时间复杂度O(n*m)高很多：
	* 实际开发中，大部分情况主串和子串的长度不会太长
	* 每次模式串与主串中的子串匹配的时候，当中途遇到不能匹配的字符的时候，就可以就停止了
2. 匹配算法思想简单，代码实现也非常简单。KISS（Keep it Simple and Stupid）设计原则。

#### RK算法 ####

RK 算法的全称叫 Rabin-Karp 算法

RK 算法的思路是这样的：我们通过哈希算法对主串中的 n-m+1 个子串分别求哈希值，然后逐个与模式串的哈希值比较大小。如果某个子串的哈希值与模式串相等，那就说明对应的子串和模式串匹配了（这里先不考虑哈希冲突的问题，后面我们会讲到）。因为哈希值是一个数字，数字之间比较是否相等是非常快速的，所以模式串和子串比较的效率就提高了。

比如要处理的字符串只包含 a～z 这 26 个小写字母，那我们就用二十六进制来表示一个字符串。我们把 a～z 这 26 个字符映射到 0～25 这 26 个数字，a 就表示 0，b 就表示 1，以此类推，z 表示 25。

小细节：26^(m-1) 这部分的计算，我们可以通过查表的方法来提高效率。我们事先计算好 26^0、26^1、26^2……26^(m-1)，并且存储在一个长度为 m 的数组中，公式中的“次方”就对应数组的下标。当我们需要计算 26 的 x 次方的时候，就可以从数组的下标为 x 的位置取值，直接使用。

整个 RK 算法包含两部分，计算子串哈希值和模式串哈希值与子串哈希值之间的比较。

1. 只需要扫描一遍主串就能计算出所有子串的哈希值了，所以这部分的时间复杂度是 O(n)。
2. 模式串哈希值与每个子串哈希值之间的比较的时间复杂度是 O(1)，总共需要比较 n-m+1 个子串的哈希值，所以，这部分的时间复杂度也是 O(n)。所以，RK 算法整体的时间复杂度就是 O(n)。

PS.模式串很长，相应的主串中的子串也会很长，通过上面的哈希算法计算得到的哈希值就可能很大，如果超过了计算机中整型数据可以表示的范围

### 解答开篇 & 内容小结 ###

两种字符串匹配算法，BF算法和RK算法。

* BF 算法是最简单、粗暴的字符串匹配算法，它的实现思路是，拿模式串与主串中是所有子串匹配，看是否有能匹配的子串。所以，时间复杂度也比较高，是 O(n*m)，n、m 表示主串和模式串的长度。不过，在实际的软件开发中，因为这种算法实现简单，对于处理小规模的字符串匹配很好用。
* RK 算法是借助哈希算法对 BF 算法进行改造，即对每个子串分别求哈希值，然后拿子串的哈希值与模式串的哈希值比较，减少了比较的时间。所以，理想情况下，RK 算法的时间复杂度是 O(n)，跟 BF 算法相比，效率提高了很多。不过这样的效率取决于哈希算法的设计方法，如果存在冲突的情况下，时间复杂度可能会退化。极端情况下，哈希算法大量冲突，时间复杂度就退化为 O(n*m)。

## 33 | 字符串匹配基础（中）：如何实现文本编辑器中的查找功能？ ##

对于查找功能是重要功能的软件来说，比如一些文本编辑器，它们的查找功能都是用哪种算法来实现的呢？有没有比 BF 算法和 RK 算法更加高效的字符串匹配算法呢？

BM（Boyer-Moore）算法，一种非常高效的字符串匹配算法。

### BM算法的核心思想 ###

### BM算法原理分析 ###

BM 算法包含两部分，分别是*坏字符规则（bad character rule）*和*好后缀规则（good suffix shift）*。

#### 1. 坏字符规则 ####

坏字符规则的缺点：根据 si-xi 计算出来的移动位数，有可能是负数，不但不会向后滑动模式串，还有可能倒退。

#### 2. 好后缀规则 ####



当模式串和主串中的某个字符不匹配的时候，如何选择用好后缀规则还是坏字符规则，来计算模式串往后滑动的位数？

分别计算好后缀和坏字符往后滑动的位数，然后取两个数中最大的，作为模式串往后滑动的位数。这种处理方法还可以避免我们前面提到的，根据坏字符规则，计算得到的往后滑动的位数，有可能是负数的情况。

### BM算法代码实现 ###

### BM 算法的性能分析及优化 ###


## 34 | 字符串匹配基础（下）：如何借助BM算法轻松理解KMP算法？ ##

## 35 | Trie树：如何实现搜索引擎的搜索关键词提示功能？ ##

搜索引擎，它们的关键词提示功能非常全面和精准，底层最基本的原理就是今天要讲的这种数据结构：Trie 树。

### 什么是“Trie树”？ ###

一种专门处理字符串匹配的数据结构，用来解决在一组字符串集合中快速查找某个字符串的问题。

Trie 树的本质，就是利用字符串之间的公共前缀，将重复的前缀合并在一起。

### 如何实现一棵Trie树？ ###

Trie 树主要有两个操作：

* 将字符串集合构造成 Trie 树
* 在 Trie 树中查询一个字符串

*如何存储一个 Trie 树？*

假设字符串中只有a到z这26个小写字母，在数组中下标为0的位置，存储指向子节点a的指针，下标为1的位置存储指向子节点b的指针，以此类推，下标为 25 的位置，存储的是指向的子节点 z 的指针。如果某个字符的子节点不存在，我们就在对应的下标的位置存储 null。
	
	class TrieNode {
	  char data;
	  TrieNode children[26];
	}

*在 Trie 树中，查找某个字符串的时间复杂度是多少？*

* 构建 Trie 树的过程，需要扫描所有的字符串，时间复杂度是 O(n)（n 表示所有字符串的长度和）。
* 每次查询时，如果要查询的字符串长度是 k，那我们只需要比对大约 k 个节点，就能完成查询操作。跟原本那组字符串的长度和个数没有任何关系。所以说，构建好 Trie 树后，在其中查找字符串的时间复杂度是 O(k)，k 表示要查找的字符串的长度

### Trie树真的很耗内存吗？ ###

实际上，Trie 树的变体有很多，都可以在一定程度上解决内存消耗的问题。比如，缩点优化，就是对只有一个子节点的节点，而且此节点不是一个串的结束节点，可以将此节点与子节点合并。这样可以节省空间，但却增加了编码难度。

### Trie 树与散列表、红黑树的比较 ###

字符串的匹配问题，笼统上讲，其实就是数据的查找问题。对于支持动态数据高效操作的数据结构，比如散列表、红黑树、跳表等等。实际上，这些数据结构也可以实现在一组字符串中查找字符串的功能。

散列表和红黑树，跟 Trie 树比较一下，看看它们各自的优缺点和应用场景。

1. 第一，字符串中包含的字符集不能太大。如果字符集太大，那存储空间可能就会浪费很多。即便可以优化，但也要付出牺牲查询、插入效率的代价。
2. 第二，要求字符串的前缀重合比较多，不然空间消耗会变大很多。
3. 第三，如果要用 Trie 树解决问题，那我们就要自己从零开始实现一个 Trie 树。
4. 我们知道，通过指针串起来的数据块是不连续的，而 Trie 树中用到了指针，所以，对缓存并不友好，性能上会打个折扣。

实际上，Trie 树只是不适合精确匹配查找，这种问题更适合用散列表或者红黑树来解决。Trie 树比较适合的是查找前缀匹配的字符串，也就是类似开篇问题的那种场景。

### 解答开题 ###

* 我刚讲的思路是针对英文的搜索关键词提示，对于更加复杂的中文来说，词库中的数据又该如何构建成 Trie 树呢？
* 如果词库中有很多关键词，在搜索提示的时候，用户输入关键词，作为前缀在 Trie 树中可以匹配的关键词也有很多，如何选择展示哪些内容呢？
* 像 Google 这样的搜索引擎，用户单词拼写错误的情况下，Google 还是可以使用正确的拼写来做关键词提示，这个又是怎么做到的呢？

Trie 树的这个应用可以扩展到更加广泛的一个应用上，就是自动输入补全，比如输入法自动补全功能、IDE 代码编辑器自动补全功能、浏览器网址输入的自动补全功能等等。

### 内容小结 ###

一种特殊的树，Trie 树。Trie 树是一种解决字符串快速匹配问题的数据结构。如果用来构建 Trie 树的这一组字符串中，前缀重复的情况不是很多，那 Trie 树这种数据结构总体上来讲是比较费内存的，是一种空间换时间的解决问题思路。

尽管比较耗费内存，但是对内存不敏感或者内存消耗在接受范围内的情况下，在 Trie 树中做字符串匹配还是非常高效的，时间复杂度是 O(k)，k 表示要匹配的字符串的长度。

但是，Trie 树的优势并不在于，用它来做动态集合数据的查找，因为，这个工作完全可以用更加合适的散列表或者红黑树来替代。Trie 树最有优势的是查找前缀匹配的字符串，比如搜索引擎中的关键词提示功能这个场景，就比较适合用它来解决，也是 Trie 树比较经典的应用场景。

### 课后思考 ###

Trie 树应用场合对数据要求比较苛刻，比如字符串的字符集不能太大，前缀重合比较多等。如果现在给你一个很大的字符串集合，比如包含 1 万条记录，如何通过编程量化分析这组字符串集合是否比较适合用 Trie 树解决呢？也就是如何统计字符串的字符集大小，以及前缀重合的程度呢？

高级篇

## 39 | 回溯算法：从电影《蝴蝶效应》中学习回溯算法的核心思想 ##

除了用来指导像深度优先搜索这种经典的算法设计之外，还可以用在很多实际的软件开发场景中，比如正则表达式匹配、编译原理中的语法分析等。

很多经典的数学问题都可以用回溯算法解决，比如数独、八皇后、0-1 背包、图的着色、旅行商问题、全排列等等。

### 如何理解“回溯算法”？ ###

回溯的处理思想，有点类似枚举搜索。我们枚举所有的解，找到满足期望的解。为了有规律地枚举所有可能的解，避免遗漏和重复，我们把问题求解的过程分为多个阶段。每个阶段，我们都会面对一个岔路口，我们先随意选一条路走，当发现这条路走不通的时候（不符合期望的解），就回退到上一个岔路口，另选一种走法继续走。

回溯算法非常适合用递归代码实现，所以，我把八皇后的算法翻译成代码。

*回溯算法的思想*

*回溯算法的代码呈现*

### 两个回溯算法的经典应用 ###

#### 1.0-1背包 ####

#### 2.正则表达式 ####

## 43 | 拓扑排序：如何确定代码源文件的编译依赖关系？ ##

## 44 | 最短路径：地图软件是如何计算出最优出行路径的？ ##



