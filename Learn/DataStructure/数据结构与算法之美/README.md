# 数据结构与算法之美 #

开篇词

## 开篇词 | 从今天起，跨过“数据结构与算法”这道坎 ##

扎实的基础，是快速学习并且获得成功的秘诀。

基础知识就像是一座大楼的地基，它决定了我们的技术高度。而要想快速做出点事情，前提条件一定是基础能力过硬，“内功”要到位。

四个递进的模块：

#### 1. 入门篇 ####

时间、空间复杂度分析是数据结构和算法中非常重要的知识点，贯穿整个专栏的学习过程。

掌握时间、空间复杂度的概念，大O表示法的由来，各种复杂度分析技巧，以及最好、最坏、平均、均摊复杂度分析法。

#### 2. 基础篇 ####

针对每种数据结构和算法，我都会结合具体的软件开发实例，由浅入深进行讲解，并适时总结一些实用“宝典”。

#### 3. 高级篇 ####

讲一些不是那么常用的数据结构和算法。设置这一部分的目的，是为了让你开拓视野，强化训练算法思维、逻辑思维。

#### 4. 实战篇 ####

绕数据结构和算法在具体软件实践中的应用来讲的，所以最后我会通过实战部分串讲一下前面讲到的数据结构和算法。我会拿一些开源项目、框架或者系统设计问题，剖析它们背后的数据结构和算法，让你有一个更加直观的感受。

人生路上，我们会遇到很多的坎。跨过去，你就可以成长，跨不过去就是困难和停滞。而在后面很长的一段时间里，你都需要为这个困难买单。

## 01 | 为什么要学习数据结构和算法 ##

*想要通关大厂面试，千万别让数据结构和算法拖了后腿*

学任何知识都是为了“用”，是为了实际解决工作问题的。

*业务开发工程师，你真的愿意做一辈子 CRUD boy 吗？*

不需要自己实现，并不代表什么都不需要了解。

用到的各种框架、中间件和底层系统，比如Spring、RPC信息、消息中间件、Redis等等，在这些基础框架中，一般都揉和了很多基础数据结构和算法的设计思想。

掌握数据结构和算法，不管对于阅读框架源码，还是理解其背后的设计思想，都是非常有用的。

*基础架构研发工程师，写出达到开源水平的框架才是你的目标！*

*对编程还有追求？不想被行业淘汰？那就不要只会写凑合能用的代码！*

性能好坏起码是其中一个非常重要的判断标准。

### 内容小结 ###

学习数据结构和算法

* 建立时间复杂度、空间复杂度意识
* 写出高质量的代码
* 能够设计基础架构
* 提升编程技能
* 训练逻辑思维
* 积攒人生经验

*掌握了数据结构与算法，你看待问题的深度，解决问题的角度就会完全不一样。*

### 课后思考 ###

### 精选留言 ###

#### 1.  ####

为什么学习数据结构和算法？我认为有3点比较重要
1.直接好处是能够有写出性能更优的代码。
2.算法，是一种解决问题的思路和方法，有机会应用到生活和事业的其他方面。
3.长期来看，大脑思考能力是个人最重要的核心竞争力，而算法是为数不多的能够有效训练大脑思考能力的途径之一。

#### 2.  ####

一定要动手写

#### 3.  ####

总感觉学了就忘，忘了又学，如此反复，老师，这种到底是没了解算法的原理导致不会灵活应用，还是写的少导致的，感觉学习算法很少能应用起来

作者回复: 1. 客观的讲，有些项目确实涉及的数据结构和算法少一些，你可以再看下我文章里写的。
2. 你提到学了又忘，我觉得一方面你是没有掌握学习的方法，学习的重点，走马观花的看肯定比较容易忘；我们02节会具体讲；
3. 不会灵活应用？那估计还是没有好的教材教你如何应用，还有可能就是确实还没掌握太牢，只是懂点皮毛，很浅，灵活应用是一个比较的境界，需要一段时间的沉淀学习。
4. 学习算法并不是为了记住几个排序、二分查找、二叉树遍历，他还能锻炼你的逻辑思维、性能意识，而且，如果你写代码能力还有欠缺，你还可以通过把学到的数据结构和算法都实现一遍，这是一种很好很好的锻炼编程能力的方法。所以不要过度追求一定要在项目里手写快排、手写二叉树才能算是用上。

## 02 | 如何抓住重点，系统高效地学习数据结构与算法？ ##

看不懂数据结构和算法，*真正的原因是没有找到好的学习方法，没有抓住学习的重点。*

### 什么是数据结构？什么是算法 ###

虽然我们说没必要深挖严格的定义，但是这并不等于不需要理解概念。

广义和狭义的理解数据结构：

1. 从广义上讲，数据结构就是指一组数据的存储结构。算法就是操作数据的一组方法。
2. 从狭义上讲，是指某些著名的数据结构和算法，比如队列、栈、堆、二分查找、动态规划等。

数据结构和算法是相辅相成的，*数据结构是为算法服务的，算法要作用在特定的数据结构之上*。 

### 学习这个专栏需要什么基础 ###

是什么 => 为什么 => 怎么做

### 学习的重点再什么地方 ###

学习数据结构与算法

1. 掌握一个数据结构与算法中最重要的概念——复杂度分析
2. 最常用，最基础的20个数据结构与算法
	* 10 个数据结构：数组、链表、栈、队列、散列表、二叉树、堆、跳表、图、Trie 树
	* 10 个算法：递归、排序、二分查找、搜索、哈希算法、贪心算法、分治算法、回溯算法、动态规划、字符串匹配算法
3. 要学习它的“来历”“自身的特点”“适合解决的问题”以及“实际的应用场景”

### 一些可以让你事半功倍的学习技巧 ###

1. 边学边练、适度刷题：可以“适度”刷题，但一定不要浪费太多时间在刷题上。我们学习的目的还是掌握，然后应用
2. 多问、多思考、多互动：学习最好的方法是，找到几个人一起学习，一块儿讨论切磋，有问题及时寻求老师答疑。
3. 打怪升级学习法：学习的过程中，我们碰到最大的问题就是，坚持不下来；我们在枯燥的学习过程中，也可以给自己设立一个切实可行的目标。
4. 知识需要沉淀，不要想视图一下子掌握所有：学习知识的过程是反复迭代、不断沉淀的过程。

### 内容小结 ###

* 数据结构和算法的学习重点，复杂度分析，以及10个数据结构和10个算法。
* 学习技巧的总结

## 03 | 复杂度分析（上）：如何分析、统计算法的执行效率和资源消耗 ##

复杂度分析是整个算法学习的精髓，只要掌握了它，数据结构和算法的内容基本上就掌握了一半。

### 为什么需要复杂度分析 ###

事后统计法

1. 测试结构非常依赖测试环境
2. 测试结果受数据规模的影响很大

*我们需要一个不用具体的测试数据来测试，就可以粗略地估计算法的执行效率的方法*。

### 大O复杂度表示法 ###

	 int cal(int n) {
	   int sum = 0;
	   int i = 1;
	   for (; i <= n; ++i) {
	     sum = sum + i;
	   }
	   return sum;
	 }

假设每行代码执行的时间都一样，为 unit_time，第 2、3 行代码分别需要 1 个 unit_time 的执行时间，第 4、5 行都运行了 n 遍，所以需要 2n*unit_time 的执行时间，所以这段代码总的执行时间就是 (2n+2)*unit_time。*所有代码的执行时间 T(n) 与每行代码的执行次数成正比。*

	T(n) = O(f(n))

T(n) 它表示代码执行的时间；n 表示数据规模的大小；f(n) 表示每行代码执行的次数总和。

大 O 时间复杂度实际上并不具体表示代码真正的执行时间，而是表示代码执行时间随数据规模增长的变化趋势，所以，也叫作渐进时间复杂度（asymptotic time complexity），简称时间复杂度。

### 时间复杂度分析 ###

1. 只关注循环次数最多的一段代码:我们在分析一个算法、一段代码的时间复杂度的时候，也只关注循环执行次数最多的那一段代码就可以了
2. 加法法则：总复杂度等于量级最大的那段代码的复杂度
3. 乘法法则：嵌套代码的复杂度等于嵌套内外代码复杂度的乘积。

### 几种常见时间复杂度实例分析 ###

* 常量阶O(1)
* 对数阶O(log^n)
* 线性阶O(n)
* 线性对数阶O(nlog^n)
* 指数阶O(2^n)
* 阶乘阶O(n!)
* 平方阶O(n^2)、立方阶O(n^3)....K次方阶O(n^k)

*多项式量级*和*非多项式量级*

非多项式量级：O(2n) 和 O(n!)

#### 1. O(1) ####

	 int i = 8;
	 int j = 6;
	 int sum = i + j;

O(1) 只是常量级时间复杂度的一种表示方法，并不是指只执行了一行代码。

一般情况下，只要算法中不存在循环语句、递归语句，即使有成千上万行的代码，其时间复杂度也是Ο(1)。

#### 2. O(logn)、O(nlogn) ####

	 i=1;
	 while (i <= n)  {
	   i = i * 2;
	 }

*在采用大 O 标记复杂度的时候，可以忽略系数，即 O(Cf(n)) = O(f(n))*。所以，O(log2n) 就等于 O(log3n)。因此，在对数阶时间复杂度的表示方法里，我们忽略对数的“底”，统一表示为 O(logn)。

2^x=n，O(log~2n)

#### 3. O(m+n)、O(m*n) ####

代码的复杂度由*两个数据的规模*来决定

	int cal(int m, int n) {
	  int sum_1 = 0;
	  int i = 1;
	  for (; i < m; ++i) {
	    sum_1 = sum_1 + i;
	  }
	
	  int sum_2 = 0;
	  int j = 1;
	  for (; j < n; ++j) {
	    sum_2 = sum_2 + j;
	  }
	
	  return sum_1 + sum_2;
	}

从代码来看，m 和 n 是表示两个数据规模。我们无法事先评估 m 和 n 谁的量级大，所以我们在表示复杂度的时候，就不能简单地利用加法法则，省略掉其中一个。所以，上面代码的时间复杂度就是 O(m+n)。需要将加法规则改为：T1(m) + T2(n) = O(f(m) + g(n))。但是乘法法则继续有效：T1(m)*T2(n) = O(f(m) * f(n))

### 空间复杂度分析 ###

时间复杂度的全称是*渐进时间复杂度，表示算法的执行时间与数据规模之间的增长关系*。空间复杂度全称就是*渐进空间复杂度（asymptotic space complexity），表示算法的存储空间与数据规模之间的增长关系*。

	void print(int n) {
	  int i = 0;
	  int[] a = new int[n];
	  for (i; i <n; ++i) {
	    a[i] = i * i;
	  }
	
	  for (i = n-1; i >= 0; --i) {
	    print out a[i]
	  }
	}

* 第 2 行代码中，我们申请了一个空间存储变量 i，但是它是常量阶的，跟数据规模 n 没有关系，所以我们可以忽略。
* 第 3 行申请了一个大小为 n 的 int 类型数组

剩下的代码都没有占用更多的空间，所以整段代码的空间复杂度就是 O(n)。 

### 内容小结 ###

复杂度也叫渐进复杂度，包括时间复杂度和空间复杂度，用来分析算法执行效率与数据规模之间的增长关系。越高阶复杂度的算法，执行效率越低。常见的复杂度并不多，从低阶到高阶有：O(1)、O(log^n)、O(n)、O(nlog^n)、O(n^2)。

*复杂度分析并不难，关键在于多练*。

### 课后思考 ###

有人说，我们项目之前都会进行性能测试，再做代码的时间复杂度、空间复杂度分析，是不是多此一举呢？而且，每段代码都分析一下时间复杂度、空间复杂度，是不是很浪费时间呢？你怎么看待这个问题呢？

### 精选留言 ###

#### 1.  ####

我不认为是多此一举，渐进时间，空间复杂度分析为我们提供了一个很好的理论分析的方向，并且它是宿主平台无关的，能够让我们对我们的程序或算法有一个大致的认识，让我们知道，比如在最坏的情况下程序的执行效率如何，同时也为我们交流提供了一个不错的桥梁，我们可以说，算法1的时间复杂度是O(n)，算法2的时间复杂度是O(logN)，这样我们立刻就对不同的算法有了一个“效率”上的感性认识。

当然，渐进式时间，空间复杂度分析只是一个理论模型，只能提供给粗略的估计分析，我们不能直接断定就觉得O(logN)的算法一定优于O(n), 针对不同的宿主环境，不同的数据集，不同的数据量的大小，在实际应用上面可能真正的性能会不同，个人觉得，针对不同的实际情况，进而进行一定的性能基准测试是很有必要的，比如在统一一批手机上(同样的硬件，系统等等)进行横向基准测试，进而选择适合特定应用场景下的最有算法。

综上所述，渐进式时间，空间复杂度分析与性能基准测试并不冲突，而是相辅相成的，但是一个低阶的时间复杂度程序有极大的可能性会优于一个高阶的时间复杂度程序，所以在实际编程中，时刻关心理论时间，空间度模型是有助于产出效率高的程序的，同时，因为渐进式时间，空间复杂度分析只是提供一个粗略的分析模型，因此也不会浪费太多时间，重点在于在编程时，要具有这种复杂度分析的思维。

## 04 | 复杂度分析（下）：浅析最好、最坏、平均、均摊时间复杂度 ##

四个复杂度分析方面的知识点：最好情况时间复杂度（best case time complexity）、最坏情况时间复杂度（worst case time complexity）、平均情况时间复杂度（average case time complexity）、均摊时间复杂度（amortized time complexity）。

### 最好、最坏情况时间复杂度 ###

	// n表示数组array的长度
	int find(int[] array, int n, int x) {
	  int i = 0;
	  int pos = -1;
	  for (; i < n; ++i) {
	    if (array[i] == x) pos = i;
	  }
	  return pos;
	}

* 最好情况时间复杂度就是，在最理想的情况下，执行这段代码的时间复杂度。
* 最坏情况时间复杂度就是，在最糟糕的情况下，执行这段代码的时间复杂度。

### 平均情况时间复杂度 ###

要查找的变量 x 在数组中的位置，有 n+1 种情况：**在数组的 0～n-1 位置中**和**不在数组中**。我们把每种情况下，查找需要遍历的元素个数累加起来，然后再除以 n+1，就可以得到需要遍历的元素个数的平均值，即：(1+2+3+⋯+n+n)/(n+1)=n(n+3))/(2(n+1)

时间复杂度的大 O 标记法中，可以省略掉系数、低阶、常量，得到的平均时间复杂度就是 O(n)。

要查找的变量x在数组中的位置，有n+1种情况：在数组的0~n-1位置中和不在数组中。我们把每种情况下，查找需要遍历的元素个数累加起来，然后再除以n+1，就可以得到需要遍历的元素个数的平均值。

概率论中的*加权平均值*，也叫作*期望值*，所以平均时间复杂度的全称应该叫*加权平均时间复杂度*或者*期望时间复杂度*。

引入概率之后，前面那段代码的加权平均值为 (3n+1)/4。用大 O 表示法来表示，去掉系数和常量，这段代码的加权平均时间复杂度仍然是 O(n)。

### 均摊时间复杂度 ###

	 // array表示一个长度为n的数组
	 // 代码中的array.length就等于n
	 int[] array = new int[n];
	 int count = 0;
	 
	 void insert(int val) {
	    if (count == array.length) {
	       int sum = 0;
	       for (int i = 0; i < array.length; ++i) {
	          sum = sum + array[i];
	       }
	       array[0] = sum;
	       count = 1;
	    }
	
	    array[count] = val;
	    ++count;
	 }

1. find（）函数在极端情况下，复杂度才为O（1）；insert() 在大部分情况下，时间复杂度都为 O(1)。只有个别情况下，复杂度才比较高，为 O(n)。这是 insert()第一个区别于 find() 的地方。
2. 对于 insert() 函数来说，O(1) 时间复杂度的插入和 O(n) 时间复杂度的插入，出现的频率是非常有规律的，而且有一定的前后时序关系，一般都是一个 O(n) 插入之后，紧跟着 n-1 个 O(1) 的插入操作，循环往复。

针对这样一种特殊场景的复杂度分析，我们并不需要像之前讲平均复杂度分析方法那样，找出所有的输入情况及相应的发生概率，然后再计算加权平均值。

摊还分析法，通过摊还分析得到的时间复杂度我们起了一个名字，叫均摊时间复杂度。

均摊时间复杂度就是一种特殊的平均时间复杂度

### 内容小结 ###

复杂度分析方法分别有：最好情况时间复杂度、最坏情况时间复杂度、平均情况时间复杂度、均摊时间复杂度。

### 课后思考 ###

分析一下下面这个 add() 函数的时间复杂度

	// 全局变量，大小为10的数组array，长度len，下标i。
	int array[] = new int[10]; 
	int len = 10;
	int i = 0;
	
	// 往数组中添加一个元素
	void add(int element) {
	   if (i >= len) { // 数组空间不够了
	     // 重新申请一个2倍大小的数组空间
	     int new_array[] = new int[len*2];
	     // 把原来array数组中的数据依次copy到new_array
	     for (int j = 0; j < len; ++j) {
	       new_array[j] = array[j];
	     }
	     // new_array复制给array，array现在大小就是2倍len了
	     array = new_array;
	     len = 2 * len;
	   }
	   // 将element放到下标为i的位置，下标i加一
	   array[i] = element;
	   ++i;
	}

基础篇

## 05 | 数组：为什么很多编程语言中数组都从0开始编号？ ##

*为什么数组要从 0 开始编号，而不是从 1 开始呢？*

### 如何实现随机访问？ ###

数组（Array）是一种线性表数据结构。它用一组连续的内存空间，来存储一组具有相同类型的数据。

1.* 线性表（Linear List）*：线性表就是数据排成像一条线一样的结构。每个线性表上的数据最多只有前和后两个方向。其实除了数组，链表、队列、栈等也是线性表结构。*非线性表*，比如二叉树、堆、图等。在非线性表中，数据之间并不是简单的前后关系。
2. *连续的内存空间和相同类型的数据*。堪称“杀手锏”的特性：**“随机访问”**，但随之带来让数组的很多操作变得非常低效。

### 低效的“插入”和删除 ###

#### 插入操作 ####

* 如果插入数组末尾的数据，则最好情况时间复杂度为 O(1)；
* 如果插入开头的数据，则最坏情况时间复杂度为 O(n)；
* 平均情况时间复杂度也为 O(n)。

如果是有序数组，在某个位置插入一个新的元素，必需按照搬移k之后的数据进行操作；但如果是无序数组，直接将第 k 位的数据搬移到数组元素的最后，把新的元素直接放入第 k 个位置（避免出现大规模数据迁移）。

	假设数组 a[10]中存储了如下 5 个元素：a，b，c，d，e。我们现在需要将元素 x 插入到第 3 个位置。我们只需要将 c 放入到 a[5]，将 a[2]赋值为 x 即可。最后，数组中的元素如下： a，b，x，d，e，c。

![3f70b4ad9069ec568a2caaddc231b7dc.jpg](img/3f70b4ad9069ec568a2caaddc231b7dc.jpg)

	利用这种处理技巧，在特定场景下，在第 k 个位置插入一个元素的时间复杂度就会降为 O(1)。

#### 删除操作 ####

* 如果删除数组末尾的数据，则最好情况时间复杂度为 O(1)；
* 如果删除开头的数据，则最坏情况时间复杂度为 O(n)；
* 平均情况时间复杂度也为 O(n)。

在某些特殊的场景下，不一定非得追求数组中数据的连续性。如果我们将多次删除操作集中在一起执行，删除的效率是不是会提高很多呢？

	数组 a[10]中存储了 8 个元素：a，b，c，d，e，f，g，h。现在，我们要依次删除 a，b，c 三个元素。

![b69b8c5dbf6248649ddab7d3e7cfd7e5.jpg](img/b69b8c5dbf6248649ddab7d3e7cfd7e5.jpg)

	为了避免d，e，f，g，h这几个数据会被搬移三次，先记录下已经删除的数据。每次的删除操作并不是真正地搬移数据，只是记录数据已经被删除。。当数组没有更多空间存储数据时，我们再触发执行一次真正的删除操作。（JVM标记清除垃圾回收算法的核心思想）

很多时候我们并不是要去死记硬背某个数据结构或者算法，而是要学习它背后的思想和处理技巧，这些东西才是最有价值的。

### 警惕数组的访问越界问题 ###

	int main(int argc, char* argv[]){
	    int i = 0;
	    int arr[3] = {0};
	    for(; i<=3; i++){
	        arr[i] = 0;
	        printf("hello world\n");
	    }
	    return 0;
	}

### 容器能否完全替代数组？ ###

容器的优点：

1. 封装细节
2. 支持动态扩容

虽然容器支持动态扩容，但涉及内存申请和数据搬移（比较耗时）。以，如果事先能确定需要存储的数据大小，最好在创建 ArrayList 的时候事先指定数据大小。

1. Java ArrayList 无法存储基本类型，比如 int、long，需要封装为 Integer、Long 类，而 Autoboxing、Unboxing 则有一定的性能消耗，所以如果特别关注性能，或者希望使用基本类型，就可以选用数组。
2. 如果数据大小事先已知，并且对数据的操作非常简单，用不到 ArrayList 提供的大部分方法，也可以直接使用数组。
3. 还有一个是我个人的喜好，当要表示多维数组时，用数组往往会更加直观。比如 `Object[][] array`；而用容器的话则需要这样定义：`ArrayList<ArrayList<object>> array。`

### 解答开题 ###

为什么大多数编程语言中，数组要从 0 开始编号，而不是从 1 开始呢？

1. 从数组存储的内存模型上来看，“下标”最确切的定义应该是“偏移（offset）”如果用 a 来表示数组的首地址，a[0]就是偏移为 0 的位置，也就是首地址，a[k]就表示偏移 k 个 type_size 的位置，所以计算 a[k]的内存地址只需要用这个公式：`a[k]_address = base_address + k * type_size`。但是，如果数组从 1 开始计数，那我们计算数组元素 a[k]的内存地址就会变为：`a[k]_address = base_address + (k-1)*type_size`（对CPU来说，多一个减法指令）
2. 历史原因

### 内容小结 ###

数组：最基础、最简单的数据结构；数组用一块连续的内存空间，来存储相同类型的一组数据，最大的特点就是支持随机访问，但插入、删除操作也因此变得比较低效，平均情况时间复杂度为 O(n)。

1. 在平时的业务开发中，我们可以直接使用编程语言提供的容器类
2. 如果是特别底层的开发，直接使用数组可能会更合适

### 课后思考 ###

1. 前面我基于数组的原理引出 JVM 的标记清除垃圾回收算法的核心理念。我不知道你是否使用 Java 语言，理解 JVM，如果你熟悉，可以在评论区回顾下你理解的标记清除垃圾回收算法。
2. 前面我们讲到一维数组的内存寻址公式，那你可以思考一下，类比一下，二维数组的内存寻址公式是怎样的呢？

## 06 | 链表（上）：如何实现LRU缓存淘汰算法？ ##

如何用链表来实现 LRU 缓存淘汰策略呢？

### 五花八门的链表结构 ###

#### 底层的存储结构 ####

* 数组：需要**一块连续的内存空间**
* 链表：通过“指针”将一组**零散的内存块**串联起来使用，其中，我们把内存块称为链表的“结点”。为了将所有的结点串起来，每个链表的结点除了存储数据之外，还需要记录链上的下一个结点的地址（后继指针next）。

两个特殊结点：

* 头结点：第一个结点，记录链表的基地址
* 尾结点：最后一个结点，不是指向下一个结点，而是指向一个空地址 NULL

优点和缺点

* 链表的存储空间本身不是连续的，所以插入和删除一个数据是非常快速的。
* 链表要想随机访问第 k 个元素，就没有数组那么高效了。因为链表中的数据并非连续存储的，（根据首地址和下标，通过寻址公式就能直接计算出对应的内存地址——数组的优点），而是需要根据指针一个结点一个结点地依次遍历，直到找到相应的结点。

#### 循环链表 ####

循环链表是一种特殊的单链表

优点：从链尾到链头比较方便：当要处理的数据具有环型结构特点时，就特别适合采用循环链表

#### 双向链表 ####

双向链表，顾名思义，它支持两个方向，每个结点不止有一个后继指针 next 指向后面的结点，还有一个前驱指针 prev 指向前面的结点。

从链表中删除一个数据：

* 删除结点中“值等于某个给定值”的结点
* 删除给定指针指向的结点：

**删除结点中“值等于某个给定值”的结点**

需要一个结点一个结点遍历，尽管单纯的删除操作时间复杂度是 O(1)，但遍历查找的时间是主要的耗时点，对应的时间复杂度为 O(n)。根据时间复杂度分析中的加法法则，删除值等于给定值的结点对应的链表操作的总时间复杂度为 O(n)。

**删除给定指针指向的结点**

因为双向链表中的结点已经保存了前驱结点的指针，不需要像单链表那样遍历。

Java的实现，LinkedHashMap


**空间换时间**的设计思想，当内存空间充足的时候，如果我们更加追求代码的执行速度，我们就可以选择空间复杂度相对较高、但时间复杂度相对很低的算法或者数据结构。相反，如果内存比较紧缺，比如代码跑在手机或者单片机上，这个时候，就要反过来用时间换空间的设计思路。

扩展：双向循环链表

### 链表 VS 数组性能大比拼 ###

* 数组：简单易用，在实现上使用的是连续的内存空间，可以借助 CPU 的缓存机制，预读数组中的数据，所以访问效率更高。
* 链表：链表在内存中并不是连续存储，所以对 CPU 缓存不友好，没办法有效预读。
* 数组：大小固定，如果声明的数组过大，系统可能没有足够的连续内存空间分配给它，导致“内存不足（out of memory）”。如果声明的数组过小，则可能出现不够用的情况。这时只能再申请一个更大的内存空间，把原数组拷贝进去，非常费时。
* 链表：链表本身没有大小的限制，天然地支持动态扩容

### 解答开篇 ###



## 07 | 链表（下）：如何轻松写出正确的链表代码？ ##

### 技巧一：理解指针或引用的含义 ###

*将某个变量赋值给指针，实际上就是将这个变量的地址赋值给指针，或者反过来说，指针中存储了这个变量的内存地址，指向了这个变量，通过指针就能找到这个变量。*

### 技巧二：警惕指针丢失和内存泄漏 ###

* 插入结点时，一定要注意操作的顺序
* 删除链表结点时，也一定要记得手动释放内存空间

### 技巧三：利用哨兵简化实现难度 ###

head 指针都会一直指向这个哨兵结点。我们也把这种有哨兵结点的链表叫带头链表。相反，没有哨兵结点的链表就叫作不带头链表。

### 技巧四：重点留意边界条件处理 ###

要实现没有 Bug 的链表代码，一定要在编写的过程中以及编写完成之后，检查边界条件是否考虑全面，以及代码在边界条件下是否能正确运行。

检查链表代码是否正确的边界条件有这样几个：

* 如果链表为空时，代码是否能正常工作？
* 如果链表只包含一个结点时，代码是否能正常工作？
* 如果链表只包含两个结点时，代码是否能正常工作？
* 代码逻辑在处理头结点和尾结点的时候，是否能正常工作？

不光光是写链表代码，你在写任何代码时，也千万不要只是实现业务正常情况下的功能就好了，一定要多想想，你的代码在运行的时候，可能会遇到哪些边界情况或者异常情况。遇到了应该如何应对，这样写出来的代码才够健壮！

### 技巧五：举例画图，辅助思考 ###

“举例法”和“画图法”

### 技巧六：多写多练，没有捷径 ###

* 单链表反转
* 链表中环的检测
* 两个有序的链表合并
* 删除链表倒数第 n 个结点
* 求链表的中间结点

### 内容小结 ###

* 理解指针或引用的含义
* 警惕指针丢失和内存泄漏
* 利用哨兵简化实现难度
* 重点留意边界条件处理
* 以及举例画图、辅助思考
* 多写多练。

写链表代码是最考验逻辑思维能力的。

### 课后思考 ###

今天我们讲到用哨兵来简化编码实现，你是否还能够想到其他场景，利用哨兵可以大大地简化编码难度？

## 08 | 栈：如何实现浏览器的前进和后退功能？ ##

### 如何理解“栈”？ ###

后进者先出，先进者后出，这就是典型的“栈”结构。

* 操作特性：栈是一种“操作受限”的线性表
* 功能：数组和链表都可以替代栈，但操作上灵活自由，使用时比较不可控

当某个数据集合只涉及在一端插入和删除数据，并且满足后进先出、先进后出的特性，我们就应该首选“栈”这种数据结构。

### 如何实现一个“栈”？ ###

* 顺序栈
* 链式栈

不管是顺序栈还是链式栈，我们存储数据只需要一个大小为 n 的数组就够了。在入栈和出栈过程中，只需要一两个临时变量存储空间，所以空间复杂度是 O(1)。

注意，这里存储数据需要一个大小为 n 的数组，并不是说空间复杂度就是 O(n)。

### 支持动态扩容的顺序栈 ###

对于出栈操作来说，我们不会涉及内存的重新申请和数据的搬移，所以出栈的时间复杂度仍然是 O(1)。

对于入栈操作来说，情况就不一样了。当栈中有空闲空间时，入栈操作的时间复杂度为 O(1)。但当空间不够时，就需要重新申请内存和数据搬移，所以时间复杂度就变成了 O(n)。

* 栈空间不够时，我们重新申请一个是原来大小两倍的数组；
* 为了简化分析，假设只有入栈操作没有出栈操作；
* 定义不涉及内存搬移的入栈操作为 simple-push 操作，时间复杂度为 O(1)。

这 K 次入栈操作，总共涉及了 K 个数据的搬移，以及 K 次 simple-push 操作。将 K 个数据搬移均摊到 K 次入栈操作，那每个入栈操作只需要一个数据搬移和一个 simple-push 操作。以此类推，入栈操作的均摊时间复杂度就为 O(1)。

均摊时间复杂度一般都等于最好情况时间复杂度。入栈操作的时间复杂度 O 都是 O(1)，只有在个别时刻才会退化为 O(n)，所以把耗时多的入栈操作的时间均摊到其他入栈操作上，平均情况下的耗时就接近 O(1)。

### 栈在函数调用中的应用 ###

函数调用栈

操作系统给每个线程分配了一块独立的内存空间，这块内存被组织成“栈”这种结构, 用来存储函数调用时的临时变量。每进入一个函数，就会将临时变量作为一个栈帧入栈，当被调用函数执行完成，返回之后，将这个函数对应的栈帧出栈。

### 栈在表达式求值中的应用 ###

表达式求值。

1. 实际上，编译器就是通过两个栈来实现的。其中一个保存操作数的栈，另一个是保存运算符的栈。我们从左向右遍历表达式，当遇到数字，我们就直接压入操作数栈；当遇到运算符，就与运算符栈的栈顶元素进行比较。
2. 如果比运算符栈顶元素的优先级高，就将当前运算符压入栈；如果比运算符栈顶元素的优先级低或者相同，从运算符栈中取栈顶运算符，从操作数栈的栈顶取 2 个操作数，然后进行计算，再把计算完的结果压入操作数栈，继续比较。

### 栈在括号匹配中的应用 ###

借助栈来检查表达式中的括号是否匹配。

* 用栈来保存未匹配的左括号，从左到右依次扫描字符串。当扫描到左括号时，则将其压入栈中；当扫描到右括号时，从栈顶取出一个左括号。
* 如果能够匹配，比如“(”跟“)”匹配，“[”跟“]”匹配，“{”跟“}”匹配，则继续扫描剩下的字符串。
* 如果扫描的过程中，遇到不能配对的右括号，或者栈中没有数据，则说明为非法格式。

### 解答开题 ###

### 内容小结 ###

栈是一种操作受限的数据结构，只支持入栈和出栈操作。后进先出是它最大的特点。栈既可以通过数组实现，也可以通过链表来实现。不管基于数组还是链表，入栈、出栈的时间复杂度都为 O(1)。

一种支持动态扩容的顺序栈，需要重点掌握它的均摊时间复杂度分析方法。

### 课后思考 ###

1. 我们在讲栈的应用时，讲到用函数调用栈来保存临时变量，为什么函数调用要用“栈”来保存临时变量呢？用其他数据结构不行吗？
2. 我们都知道，JVM 内存管理中有个“堆栈”的概念。栈内存用来存储局部变量和方法调用，堆内存用来存储 Java 中的对象。那 JVM 里面的“栈”跟我们这里说的“栈”是不是一回事呢？如果不是，那它为什么又叫作“栈”呢？

答：

1. 栈更符合计算机逻辑，跟~~命名空间~~作用域有关，~~越早入栈的变量，范围越大，越迟出栈~~栈的特性正好跟作用域的特性接近
2. ~~不是，只是JVM的栈，应该更类似数组。相当于值类型，模仿计算机的栈。~~

### 精选留言 ###

#### 1 ####

为什么函数调用要用“栈”来保存临时变量呢？用其他数据结构不行吗？

其实，我们不一定非要用栈来保存临时变量，只不过如果这个函数调用符合后进先出的特性，用栈这种数据结构来实现，是最顺理成章的选择。

从调用函数进入被调用函数，对于数据来说，变化的是什么呢？是作用域。所以根本上，只要能保证每进入一个新的函数，都是一个新的作用域就可以。而要实现这个，用栈就非常方便。在进入被调用函数的时候，分配一段栈空间给这个函数的变量，在函数结束的时候，将栈顶复位，正好回到调用函数的作用域内。

#### 2 ####



## 09 | 队列：队列在线程池等有限资源池中的应用 ##

当我们向固定大小的线程池中请求一个线程时，如果线程池中没有空闲资源了，这个时候线程池如何处理这个请求？是拒绝请求还是排队请求？各种处理策略又是怎么实现的呢？

### 如何理解“队列”？ ###

先进者先出，这就是典型的“队列”

* 栈只支持两个基本操作：入栈 push()和出栈 pop()
* 队列只支持两个基本操作：入队 enqueue()和出队 dequeue()

队列和栈一样，也是一种操作受限的线性表数据结构

高性能队列 Disruptor、Linux 环形缓存，都用到了循环并发队列；
Java concurrent 并发包利用 ArrayBlockingQueue 来实现公平锁等

### 顺序队列和链式队列 ###

* 对于栈来说，只需要一个栈顶指针就可以了
* 队列需要两个指针：一个是head指针，指向队头；一个是tail指针，指向队尾。

### 循环队列 ###

确定好队空和队满的判定条件。

* 队列为空的判断条件仍然是 head == tail
* 当队满时，(tail+1)%n=head

### 阻塞队列和并发队列 ###

阻塞队列其实就是在队列基础上增加了阻塞操作。你应该已经发现了，上述的定义就是一个“生产者 - 消费者模型”！是的，我们可以使用阻塞队列，轻松实现一个“生产者 - 消费者模型”！

线程安全的队列我们叫作并发队列。最简单直接的实现方式是直接在 enqueue()、dequeue() 方法上加锁，但是锁粒度大并发度会比较低，同一时刻仅允许一个存或者取操作。实际上，基于数组的循环队列，利用 CAS 原子操作，可以实现非常高效的并发队列。这也是循环队列比链式队列应用更加广泛的原因。在实战篇讲 Disruptor 的时候，我会再详细讲并发队列的应用。

### 解答开篇 ###

实际上，对于大部分资源有限的场景，当没有空闲资源时，基本上都可以通过“队列”这种数据结构来实现请求排队。

### 内容小结 ###

队列最大的特点就是先进先出，主要的两个操作是入队和出队。跟栈一样，它既可以用数组来实现，也可以用链表来实现。用数组实现的叫顺序队列，用链表实现的叫链式队列。

循环队列是我们这节的重点。要想写出没有 bug 的循环队列实现代码，关键要确定好队空和队满的判定条件，具体的代码你要能写出来。

还讲了几种高级的队列结构，阻塞队列、并发队列，底层都还是队列这种数据结构，只不过在之上附加了很多其他功能。阻塞队列就是入队、出队操作可以阻塞，并发队列就是队列的操作多线程安全。

### 课后思考 ###

1. 除了线程池这种池结构会用到队列排队请求，你还知道有哪些类似的池结构或者场景中会用到队列的排队请求呢？
2. 今天讲到并发队列，关于如何实现无锁并发队列，网上有非常多的讨论。对这个问题，你怎么看呢？

## 10 |递归：如何用三行代码找到“最终推荐人” ##

给定一个用户 ID，如何查找这个用户的“最终推荐人”

### 如何理解“递归”？ ###

最难理解的点

1. 动态规划
2. 递归

递归是一种应用非常广泛的算法（或者编程技巧）。很多数据结构和算法的编码实现都要用到递归，比如 DFS 深度优先搜索、前中后序二叉树遍历等等。

	f(n)=f(n-1)+1 其中，f(1)=1

### 递归需要满足的三个条件 ###

1. 一个问题的解可以分解为几个子问题的解

2. 这个问题与分解之后的子问题，除了数据规模之外，求解思路完全一样

3. 存在递归终止条件

### 如何编写递归代码？ ###

写出递推公式，找到终止条件

写递归代码的关键就是找到如何将大问题分解为小问题的规律，并且基于此写出递推公式，然后再推敲终止条件，最后将递推公式和终止条件翻译成代码。

如果一个问题 A 可以分解为若干子问题 B、C、D，你可以假设子问题 B、C、D 已经解决，在此基础上思考如何解决问题 A。而且，你只需要思考问题 A 与子问题 B、C、D 两层之间的关系即可，不需要一层一层往下思考子问题与子子问题，子子问题与子子子问题之间的关系。屏蔽掉递归细节，这样子理解起来就简单多了。

编写递归代码的关键是，只要遇到递归，我们就把它抽象成一个递推公式，不用想一层层的调用关系，不要试图用人脑去分解递归的每个步骤。

### 递归代码要警惕堆栈溢出 ###

函数调用会使用栈来保存临时变量。每调用一个函数，都会将临时变量封装为栈帧压入内存栈，等函数执行完成返回时，才出栈。系统栈或者虚拟机栈空间一般都不大。如果递归求解的数据规模很大，调用层次很深，一直压入栈，就会有堆栈溢出的风险。

如何避免出现堆栈溢出

1. 通过在代码中限制递归调用的最大深度的方式来解决这个问题

### 递归代码要警惕重复计算 ###

为了避免重复计算，我们可以通过一个数据结构（比如散列表）来保存已经求解过的 f(k)。当递归调用到 f(k) 时，先看下是否已经求解过了。如果是，则直接从散列表中取值返回。

在时间效率上，递归代码里多了很多函数调用，当这些函数调用的数量较大时，就会积聚成一个可观的时间成本。在空间复杂度上，因为递归调用一次就会在内存栈中保存一次现场数据，所以在分析递归代码空间复杂度时，需要额外考虑这部分的开销，比如我们前面讲到的电影院递归代码，空间复杂度并不是 O(1)，而是 O(n)。

### 怎么将递归代码改写为非递归代码？ ###

递归有利有弊：

* 利是递归代码的表达力很强，写起来非常简洁；
* 弊就是空间复杂度高、有堆栈溢出的风险、存在重复计算、过多的函数调用会耗时较多等问题。

因为递归本身就是借助栈来实现的，只不过我们使用的栈是系统或者虚拟机本身提供的，我们没有感知罢了。如果我们自己在内存堆上实现栈，手动模拟入栈、出栈过程，这样任何递归代码都可以改写成看上去不是递归代码的样子。

即：*所有的递归代码都可以改为这种迭代循环的非递归写法*

### 解答开篇 ###

1. 如果递归很深，可能会有堆栈溢出的问题。
2. 如果数据库里存在脏数据，我们还需要处理由此产生的无限递归问题。比如 demo 环境下数据库中，测试工程师为了方便测试，会人为地插入一些数据，就会出现脏数据。如果 A 的推荐人是 B，B 的推荐人是 C，C 的推荐人是 A，这样就会发生死循环。

第一个问题，我前面已经解答过了，可以用限制递归深度来解决。第二个问题，也可以用限制递归深度来解决。不过，还有一个更高级的处理方法，就是自动检测 A-B-C-A 这种“环”的存在。

### 内容小结 ###

递归是一种非常高效、简洁的编码技巧。只要是满足“三个条件”的问题就可以通过递归代码来解决。

1. 写出递推公式
2. 找出终止条件
3. 然后再翻译成递归代码。

递归代码虽然简洁高效，但是，递归代码也有很多弊端。比如，堆栈溢出、重复计算、函数调用耗时多、空间复杂度高等，所以，在编写递归代码的时候，一定要控制好这些副作用。

### 课后思考 ###

我们平时调试代码喜欢使用 IDE 的单步跟踪功能，像规模比较大、递归层次很深的递归代码，几乎无法使用这种调试方式。对于递归代码，你有什么好的调试方法呢？

## 11 | 排序（上）：为什么插入排序比冒泡排序更受欢迎？ ##

最常用的：冒泡排序、插入排序、选择排序、归并排序、快速排序、计数排序、基数排序、桶排序

插入排序和冒泡排序的时间复杂度相同，都是 O(n2)，在实际的软件开发里，为什么我们更倾向于使用插入排序算法而不是冒泡排序算法呢？

### 如何分析一个“排序算法”？ ###

#### 排序算法的执行效率 ####

**1. 最好情况、最坏情况、平均情况时间复杂度**

在分析排序算法的时间复杂度时，要分别给出最好情况、最坏情况、平均情况下的时间复杂度。除此之外，你还要说出最好、最坏时间复杂度对应的要排序的原始数据是什么样的。

* 有些排序算法会区分，为了好对比，所以我们最好都做一下区分。
* 对于要排序的数据，有的接近有序，有的完全无序。有序度不同的数据，对于排序的执行时间肯定是有影响的，

**2. 时间复杂度的系数、常数 、低阶**

时间复杂度反应的是数据规模 n 很大的时候的一个增长趋势，所以它表示的时候会忽略系数、常数、低阶。

**3. 比较次数和交换（或移动）次数**

如果我们在分析排序算法的执行效率的时候，应该把比较次数和交换（或移动）次数也考虑进去。

### 排序算法的内存消耗 ###

原地排序（Sorted in place）。原地排序算法，就是特指空间复杂度是 O(1) 的排序算法。

### 排序算法的稳定性 ###

仅仅用执行效率和内存消耗来衡量排序算法的好坏是不够的。针对排序算法，我们还有一个重要的度量指标，稳定性。如果待排序的序列中存在值相等的元素，经过排序之后，相等元素之间原有的先后顺序不变。

稳定排序算法可以保持金额相同的两个对象，在排序之后的前后顺序不变。

### 冒泡算法 ###

**第一，冒泡排序是原地排序算法吗？**

空间复杂度为 O(1)，是一个原地排序算法。

**第二，冒泡排序是稳定的排序算法吗？**

稳定的排序算法

**第三，冒泡排序的时间复杂度是多少**

* 最好情况时间复杂度是 O(n)
* 最坏情况时间复杂度是 O(n^2)

平均时间复杂度就是加权平均期望时间复杂度，如果用概率论方法定量分析平均时间复杂度，涉及的数学推理和计算就会很复杂。

“有序度”和“逆序度”

**有序度**是数组中具有有序关系的元素对的个数。有序元素对用数学表达式表示就是这样：

	有序元素对：a[i] <= a[j], 如果i < j。

逆序度的定义正好跟有序度相反（默认从小到大为有序）
	
	逆序元素对：a[i] > a[j], 如果i < j。

**逆序度 = 满有序度 - 有序度。**

冒泡排序包含两个操作原子，比较和交换。每交换一次，有序度就加 1。不管算法怎么改进，交换次数总是确定的，即为**逆序度，也就是n*(n-1)/2–初始有序度**。

### 插入排序（Insertion Sort） ###

这是一个动态排序的过程，即动态地往有序集合中添加数据，我们可以通过这种方法保持集合中的数据一直有序。

已排序区间和未排序区间，初始已排序区间只有一个元素，就是数组的第一个元素。插入算法的核心思想是取未排序区间中的元素，在已排序区间中找到合适的插入位置将其插入，并保证已排序区间数据一直有序。重复这个过程，直到未排序区间中元素为空，算法结束。

**第一，插入排序是原地排序算法吗？**

原地排序算法

**第二，插入排序是稳定的排序算法吗？**

稳定的排序算法

**第三，插入排序的时间复杂度是多少？**

* 最好是时间复杂度为 O(n)，**从尾到头遍历已经有序的数据**
* 

### 选择排序（Selection Sort） ###

选择排序算法的实现思路有点类似插入排序，也分已排序区间和未排序区间。但是选择排序每次会从未排序区间中找到最小的元素，将其放到已排序区间的末尾。

1. 选择排序空间复杂度为 O(1)，原地排序算法
2. 选择排序的最好情况时间复杂度、最坏情况和平均情况时间复杂度都为 O(n2)。
3. 选择排序是一种不稳定的排序算法

### 解答开篇 ###

* 冒泡排序不管怎么优化，元素交换的次数是一个固定值，是原始数据的逆序度。
* 插入排序是同样的，不管怎么优化，元素移动的次数也等于原始数据的逆序度。

冒泡排序的数据交换要比插入排序的数据移动要复杂

所以，虽然冒泡排序和插入排序在时间复杂度上是一样的，都是 O(n2)，但是如果我们希望把性能优化做到极致，那肯定首选插入排序。

### 内容小结 ###

![348604caaf0a1b1d7fee0512822f0e50.jpg](/img/348604caaf0a1b1d7fee0512822f0e50.jpg)

这三种排序算法，实现代码都非常简单，对于小规模数据的排序，用起来非常高效。但是在大规模数据排序的时候，这个时间复杂度还是稍微有点高，所以更倾向于用时间复杂度为 O(nlogn) 的排序算法。

### 课后思考 ###

我们讲过，特定算法是依赖特定的数据结构的。我们今天讲的几种排序算法，都是基于数组实现的。如果数据存储在链表中，这三种排序算法还能工作吗？如果能，那相应的时间、空间复杂度又是多少呢？

## 12 | 排序（下）：如何用快排思想在O(n)内查找第K大元素？ ##

两种时间复杂度为 O(nlogn) 的排序算法，**归并排序和快速排序**。

如何在 O(n) 的时间复杂度内查找一个无序数组中的第 K 大元素？

### 归并排序的原理 ###

**归并排序**（Merge Sort）

归并排序的核心思想还是蛮简单的。如果要排序一个数组，我们先把数组从中间分成前后两部分，然后对前后两部分分别排序，再将排好序的两部分合并在一起，这样整个数组就都有序了。

归并排序使用的是分治思想。

分治是一种解决问题的处理思想，递归是一种编程技巧。归并排序用的是分治思想，可以用递归来实现。**我们现在就来看看如何用递归代码来实现归并排序**。

	递推公式：
	merge_sort(p…r) = merge(merge_sort(p…q), merge_sort(q+1…r))
	
	终止条件：
	p >= r 不用再继续分解

merge_sort(p…r) 表示，给下标从 p 到 r 之间的数组排序。我们将这个排序问题转化为了两个子问题，merge_sort(p…q) 和 merge_sort(q+1…r)，其中下标 q 等于 p 和 r 的中间位置，也就是 (p+r)/2。当下标从 p 到 q 和从 q+1 到 r 这两个子数组都排好序之后，我们再将两个有序的子数组合并在一起，这样下标从 p 到 r 之间的数据就也排好序了。

### 归并排序的性能分析 ###

#### 第一，归并排序是稳定的排序算法吗？ ####

归并排序稳不稳定关键要看 merge() 函数，也就是两个有序子数组合并成一个有序数组的那部分代码。

归并排序是一个稳定的排序算法。

#### 第二，归并排序的时间复杂度是多少？ ####

不仅递归求解的问题可以写成递推公式，递归代码的时间复杂度也可以写成递推公式。

T(n) 就等于 O(nlogn)。所以归并排序的时间复杂度是 O(nlogn)。从我们的原理分析和伪代码可以看出，归并排序的执行效率与要排序的原始数组的有序程度无关，所以其时间复杂度是非常稳定的，不管是最好情况、最坏情况，还是平均情况，时间复杂度都是 O(nlogn)。

#### 第三，归并排序的空间复杂度是多少？ ####

归并排序的时间复杂度任何情况下都是 O(nlogn)。归并排序并没有像快排那样，应用广泛，这是为什么呢？因为它有一个致命的“弱点”，那就是归并排序不是原地排序算法。

这是因为归并排序的合并函数，在合并两个有序数组为一个有序数组时，需要借助额外的存储空间。

尽管每次合并操作都需要申请额外的内存空间，但在合并完成之后，临时开辟的内存空间就被释放掉了。在任意时刻，CPU 只会有一个函数在执行，也就只会有一个临时的内存空间在使用。临时内存空间最大也不会超过 n 个数据的大小，所以空间复杂度是 O(n)。

### 快速排序的原理 ###

快速排序算法（Quicksort），快排利用的也是分治思想。

快排的思想是这样的：如果要排序数组中下标从 p 到 r 之间的一组数据，我们选择 p 到 r 之间的任意一个数据作为 pivot（分区点）。

归并排序的处理过程是由下到上的，先处理子问题，然后再合并。而快排正好相反，它的处理过程是由上到下的，先分区，然后再处理子问题。

### 快速排序的性能分析 ###

快排是一种原地、不稳定的排序算法。

快排也是用递归来实现的。如果每次分区操作，都能正好把数组分成大小接近相等的两个小区间，那快排的时间复杂度递推求解公式跟归并是相同的。所以，快排的时间复杂度也是 O(nlogn)。

T(n) 在大部分情况下的时间复杂度都可以做到 O(nlogn)，只有在极端情况下，才会退化到 O(n2)。

### 解答开篇 ###

快排核心思想就是**分治**和**分区**

### 内容小结 ###

归并排序和快速排序是两种稍微复杂的排序算法，它们用的都是分治的思想，代码都通过递归来实现，过程非常相似。理解归并排序的重点是理解递推公式和 merge() 合并函数。同理，理解快排的重点也是理解递推公式，还有 partition() 分区函数。

归并排序和快速排序是两种稍微复杂的排序算法，它们用的都是分治的思想，代码都通过递归来实现，过程非常相似。理解归并排序的重点是理解递推公式和 merge() 合并函数。同理，理解快排的重点也是理解递推公式，还有 partition() 分区函数。

快速排序算法虽然最坏情况下的时间复杂度是 O(n2)，但是平均情况下时间复杂度都是 O(nlogn)。不仅如此，快速排序算法时间复杂度退化到 O(n2) 的概率非常小，我们可以通过合理地选择 pivot 来避免这种情况。

### 课后思考 ###

## 13 | 线性排序：如何根据年龄给100万用户数据排序？ ##

桶排序、计数排序、基数排序

* 时间复杂度是线性的，线性排序
* 不涉及元素之间的比较操作

如何根据年龄给 100 万用户排序？ 

### 桶排序 ###

会用到“桶”，核心思想是将要排序的数据分到几个有序的桶里，每个桶里的数据再单独进行排序。桶内排完序之后，再把每个桶里的数据按照顺序依次取出，组成的序列就是有序的了。

如果要排序的数据有 n 个，我们把它们均匀地划分到 m 个桶内，每个桶里就有 k=n/m 个元素。每个桶内部使用快速排序，时间复杂度为 O(k * logk)。m 个桶排序的时间复杂度就是 O(m * k * logk)，因为 k=n/m，所以整个桶排序的时间复杂度就是 O(n*log(n/m))。当桶的个数 m 接近数据个数 n 时，log(n/m) 就是一个非常小的常量，这个时候桶排序的时间复杂度接近 O(n)。

**桶排序看起来很优秀，那它是不是可以替代我们之前讲的排序算法呢？**

不能

1. 需要划分成m个桶
2. 数据在各个桶之间的分布是比较均匀的，在极端情况下，如果数据都被划分到一个桶里，那就退化为 O(nlogn) 的排序算法了。

*桶排序比较适合用在外部排序中。*

### 计数排序（Counting sort） ###

*计数排序其实是桶排序的一种特殊情况。*

**不过，为什么这个排序算法叫“计数”排序呢？“计数”的含义来自哪里呢？**

*计数排序只能用在数据范围不大的场景中，如果数据范围 k 比要排序的数据 n 大很多，就不适合用计数排序了。而且，计数排序只能给非负整数排序，如果要排序的数据是其他类型的，要将其在不改变相对大小的情况下，转化为非负整数。* 如果将所有的分数都先乘以 10转化成整数；如果要排序的数据中有负数，数据的范围是[-1000, 1000]，那我们就需要先对每个数据都加 1000，转化成非负整数。

### 基数排序（Radix sort） ###

这里按照每位来排序的排序算法要是稳定的。先按照最后一位来排序手机号码，然后，再按照倒数第二位重新排序，以此类推，最后按照第一位重新排序。经过 11 次排序之后，手机号码就都有序了。

我们可以把所有的单词补齐到相同长度，位数不够的可以在后面补“0”

基数排序对要排序的数据是有要求的，需要可以分割出独立的“位”来比较，而且位之间有递进的关系，如果 a 数据的高位比 b 数据大，那剩下的低位就不用比较了。除此之外，每一位的数据范围不能太大，要可以用线性排序算法来排序，否则，基数排序的时间复杂度就无法做到 O(n) 了。

### 解答开题 ###

### 内容小结 ###

 3 种排序算法，有桶排序、计数排序、基数排序

1. 线性时间复杂度
2. 对要排序的数据都有比较苛刻的要求，如果数据特征比较符合这些排序算法的要求，应用这些算法，会非常高效，线性时间复杂度可以达到 O(n)。
3. 桶排序和计数排序的排序思想是非常相似的，都是针对范围不大的数据，将数据划分成不同的桶来实现排序；基数排序要求数据可以划分成高低位，位之间有递进关系。比较两个数，我们只需要比较高位，高位相同的再比较低位。而且每一位的数据范围不能太大，因为基数排序算法需要借助桶排序或者计数排序来完成每一个位的排序工作。

### 课后思考 ###

## 14 | 排序优化 ##

如何实现一个通用的、高性能的排序函数？

### 如何选择合适的排序算法？ ###



## 15 | 二分查找（上） ##

假设我们有 1000 万个整数数据，每个数据占 8 个字节，如何设计数据结构和算法，快速判断某个整数是否出现在这 1000 万数据中？ 

### 无处不在的二分查找 ###

二分查找针对的是一个有序的数据集合，查找思想有点类似分治思想。每次都通过跟区间的中间元素对比，将待查找的区间缩小为之前的一半，直到找到要查找的元素，或者区间被缩小为 0。

### O（logn）惊人的查找速度 ###

时间复杂度是O(logn)的算法，可能比常量级O(1)的算法还要高效。因为O(1)有可能表示的十一个非常大的常量值。

另外，指数时间复杂度的算法在大规模数据面前是无效的。

### 二分查找的递归与非递归实现 ###

最简单的情况就是有序数组中不存在重复元素

容易出错的3个地方

#### 1. 循环退出条件 ####

low<=high

#### 2. mid的取值 ####

`mid =（low+high/2`，low和high比较大的话，两者之和有可能会溢出。 可以改进为`low+(high-low)/2`甚至 `low+((high-low)>>1)`

#### 3. low和high的更新 ####

low=mid+1，high=mid-1。注意这里的 +1 和 -1，如果直接写成 low=mid 或者 high=mid，就可能会发生死循环。比如，当 high=3，low=3 时，如果 a[3]不等于 value，就会导致一直循环不退出。

实际上，二分查找除了用循环来实现，还可以用递归来实现

### 二分查找应用场景的局限性 ###

二分查找的时间复杂度是 O(logn)，查找数据的效率非常高。

#### 首先，二分查找依赖的是顺序表结构，简单点说就是数组 ####

二分查找无法依赖于链表，如果使用链表来查找时间复杂度会十分高。

二分查找只能用在数据是通过顺序表来存储的数据结构上。如果你的数据是通过其他数据结构存储的，则无法应用二分查找。

#### 其次，二分查找针对的是有序数据。 ####

1. 数据必须是有序的。如果数据没有序，我们需要先排序。
2. 如果数据集合有频繁的插入和删除操作，针对这种动态数据集合，维护有序的成本很高
3. 二分查找只能用在插入、删除操作不频繁，一次排序多次查找的场景中。

#### 再次，数据量太小不适合二分查找。 ####

* 要处理得数据量很小，没有必要使用二分查找，使用顺序遍历便可
* 但如果数据之间的比较操作非常耗时，不管数据量大小，都推荐使用二分查找。

#### 最后，数据量太大也不适合二分查找。 ####

二分查找的底层需要依赖数组这种数据结构，而数组为了支持随机访问的特性，要求内存空间连续，对内存的要求比较苛刻。比如，我们有 1GB 大小的数据，如果希望用数组来存储，那就需要 1GB 的连续内存空间。

### 解答开题 ###

### 内容小结 ###

针对*有序*数据的高效查找算法，二分查找，它的时间复杂度是* O(logn)*。

二分查找的核心思想理解起来非常简单，有点类似分治思想。即每次都通过跟区间中的中间元素对比，将待查找的区间缩小为一半，直到找到要查找的元素，或者区间被缩小为 0。但是二分查找的代码实现比较容易写错。你需要着重掌握它的三个容易出错的地方：

1. 循环退出条件
2. mid 的取值
3. low 和 high 的更新。

二分查找虽然性能比较优秀，但应用场景也比较有限。

1. 底层必须依赖数组，并且还要求数据是有序的。
2. 对于较小规模的数据查找，我们直接使用顺序遍历就可以了，二分查找的优势并不明显。
3. 二分查找更适合处理静态数据，也就是没有频繁的数据插入、删除操作。

### 课后思考 ###

## 16 | 二分查找（下）	 ##

假设我们有 12 万条这样的 IP 区间与归属地的对应关系，如何快速定位出一个 IP 地址的归属地呢？

4种常见的二分查找变形问题

* 查找第一个值等于给定值的元素
* 查找最后一个值等于给定值的元素
* 查找第一个大于等于给定值的元素
* 查找最后一个小于等于给定值的元素

### 变体一：查找第一个值等于给定值的元素 ###

很多人都觉得变形的二分查找很难写，主要原因是太追求第一种那样完美、简洁的写法。

### 变体二：查找最后一个值等于给定值的元素 ###

### 变体三：查找第一个大于等于给定值的元素 ###

### 变体四：查找最后一个小于等于给定值的元素 ###

### 解答开篇 ###

如何快速定位出一个 IP 地址的归属地？

1. 如果 IP 区间与归属地的对应关系不经常更新，我们可以先预处理这 12 万条数据，让其按照起始 IP 从小到大排序。(IP 地址可以转化为 32 位的整型数。所以，我们可以将起始地址，按照对应的整型值的大小关系，从小到大进行排序)
2. 转化：第四种变形问题“在有序数组中，查找最后一个小于等于某个给定值的元素”了
3. 先通过二分查找，找到最后一个起始 IP 小于等于这个 IP 的 IP 区间
	* 检查IP是否在这个IP区间内，如果在，取出对应的归属地显示
	* 如果不在，就返回未查找到

### 内容小结 ###

凡是用二分查找能解觉得，绝大部分倾向于用散列表或者二叉查找树。

求“值等于给定值”的二分查找确实不怎么会被用到，二分查找更适合用在“近似”查找问题，在这类问题上，二分查找的优势更加明显。比如今天讲的这几种变体问题，用其他数据结构，比如散列表、二叉树，就比较难实现了。

变体的二分查找算法写起来非常烧脑，很容易因为细节处理不好而产生 Bug，这些容易出错的细节有：*终止条件、区间上下界更新方法、返回值选择*。所以今天的内容你最好能用自己实现一遍，对锻炼编码能力、逻辑思维、写出 Bug free 代码，会很有帮助。

### 课后思考 ###

如果有序数组是一个循环有序数组，比如 4，5，6，1，2，3。针对这种情况，如何实现一个求“值等于给定值”的二分查找算法呢？

## 17 | 跳表：为什么Redis一定要用跳表来实现有序集合 ##


## 18 | 散列表（上）：Word文档中的单词拼写检查功能是如何实现的？ ##

### 散列思想 ###

散列表用的是数组支持按照下标随机访问数据的特性，所以散列表其实就是数组的一种扩展，由数组演化而来。可以说，如果没有数组，就没有散列表。

**键（key）**或者**关键字**，把参数编号转化为数组下标的映射方法就叫做**散列函数**，而散列函数计算得到的值就叫做**散列值**（或“Hash值” “哈希值”）

散列表用的就是数组支持按照下标随机访问的时候，时间复杂度是 O(1) 的特性。我们通过散列函数把元素的键值映射为下标，然后将数据存储在数组中对应下标的位置。当我们按照键值查询元素时，我们用同样的散列函数，将键值转化数组下标，从对应的数组下标的位置取数据。

该如何构造散列函数呢？我总结了三点散列函数设计的基本要求：

1. 散列函数计算得到的散列值是一个非负整数；
2. 如果 key1 = key2，那 hash(key1) == hash(key2)；
3. 如果 key1 ≠ key2，那 hash(key1) ≠ hash(key2)。在真实的情况下，要想找到一个不同的key对应的散列值都不一样的散列函数，几乎是不可能的。即使像MD5、SHA、CRC等哈希算法，也无法完全避免这种*散列冲突*。

### 散列冲突 ###

常用的散列冲突解决方法有两类，开放寻址法（open addressing）和链表法（chaining）。

#### 1. 开放寻址法 ####

开放寻址法的核心思想是，如果出现了散列冲突，我们就重新探测一个空闲位置，将其插入。

**线性探测**（Linear Probing）

**二次探测**（Quadratic probing）

线性探测每次探测的步长是 1，那它探测的下标序列就是 hash(key)+0，hash(key)+1，hash(key)+2，而二次探测探测的步长就变成了原来的“二次方”，也就是说，它探测的下标序列就是 hash(key)+0，hash(key)+1^2^，hash(key)+2^2^。

**双重散列**（Double hashing）

意思就是不仅要使用一个散列函数。我们使用一组散列函数 hash1(key)，hash2(key)，hash3(key)……我们先用第一个散列函数，如果计算得到的存储位置已经被占用，再用第二个散列函数，依次类推，直到找到空闲的存储位置。

装载因子（load factor：来表示空位的多少）的计算公式是：

	散列表的装载因子=填入表中的元素个数/散列表的长度

装载因子越大，说明空闲位置越少，冲突越多，散列表的性能会下降。

#### 2. 链表法 ####

链表法是一种更加常用的散列冲突解决办法，相比开放寻址法，它要简单很多。在散列表中，每个“桶（bucket）”或者“槽（slot）”会对应一条链表，所有散列值相同的元素我们都放到相同槽位对应的链表中。

对于散列比较均匀的散列函数来说，理论上讲，k=n/m，其中 n 表示散列中数据的个数，m 表示散列表中“槽”的个数。

### 解答开篇 ###

### 内容小结 ###

散列表两个核心问题：

1. 散列函数设计
2. 散列冲突解决
	* 开放寻址法
	* 链表法

### 课后思考 ###

## 19 | 散列表（中）：如何打造一个工业级水平的散列表 ##

如何设计一个可以应对各种异常情况的工业级散列表，来避免在散列冲突的情况下，散列表性能的急剧下降，并且能抵抗散列碰撞攻击？

### 如何设计散列函数？ ###

1. 散列函数的设计不能太复杂
2. 散列函数生成的值要尽可能随机并且均匀分布

#### 设计方法 ####

1. 数据分析法
2. ASCII码值“进位”相加，然后再跟散列表的大小求余、取模

### 装载因子过大了怎么办？ ###

装载因子越大，说明散列表中的元素越多，空闲位置越少，散列冲突的概率就越大。

插入一个数据，最好情况下，不需要扩容，最好时间复杂度是 O(1)。最坏情况下，散列表装载因子过高，启动扩容，我们需要重新申请内存空间，重新计算哈希位置，并且搬移数据，所以时间复杂度是 O(n)。用摊还分析法，均摊情况下，时间复杂度接近最好情况，就是 O(1)。

实际上，对于动态散列表，随着数据的删除，散列表中的数据会越来越少，空闲空间会越来越多。如果我们对空间消耗非常敏感，我们可以在装载因子小于某个值之后，启动动态缩容。当然，如果我们更加在意执行效率，能够容忍多消耗一点内存空间，那就可以不用费劲来缩容了。

装载因子阈值的设置要权衡时间、空间复杂度。如果内存空间不紧张，对执行效率要求很高，可以降低负载因子的阈值；相反，如果内存空间紧张，对执行效率要求又不高，可以增加负载因子的值，甚至可以大于 1。

### 如何避免低效地扩容？ ###

为了解决一次性扩容耗时过多的情况，我们可以将扩容操作穿插在插入操作的过程中，分批完成。当装载因子触达阈值之后，我们只申请新空间，但并不将老的数据搬移到新散列表中。

当有新数据要插入时，我们将新数据插入新散列表中，并且从老的散列表中拿出一个数据放入到新散列表。每次插入一个数据到散列表，我们都重复上面的过程。经过多次插入操作之后，老的散列表中的数据就一点一点全部搬移到新散列表中了。这样没有了集中的一次性数据搬移，插入操作就都变得很快了。

这期间的查询操作怎么来做呢？对于查询操作，为了兼容了新、老散列表中的数据，我们先从新散列表中查找，如果没有找到，再去老的散列表中查找。

通过这样均摊的方法，将一次性扩容的代价，均摊到多次插入操作中，就避免了一次性扩容耗时过多的情况。这种实现方式，任何情况下，插入一个数据的时间复杂度都是 O(1)。

### 如何选择冲突解决方法？ ###

比如，Java 中 LinkedHashMap 就采用了链表法解决冲突，ThreadLocalMap 是通过线性探测的开放寻址法来解决冲突。

#### 1. 开放寻址法 ####

开放寻址法不像链表法，需要拉很多链表。散列表中的数据都存储在数组中，可以有效地利用 CPU 缓存加快查询速度。而且，这种方法实现的散列表，序列化起来比较简单。链表法包含指针，序列化起来就没那么容易。

用开放寻址法解决冲突的散列表，删除数据的时候比较麻烦，需要特殊标记已经删除掉的数据。而且，在开放寻址法中，所有的数据都存储在一个数组中，比起链表法来说，冲突的代价更高。所以，使用开放寻址法解决冲突的散列表，装载因子的上限不能太大。这也导致这种方法比链表法更浪费内存空间。

*我总结一下，当数据量比较小、装载因子小的时候，适合采用开放寻址法。这也是 Java 中的ThreadLocalMap使用开放寻址法解决散列冲突的原因。*

#### 2. 链表法 ####

1. 表法对内存的利用率比开放寻址法要高。因为链表结点可以在需要的时候再创建，并不需要像开放寻址法那样事先申请好。实际上，这一点也是我们前面讲过的链表优于数组的地方。
2. 对于链表法来说，只要散列函数的值随机均匀，即便装载因子变成 10，也就是链表的长度变长了而已，虽然查找效率有所下降，但是比起顺序查找还是快很多。
3. 链表因为要存储指针，所以对于比较小的对象的存储，是比较消耗内存的，还有可能会让内存的消耗翻倍。而且，因为链表中的结点是零散分布在内存中的，不是连续的，所以对 CPU 缓存是不友好的，这方面对于执行效率也有一定的影响。

对链表法稍加改造，可以实现一个更加高效的散列表。那就是，我们将链表法中的链表改造为其他高效的动态数据结构，比如跳表、红黑树。这样，

*我总结一下，基于链表的散列冲突处理方法比较适合存储大对象、大数据量的散列表，而且，比起开放寻址法，它更加灵活，支持更多的优化策略，比如用红黑树代替链表。*

### 工业级散列表举例分析 ###

Java 中的 HashMap 这样一个工业级的散列表

#### 1.  初始大小 ####

HashMap 默认的初始大小是 16，当然这个默认值是可以设置的，如果事先知道大概的数据量有多大，可以通过修改默认初始大小，减少动态扩容的次数，这样会大大提高 HashMap 的性能。

#### 2.  装载因子和动态扩容 ####

最大装载因子默认是 0.75，当 HashMap 中元素个数超过 0.75*capacity（capacity 表示散列表的容量）的时候，就会启动扩容，每次扩容都会扩容为原来的两倍大小。

#### 3.  散列冲突解决方法 ####

HashMap 底层采用链表法来解决冲突。即使负载因子和散列函数设计得再合理，也免不了会出现拉链过长的情况，一旦出现拉链过长，则会严重影响 HashMap 的性能。

JDK1.8版本中，为了对 HashMap 做进一步优化，引入了红黑树。当链表长度太长（默认超过 8）时，链表就转换为红黑树。我们可以利用红黑树快速增删改查的特点，提高 HashMap 的性能。当红黑树结点个数少于 8 个的时候，又会将红黑树转化为链表。因为在数据量较小的情况下，红黑树要维护平衡，比起链表来，性能上的优势并不明显。

#### 4. 散列函数 ####

散列函数的设计并不复杂，追求的是简单高效、分布均匀。
	
	int hash(Object key) {
	    int h = key.hashCode()；
	    return (h ^ (h >>> 16)) & (capicity -1); //capicity表示散列表的大小
	}

其中，hashCode() 返回的是 Java 对象的 hash code。

### 解答开篇 ###

*何为一个工业级的散列表？工业级的散列表应该具有哪些特性？*

1. 支持快速的查询、插入、删除操作；
2. 内存占用合理，不能浪费过多的内存空间；
3. 性能稳定，极端情况下，散列表的性能也不会退化到无法接受的情况。

*如何实现这样一个散列表呢*

1. 设计一个合适的散列函数；
2. 定义装载因子阈值，并且设计动态扩容策略；
3. 选择合适的散列冲突解决方法。

### 内容小结 ###

如何设计一个工业级的散列表，以及如何应对各种异常情况，防止在极端情况下，散列表的性能退化过于严重。应对策略：

1. 如何设计散列函数
2. 如何根据装载因子动态扩容
3. 如何选择散列冲突解决方法。

关于散列函数的设计，我们要尽可能让散列后的值随机且均匀分布，

关于散列冲突解决方法的选择，我对比了开放寻址法和链表法两种方法的优劣和适应的场景。大部分情况下，链表法更加普适。而且，我们还可以通过将链表法中的链表改造成其他动态查找数据结构，比如红黑树，来避免散列表时间复杂度退化成 O(n)，抵御散列碰撞攻击。但是，对于小规模数据、装载因子不高的散列表，比较适合用开放寻址法。

对于动态散列表来说，不管我们如何设计散列函数，选择什么样的散列冲突解决方法。随着数据的不断增加，散列表总会出现装载因子过高的情况。这个时候，我们就需要启动动态扩容。

## 20 | 散列表（下）：为什么散列表和链表经常会一起使用？ ##

如何用链表来实现 LRU 缓存淘汰算法，但是链表实现的 LRU 缓存淘汰算法的时间复杂度是 O(n)

工业实现：

* Redis 的有序集合是使用跳表来实现的，跳表可以看作一种改进版的链表。Redis 有序集合不仅使用了跳表，还用到了散列表。
* Java的LinkedHashMap 这样一个常用的容器，也用到了散列表和链表两种数据结构。

### LRU缓存淘汰算法 ###

在链表那一节中，我提到，借助散列表，我们可以把 LRU 缓存淘汰算法的时间复杂度降低为 O(1)。现在，我们就来看看它是如何做到的。

1. 维护一个按照访问时间从大到小有序排列的链表结构。因为缓存大小有限，当缓存空间不够，需要淘汰一个数据的时候，我们就直接将链表头部的结点删除。
2. 当要缓存某个数据的时候，先在链表中查找这个数据。如果没有找到，则直接将数据放到链表的尾部；如果找到了，我们就把它移动到链表的尾部。因为查找数据需要遍历链表，所以单纯用链表实现的 LRU 缓存淘汰算法的时间复杂很高，是 O(n)。

一个缓存（cache）系统主要包含下面这几个操作：

1. 往缓存中添加一个数据；
2. 从缓存中删除一个数据；
3. 在缓存中查找一个数据。

这三个操作都要涉及“查找”操作，如果单纯地采用链表的话，时间复杂度只能是 O(n)。如果我们将散列表和链表两种数据结构组合使用，可以将这三个操作的时间复杂度都降低到 O(1)。

![eaefd5f4028cc7d4cfbb56b24ce8ae6e.jpg](img/eaefd5f4028cc7d4cfbb56b24ce8ae6e.jpg)

使用双向链表存储数据，链表中的每个结点处理存储数据（data）、前驱指针（prev）、后继指针（next）之外，还新增了一个特殊的字段 hnext。这个 hnext 有什么作用呢？

因为我们的散列表是通过链表法解决散列冲突的，所以每个结点会在两条链中。一个链是刚刚我们提到的*双向链表*，另一个链是散列表中的*拉链*。*前驱和后继指针是为了将结点串在双向链表中，hnext 指针是为了将结点串在散列表的拉链中。*如何做到时间复杂度是O（1）的？

#### 1. 如何查找一个数据 ####

散列表中查找数据的时间复杂度接近 O(1)，所以通过散列表，我们可以很快地在缓存中找到一个数据。当找到数据之后，我们还需要将它移动到双向链表的尾部。

#### 2. 如何删除一个数据 ####

我们需要找到数据所在的结点，然后将结点删除。借助散列表，我们可以在 O(1) 时间复杂度里找到要删除的结点。因为我们的链表是双向链表，双向链表可以通过前驱指针 O(1) 时间复杂度获取前驱结点，所以在双向链表中，删除结点只需要 O(1) 的时间复杂度。

#### 3. 如何添加一个数据 ####

添加数据到缓存稍微有点麻烦，我们需要先看这个数据是否已经在缓存中。如果已经在其中，需要将其移动到双向链表的尾部；如果不在其中，还要看缓存有没有满。如果满了，则将双向链表头部的结点删除，然后再将数据放到链表的尾部；如果没有满，就直接将数据放到链表的尾部。

这整个过程涉及的查找操作都可以通过散列表来完成。其他的操作，比如删除头结点、链表尾部插入数据等，都可以在 O(1) 的时间复杂度内完成。所以，这三个操作的时间复杂度都是 O(1)。至此，我们就通过散列表和双向链表的组合使用，实现了一个高效的、支持 LRU 缓存淘汰算法的缓存系统原型。

### Redis 有序集合 ###

在有序集合中，每个成员对象有两个重要的属性，key（键值）和 score（分值）。我们不仅会通过 score 来查找数据，还会通过 key 来查找数据。

### Java LinkedHashMap ###

*LinkedHashMap 是通过双向链表和散列表这两种数据结构组合实现的。LinkedHashMap 中的“Linked”实际上是指的是双向链表，并非指用链表法解决散列冲突。*

### 解答开篇 & 内容小结 ###

### 课后思考 ###

## 21 | 哈希算法（上）：如何防止数据库中的用户信息被脱库 ##

### 什么是哈希算法？ ###

Hash，将任意长度的二进制值串映射为固定长度的二进制值串，这个映射的规则就是*哈希算法*，而通过原始数据映射之后得到的二进制值串就是*哈希值*。

需要满足的要求：

* 从哈希值不能反向推导出原始数据（所以哈希算法也叫单向哈希算法）；
* 对输入数据非常敏感，哪怕原始数据只修改了一个 Bit，最后得到的哈希值也大不相同；
* 散列冲突的概率要很小，对于不同的原始数据，哈希值相同的概率非常小；
* 哈希算法的执行效率要尽量高效，针对较长的文本，也能快速地计算出哈希值。

哈希算法的应用非常非常多，我选了最常见的七个，分别是安全加密、唯一标识、数据校验、散列函数、负载均衡、数据分片、分布式存储。

### 应用一：安全加密 ###

最常用于加密的哈希算法是 

* *MD5*（MD5 Message-Digest Algorithm，MD5 消息摘要算法）
* *SHA*（Secure Hash Algorithm，安全散列算法）

其他加密算法：

* DES（Data Encryption Standard，数据加密标准）
* AES（Advanced Encryption Standard，高级加密标准）

对用于加密的哈希算法来说，有两点格外重要。

1. 很难根据哈希值反向推导出原始数据
2. 散列冲突的概率要很小。

#### 为什么哈希算法无法做到零冲突？ ####

一般情况下，哈希值越长的哈希算法，散列冲突的概率越低。

除此之外，没有绝对安全的加密。越复杂、越难破解的加密算法，需要的计算时间也越长。

### 应用二：唯一标识 ###

给每一个图片取一个唯一标识，或者说信息摘要。比如，我们可以从图片的二进制码串开头取 100 个字节，从中间取 100 个字节，从最后再取 100 个字节，然后将这 300 个字节放到一块，通过哈希算法（比如 MD5），得到一个哈希字符串，用它作为图片的唯一标识。通过这个唯一标识来判定图片是否在图库中，这样就可以减少很多工作量。

可以把每个图片的唯一标识，和相应的图片文件在图库中的路径信息，都存储在散列表中。当要查看某个图片是不是在图库中的时候，我们先通过哈希算法对这个图片取唯一标识，然后在散列表中查找是否存在这个唯一标识。

如果不存在，那就说明这个图片不在图库中；如果存在，我们再通过散列表中存储的文件路径，获取到这个已经存在的图片，跟现在要插入的图片做全量的比对，看是否完全一样。如果一样，就说明已经存在；如果不一样，说明两张图片尽管唯一标识相同，但是并不是相同的图片。

### 应用三：数据校验 ###

们通过哈希算法，对 100 个文件块分别取哈希值，并且保存在种子文件中。对数据很敏感。只要文件块的内容有一丁点儿的改变，最后计算出的哈希值就会完全不同。所以，当文件块下载完成之后，我们可以通过相同的哈希算法，对下载好的文件块逐一求哈希值，然后跟种子文件中保存的哈希值比对。如果不同，说明这个文件块不完整或者被篡改了，需要再重新从其他宿主机器上下载这个文件块。

### 应用四：散列函数 ###

散列函数是设计一个散列表的关键。它直接决定了散列冲突的概率和散列表的性能。不过，相对哈希算法的其他应用，散列函数对于散列算法冲突的要求要低很多。即便出现个别散列冲突，只要不是过于严重，我们都可以通过开放寻址法或者链表法解决。

散列函数用的散列算法一般都比较简单，比较追求效率。

### 解答开篇 ###

针对字典攻击，我们可以引入一个盐（salt），跟用户的密码组合在一起，增加密码的复杂度。我们拿组合之后的字符串来做哈希算法加密，将它存储到数据库中，进一步增加破解的难度。不过我这里想多说一句，我认为安全和攻击是一种博弈关系，不存在绝对的安全。所有的安全措施，只是增加攻击的成本而已。

### 内容小结 ###

哈希算法的四个应用场景

1. 唯一标识，哈希算法可以对大数据做信息摘要，通过一个较短的二进制编码来表示很大的数据。
2. 校验数据的完整性和正确性
3. 安全加密，我们讲到任何哈希算法都会出现散列冲突，但是这个冲突概率非常小。越是复杂哈希算法越难破解，但同样计算时间也就越长。所以，选择哈希算法的时候，要权衡安全性和计算时间来决定用哪种哈希算法。
4. 散列函数，这个我们前面讲散列表的时候已经详细地讲过，它对哈希算法的要求非常特别，更加看重的是散列的平均性和哈希算法的执行效率。


### 课后思考 ###

区块链

## 22 | 哈希算法（下）：哈希算法在分布式系统中有哪些应用 ##

分布式系统相关：

* 负载均衡
* 数据分片
* 分布式存储

### 应用五：负载均衡 ###

负载均衡算法有很多，比如轮询、随机、加权轮询等。

最直接的方法就是，维护一张映射关系表，这张表的内容是客户端 IP 地址或者会话 ID 与服务器编号的映射关系。客户端发出的每次请求，都要先在映射表中查找应该路由到的服务器编号，然后再请求编号对应的服务器。这种方法简单直观，但也有几个弊端：

* 如果客户端很多，映射表可能会很大，比较浪费内存空间；
* 客户端下线、上线，服务器扩容、缩容都会导致映射失效，这样维护映射表的成本就会很大；

我们可以通过哈希算法，对客户端 IP 地址或者会话 ID 计算哈希值，将取得的哈希值与服务器列表的大小进行取模运算，最终得到的值就是应该被路由到的服务器编号。

### 应用六：数据分片 ###

#### 1. 如何统计“搜索关键词”出现的次数？ ####

假如我们有 1T 的日志文件，这里面记录了用户的搜索关键词，我们想要快速统计出每个关键词被搜索的次数，该怎么做呢？

有两个难点：

* 搜索日志很大，没办法放到一台机器的内存中。
* 如果只用一台机器来处理这么巨大的数据，处理时间会很长。

针对这两个难点，采取以下对策：

1. 对数据进行分片
2. 采用多台机器处理的方法，来提高处理速度

具体的方式：

1. 为了提高处理的速度，我们用 n 台机器并行处理。
2. 从搜索记录的日志文件中，依次读出每个搜索关键词
3. 通过哈希函数计算哈希值
4. 再跟 n 取模，最终得到的值，就是应该被分配到的机器编号。

处理过程也是 MapReduce 的基本设计思想

#### 2. 如何快速判断图片是否在图库中？ ####

设现在我们的图库中有 1 亿张图片，很显然，在单台机器上构建散列表是行不通的。因为单台机器的内存有限，而 1 亿张图片构建散列表显然远远超过了单台机器的内存上限。

1. 可以对数据分片，采用多机处理，准备n台机器，让每台机器只维护某一部分图片对应的散列表
2. 每次从图库中读取一个图片，计算唯一标识
3. 然后与机器个数 n 求余取模，得到的值就对应要分配的机器编号
4. 然后将这个图片的唯一标识和图片路径发往对应的机器构建散列表。

针对这种海量数据的处理问题，我们都可以采用多机分布式处理。借助这种分片的思路，可以突破单机内存、CPU 等资源的限制。

### 应用七：分布式存储 ###

该如何决定将哪个数据放到哪个机器上呢？我们可以借用前面数据分片的思想，即通过哈希算法对数据取哈希值，然后对机器个数取模，这个最终值就是应该存储的缓存机器编号。

使得在新加入一个机器后，并不需要做大量的数据搬移。这时候，一致性哈希算法就要登场了

假设我们有 k 个机器，数据的哈希值的范围是[0, MAX]。我们将整个范围划分成 m 个小区间（m 远大于 k），每个机器负责 m/k 个小区间。当有新机器加入的时候，我们就将某几个小区间的数据，从原来的机器中搬移到新的机器中。这样，既不用全部重新哈希、搬移数据，也保持了各个机器上数据数量的均衡。

除了我们上面讲到的分布式缓存，实际上，一致性哈希算法的应用非常广泛，在很多分布式存储系统中，都可以见到一致性哈希算法的影子。

### 解答开篇 & 内容小结 ###

三种哈希算法在分布式系统中的应用，它们分别是：负载均衡、数据分片、分布式存储。

* 在负载均衡应用中，利用哈希算法替代映射表，可以实现一个会话粘滞的负载均衡策略。
* 在数据分片应用中，通过哈希算法对处理的海量数据进行分片，多机分布式处理，可以突破单机资源的限制。
* 在分布式存储应用中，利用一致性哈希算法，可以解决缓存等分布式系统的扩容、缩容导致数据大量搬移的难题。

## 23 | 二叉树基础（上）：什么样的二叉树适合用数组来存储？ ##

二叉树有哪几种存储方式？什么样的二叉树适合用数组来存储？

### 树（Tree） ###

![220043e683ea33b9912425ef759556ae.jpg](img/220043e683ea33b9912425ef759556ae.jpg)

A 节点就是 B 节点的父节点，B 节点是 A 节点的子节点。B、C、D 这三个节点的父节点是同一个节点，所以它们之间互称为兄弟节点。我们把没有父节点的节点叫作根节点，也就是图中的节点 E。我们把没有子节点的节点叫作叶子节点或者叶节点，比如图中的 G、H、I、J、K、L 都是叶子节点。

高度（Height）、深度（Depth）、层（Level）

### 二叉树（Binary Tree） ###

![09c2972d56eb0cf67e727deda0e9412b.jpg](img/09c2972d56eb0cf67e727deda0e9412b.jpg)

* 满二叉树：编号 2 的二叉树中，叶子节点全都在最底层，除了叶子节点之外，每个节点都有左右两个子节点
* 完全二叉树：编号 3 的二叉树中，叶子节点都在最底下两层，最后一层的叶子节点都靠左排列，并且除了最后一层，其他层的节点个数都要达到最大

#### 如何表示（或者存储）一棵二叉树？ ####

需要利用完全二叉树

有两种方法：

* 一种是基于指针或者引用的二叉链式存储法
* 一种是基于数组的顺序存储法。

* 链式存储法，每个节点有三个字段，其中一个存储数据，另外两个是指向左右子节点的指针。
* 顺序存储法，我们把根节点存储在下标 i = 1 的位置，那左子节点存储在下标 2 * i = 2 的位置，右子节点存储在 2 * i + 1 = 3 的位置。以此类推，B 节点的左子节点存储在 2 * i = 2 * 2 = 4 的位置，右子节点存储在 2 * i + 1 = 2 * 2 + 1 = 5 的位置

![14eaa820cb89a17a7303e8847a412330.jpg](img/14eaa820cb89a17a7303e8847a412330.jpg)

举的例子是一棵完全二叉树，所以仅仅“浪费”了一个下标为 0 的存储位置。如果是非完全二叉树，其实会浪费比较多的数组存储空间。

![08bd43991561ceeb76679fbb77071223.jpg](img/08bd43991561ceeb76679fbb77071223.jpg)

如果某棵二叉树是一棵完全二叉树，那用数组存储无疑是最节省内存的一种方式。因为数组的存储方式并不需要像链式存储法那样，要存储额外的左右子节点的指针。这也是为什么完全二叉树会单独拎出来的原因，也是为什么完全二叉树要求最后一层的子节点都靠左的原因。
当我们讲到堆和堆排序的时候，你会发现，堆其实就是一种完全二叉树，最常用的存储方式就是数组。

### 二叉树的遍历 ###

经典的方法有三种，*前序遍历、中序遍历和后序遍历*。其中，前、中、后序，表示的是节点与它的左右子树节点遍历打印的先后顺序。

* 前序遍历是指，对于树中的任意节点来说，先打印这个节点，然后再打印它的左子树，最后打印它的右子树。
* 中序遍历是指，对于树中的任意节点来说，先打印它的左子树，然后再打印它本身，最后打印它的右子树。
* 后序遍历是指，对于树中的任意节点来说，先打印它的左子树，然后再打印它的右子树，最后打印这个节点本身。

*实际上，二叉树的前、中、后序遍历就是一个递归的过程。*

二叉树遍历的时间复杂度是多少

### 解答开篇 & 内容小结 ###

一种非线性表数据结构，树。关于树，有几个比较常用的概念：根节点、叶子节点、父节点、子节点、兄弟节点，还有节点的高度、深度、层数，以及树的高度。

平时最常用的树就是二叉树。二叉树的每个节点最多有两个子节点，分别是左子节点和右子节点。二叉树中，有两种比较特殊的树，分别是满二叉树和完全二叉树。满二叉树又是完全二叉树的一种特殊情况。

二叉树既可以用链式存储，也可以用数组顺序存储。数组顺序存储的方式比较适合完全二叉树，其他类型的二叉树用数组存储会比较浪费存储空间。除此之外，二叉树里非常重要的操作就是前、中、后序遍历操作，遍历的时间复杂度是 O(n)，你需要理解并能用递归代码来实现。

### 课后思考 ###

## 24 | 二叉树基础（下）：有了如此高效的散列表，为什么还需要二叉树？ ##

既然有了这么高效的散列表，使用二叉树的地方是不是都可以替换成散列表呢？有没有哪些地方是散列表做不了，必须要用二叉树来做的呢？

### 二叉查找树（Binary Search Tree） ###

二叉查找树是二叉树中最常用的一种类型，也叫二叉搜索树。二叉查找树是为了实现快速查找而生的。不过，它不仅仅支持快速查找一个数据，还支持快速插入、删除一个数据。

二叉查找树要求，在树中的任意一个节点，其左子树中的每个节点的值，都要小于这个节点的值，而右子树节点的值都大于这个节点的值。

#### 1. 二叉查找树的查找操作 ####

1. 在二叉查找树中查找一个节点

#### 2. 二叉查找树的插入操作 ####

二叉查找树的插入过程有点类似查找操作。新插入的数据一般都是在叶子节点上，所以我们只需要从根节点开始，依次比较要插入的数据和节点的大小关系。

* 如果要插入的数据比节点的数据大，并且节点的右子树为空，就将新数据直接插到右子节点的位置；
* 如果不为空，就再递归遍历右子树，查找插入位置。
* 同理，如果要插入的数据比节点数值小，并且节点的左子树为空，就将新数据插入到左子节点的位置；
* 如果不为空，就再递归遍历左子树，查找插入位置。

#### 3. 二叉查找树的删除操作 ####

1. 如果要删除的节点没有子节点，我们只需要直接将父节点中，指向要删除节点的指针置为 null
2. 如果要删除的节点只有一个子节点（只有左子节点或者右子节点）只需要更新父节点中，指向要删除节点的指针，让它指向要删除节点的子节点就可以了。
3. 如果要删除的节点有两个子节点，们需要找到这个节点的右子树中的最小节点，把它替换到要删除的节点上。然后再删除掉这个最小节点，因为最小节点肯定没有左子节点（如果有左子结点，那就不是最小节点了）。

#### 4. 二叉查找树的其他操作 ####

除了插入、删除、查找操作之外，二叉查找树中还可以支持快速地查找最大节点和最小节点、前驱节点和后继节点。

二叉查找树除了支持上面几个操作之外，还有一个重要的特性：就是*中序遍历二叉查找树，可以输出有序的数据序列，时间复杂度是 O(n)，非常高效*。因此，二叉查找树也叫作二叉排序树。

### 支持重复数据的二叉查找树 ###

1. 二叉查找树中每一个节点不仅会存储一个数据，因此我们通过链表和支持动态扩容的数组等数据结构，把值相同的数据都存储在同一个节点上。
2. 每个节点仍然只存储一个数据。在查找插入位置的过程中，如果碰到一个节点的值，与要插入数据的值相同，我们就将这个要插入的数据放到这个节点的右子树，也就是说，把这个新插入的数据当作大于这个节点的值来处理。
	* 查找当要查找数据的时候，遇到值相同的节点，我们并不停止查找操作，而是继续在右子树中查找，直到遇到叶子节点，才停止。这样就可以把键值等于要查找值的所有节点都找出来。
	* 删除，需要先查找到每个要删除的节点，然后再按前面讲的删除操作的方法，依次删除。

### 二叉查找树的时间复杂度分析 ###

二叉查找树的插入、删除、查找操作的时间复杂度

不管操作是插入、删除还是查找，*时间复杂度其实都跟树的高度成正比，也就是 O(height)*。

树的高度就等于最大层数减一，

极度不平衡的二叉查找树，它的查找性能肯定不能满足我们的需求。我们需要构建一种不管怎么删除、插入数据，在任何时候，都能保持任意节点左右子树都比较平衡的二叉查找树， =》 这就是我们下一节课要详细讲的，一种特殊的二叉查找树，平衡二叉查找树。平衡二叉查找树的高度接近 logn，所以插入、删除、查找操作的时间复杂度也比较稳定，是 O(logn)。

### 解答开篇 ###

散列表的插入、删除、查找操作的时间复杂度可以做到常量级的 O(1)，而二叉查找树在比较平衡的情况下，插入、删除、查找操作时间复杂度才是 O(logn)。

有下面几个原因：

1. 散列表中的数据是无序存储的，如果要输出有序的数据，需要先进行排序。而对于二叉查找树来说，我们只需要中序遍历，就可以在 O(n) 的时间复杂度内，输出有序的数据序列。
2. 散列表扩容耗时很多，而且当遇到散列冲突时，性能不稳定，尽管二叉查找树的性能不稳定，但是在工程中，我们最常用的平衡二叉查找树的性能非常稳定，时间复杂度稳定在 O(logn)。
3. 尽管散列表的查找等操作的时间复杂度是常量级的，但因为哈希冲突的存在，这个常量不一定比 logn 小，所以实际的查找速度可能不一定比 O(logn) 快。加上哈希函数的耗时，也不一定就比平衡二叉查找树的效率高。
4. 散列表的构造比二叉查找树要复杂，需要考虑的东西很多。比如散列函数的设计、冲突解决办法、扩容、缩容等。平衡二叉查找树只需要考虑平衡性这一个问题，而且这个问题的解决方案比较成熟、固定。
5. 为了避免过多的散列冲突，散列表装载因子不能太大，特别是基于开放寻址法解决冲突的散列表，不然会浪费一定的存储空间。

综合这几点，平衡二叉查找树在某些方面还是优于散列表的，所以，这两者的存在并不冲突。我们在实际的开发过程中，需要结合具体的需求来选择使用哪一个。

### 内容小结 ###

特殊的二叉树，二叉查找树，支持快速地查找、插入、删除操作。

* 没有重复值：二叉查找树中，每个节点的值都大于左子树节点的值，小于右子树节点的值。
* 有重复值：
	* 让每个节点存储多个值相同的数据
	* 每个节点中存储一个数据

在二叉查找树中，查找、插入、删除等很多操作的时间复杂度都跟树的高度成正比。两个极端情况的时间复杂度分别是 O(n) 和 O(logn)，分别对应二叉树退化成链表的情况和完全二叉树。  

为了避免时间复杂度的退化，针对二叉查找树，我们又设计了一种更加复杂的树，平衡二叉查找树，时间复杂度可以做到稳定的 O(logn)

## 25 | 红黑树（上）：为什么工程中都用红黑树这种二叉树？ ##

## 26 | 红黑树（下）：掌握这些技巧，你也可以实现一个红黑树 ##

## 27 | 递归树：如何借助树来求解递归算法的时间复杂度？ ##




## 28 | 堆：堆和堆排序：为什么堆排序没有快速排序快 ##

