# 深入浅出计算机组成原理 #

入门篇

## 开篇词 | 为什么你需要学习计算机组成原理 ##

如果越早去弄清楚计算机的底层原理，在你的知识体系中“储蓄”起这些知识，也就意味着你有越长的时间来收获学习知识的“利息”。虽然一开始可能不起眼，但是随着时间带来的复利效应，你的长线投资项目，就能让你在成长的过程中越走越快。

### 计算机底层知识的“第一课” ###

学习这门“第一课”的过程，会为你在整个软件开发领域中打开一扇扇窗和门，让你看到更加广阔的天地。比如说，明白了高级语言是如何对应着 CPU 能够处理的一条条指令，能为你打开编译原理这扇门；搞清楚程序是如何加载运行的，能够让你对操作系统有更深入的理解。

### 理论和实践相结合 ###

* 北京大学的《计算机组成》开放课程
* 计算机组成与设计：硬件 / 软件接口

组原难学的原因：

1. 广：组成原理中的概念非常多，每个概念的信息量也非常大。比如想要理解 CPU 中的算术逻辑单元（也就是 ALU）是怎么实现加法的，需要牵涉到如何把整数表示成二进制，还需要了解这些表示背后的电路、逻辑门、CPU 时钟、触发器等知识。
2. 深：组成原理中的很多概念，阐述开来就是计算机学科的另外一门核心课程。
3. 学不能致用：常常沉溺于概念和理论中，无法和自己日常的开发工作联系起来

我要把这些知识点和日常工作、生活以及整个计算机行业的发展史联系起来，教你真正看懂、学会、记住组成原理的核心内容，教你更多地从“为什么”这个角度，去理解这些知识点，而不是只是去记忆“是什么”。

专栏具体的设计：

1. 把组成原理里面的知识点，和我在应用开发和架构设计中遇到的实际案例，放到一起进行印证，通过代码和案例，让你消化理解。**比如，为什么 Disruptor 这个高性能队列框架里，要定义很多没有用的占位变量呢？其实这是为了确保我们唯一关心的参数，能够始终保留在 CPU 的高速缓存里面，而高速缓存比我们的内存要快百倍以上。**
2. **尽可能地多举一些我们日常生活里面的例子，让你理解计算机的各个组件是怎么运作的。**在真实的开发中，我们会遇到什么问题，这些问题产生的根源是什么。让你从知识到应用，最终又回到知识，让学习和实践之间形成一道闭环。计算机组成中很多组件的设计，都不是凭空发明出来，它们中的很多都来自现实生活中的想法和比喻。而底层很多硬件设计和开发的思路，其实也和你进行软件架构的开发设计和思路是一样的。比如说，在硬件上，我们是通过最基本的与、或、非、异或门这些最基础的门电路组合形成了强大的 CPU。而在面向对象和设计模式里，我们也常常是通过定义基本的 Command，然后组合来完成更复杂的功能；再比如说，CPU 里面的冒险和分支预测的策略，就好像在接力赛跑里面后面几棒的选手早点起跑，如果交接棒没有问题，自然占了便宜，但是如果没能交接上，就会吃个大亏。
3. **知识点和应用之外，我会多讲一些计算机硬件发展史上的成功和失败，让你明白很多设计的历史渊源，让你更容易记住“为什么”，更容易记住这些知识点。**

“人生如逆旅，我亦是行人”。学习总不会是一件太轻松的事情，希望在这个专栏里，你能和我多交流，坚持练完这一手内功。

## 01 | 冯·诺依曼体系结构：计算机组成的金字塔 ##

### 计算机的基本硬件组成 ###

1. CPU，叫中央处理器（Central Processing Unit）
2. 内存（Memory）
3. 主板（Motherboard）。主板的芯片组（Chipset）和总线（Bus）解决了 CPU 和内存之间如何通信的问题。芯片组控制了数据传输的流转，也就是数据从哪里到哪里的问题。总线则是实际数据传输的高速公路。因此，总线速度（Bus Speed）决定了数据能传输得多快。

鼠标、键盘以及硬盘，这些都是插在主板上的。作为外部 I/O 设备，它们是通过主板上的南桥（SouthBridge）芯片组，来控制和 CPU 之间的通信的。“南桥”芯片的名字很直观，一方面，它在主板上的位置，通常在主板的“南面”。另一方面，它的作用就是作为“桥”，来连接鼠标、键盘以及硬盘这些外部设备和 CPU 之间的通信。

### 冯·诺依曼体系结构 ###

SoC，System on a Chip（系统芯片）

### 总结延伸 ###

学习组成原理，其实就是学习控制器、运算器的工作原理，也就是 CPU 是怎么工作的，以及为何这样设计；学习内存的工作原理，从最基本的电路，到上层抽象给到 CPU 乃至应用程序的接口是怎样的；学习 CPU 是怎么和输入设备、输出设备打交道的。

学习组成原理，就是在理解从控制器、运算器、存储器、输入设备以及输出设备，从电路这样的硬件，到最终开放给软件的接口，是怎么运作的，为什么要设计成这样，以及在软件开发层面怎么尽可能用好它。

### 推荐阅读 ###

[First Draft of a Report on the EDVAC](https://en.wikipedia.org/wiki/First_Draft_of_a_Report_on_the_EDVAC)

### 课后思考 ###

## 02 | 给你一张知识地图，计算机组成原理应该怎么学 ##

*整个计算机组成原理，就是围绕着计算机是如何组织运作展开的*

### 计算机组成原理知识地图 ###

![12bc980053ea355a201e2b529048e2ff.jpg](img/12bc980053ea355a201e2b529048e2ff.jpg)

计算机的基本组成、计算机的指令和计算、处理器设计、以及存储器和I/O设备

1. 计算机的基本组成
	* 运算器、控制器、存储器、输入设备和输出设备这五大基本组件
	* 计算机的两个核心指标，性能和功耗
2. 计算机的指令和计算
	* 一条条指令执行的控制过程，就是由计算机五大组件之一的控制器来控制的
	* 

## 03 | 通过你的CPU主频，我们来谈谈“性能”究竟是什么？ ##



## 04 | 穿越功耗墙，我们该从哪些方面提升“性能”？ ##

# 原理篇：指令和运算 #

## 05 | 计算机指令：让我们试试用纸带编程 ##

### 在软硬件接口中，CPU 帮我们做了什么事？ ###

我们的个人电脑用的是 Intel 的 CPU，苹果手机用的是 ARM 的 CPU。这两者能听懂的语言就不太一样。类似这样两种 CPU 各自支持的语言，就是**两组不同的计算机指令集**，英文叫 Instruction Set。这里面的“Set”，其实就是数学上的集合，代表不同的单词、语法。

一个计算机程序，不可能只有一条指令，而是由成千上万条指令组成的。但是 CPU 里不能一直放着所有指令，所以计算机程序平时是存储在存储器中的。这种程序指令存储在存储器里面的计算机，我们就叫作**存储程序型计算机**（Stored-program Computer）。

### 从编译到汇编，代码怎么变成机器码？ ###

	// test.c
	int main()
	{
	  int a = 1; 
	  int b = 2;
	  a = a + b;
	}

C语言 => （Compile）汇编语言 => 汇编器（Assembler）机器码

	$ gcc -g -c test.c
	$ objdump -d -M intel -S test.o

汇编代码是“给程序员看的机器码”。机器码和汇编代码是一一对应的。

### 解析指令和机器码 ###

常见的指令可以分成五大类：

1. 算术类指令：加减乘除，变成一条条算术类指令。
2. 数据传输类指令：变量赋值、在内存里读写数据，用的都是数据传输类指令。
3. 逻辑类指令：逻辑上的与或非。
4. 条件分支类指令：if/else，其实都是条件分支类指令。
5. 无条件跳转指令：函数或者方法，在调用函数的时候，其实就是发起了一个无条件跳转指令。

![ebfd3bfe5dba764cdcf871e23b29f197.jpeg](img/ebfd3bfe5dba764cdcf871e23b29f197.jpeg)

MIPS 的指令是一个 32 位的整数，高 6 位叫操作码（Opcode），剩下的 26 位有三种格式，分别是 R、I 和 J。

* *R指令*是一般用来做算术和逻辑操作，里面有读取和写入数据的寄存器的地址。
* *I指令*通常是用在数据传输、条件分支，以及在运算的时候使用的并非变量还是常数的时候。
* *J指令*一个跳转指令，高 6 位之外的 26 位都是一个跳转后的地址。

### 总结延伸 ###

**一个 C 语言程序，是怎么被编译成为汇编语言，乃至通过汇编器再翻译成机器码的。**

其实最终都是由不同形式的程序，把我们写好的代码，转换成 CPU 能够理解的机器码来执行的。

只是解释型语言，是通过解释器在程序运行的时候逐句翻译，而 Java 这样使用虚拟机的语言，则是由虚拟机对编译出来的中间代码进行解释，或者即时编译成为机器码来最终执行。

### 推荐阅读 ###

《计算机组成与设计：软 / 硬件接口》第 5 版的 2.17 小节

### 课后思考 ###

## 06 | 指令跳转：原来if...else就是goto ##

### CPU 是如何执行指令的？ ###

写好的代码变成了指令之后，是一条一条*顺序*执行的就可以了。

CPU 其实就是由一堆寄存器组成的。而寄存器就是 CPU 内部，由多个触发器（Flip-Flop）或者锁存器（Latches）组成的简单电路。

一个CPU里面会有很多种不同功能的寄存器：

1. PC寄存器（Program Counter Register），指令地址寄存器（Instruction Address Register）。用来存放下一条需要执行的计算机指令的内存地址。
2. 指令寄存器（Instruction Register），用来存放当前正在执行的指令。
3. 条件码寄存器（Status Register），用里面的一个一个标记位（Flag），存放 CPU 进行算术或者逻辑计算的结果。

通常根据存放的数据内容来给它们取名字，比如整数寄存器、浮点数寄存器、向量寄存器和地址寄存器等等。有些寄存器既可以存放数据，又能存放地址，我们就叫它通用寄存器。

![ad91b005e97959d571bbd2a0fa30b48a.jpeg](img/ad91b005e97959d571bbd2a0fa30b48a.jpeg)

特殊指令：

* 跳转指令

### 从 if…else 来看程序的执行和跳转 ###


	// test.c
	#include <time.h>
	#include <stdlib.h>
	
	int main()
	{
	  srand(time(NULL));
	  int r = rand() % 2;
	  int a = 10;
	  if (r == 0)
	  {
	    a = 1;
	  } else {
	    a = 2;
	  } 
	}

	$ gcc -g -c test.c
	$ objdump -d -M intel -S test.o 


	    if (r == 0)
	  3b:   83 7d fc 00             cmp    DWORD PTR [rbp-0x4],0x0
	  3f:   75 09                   jne    4a <main+0x4a>
	    {
	        a = 1;
	  41:   c7 45 f8 01 00 00 00    mov    DWORD PTR [rbp-0x8],0x1
	  48:   eb 07                   jmp    51 <main+0x51>
	    }
	    else
	    {
	        a = 2;
	  4a:   c7 45 f8 02 00 00 00    mov    DWORD PTR [rbp-0x8],0x2
	  51:   b8 00 00 00 00          mov    eax,0x0
	    } 

对于 `r ==  0` 的条件判断，被编译成了 cmp 和 jne 这两条指令。cmp 指令比较了前后两个操作数的值，这里的 DWORD PTR 代表操作的数据类型是 32 位的整数，而[rbp-0x4]则是一个寄存器的地址。

1. 第一个操作数就是从寄存器里拿到的变量 r 的值。
2. 第二个操作数 0x0 就是我们设定的常量 0 的 16 进制表示。
3. cmp 指令的比较结果，会存入到*条件码寄存器*当中去。

跟着的 jne 指令，是 jump if not equal 的意思，它会查看对应的零标志位。

1. 如果为 0，会跳转到后面跟着的操作数 4a 的位置。这个 4a，对应这里汇编代码的行号，也就是上面设置的 else 条件里的第一条指令。
2. 当跳转发生的时候，PC 寄存器就不再是自增变成下一条指令的地址，而是被直接设置成这里的 4a 这个地址。
3. 这个时候，CPU 再把 4a 地址里的指令加载到指令寄存器中来执行。
4. 跳转到执行地址为 4a 的指令，实际是一条 mov 指令，第一个操作数和前面的 cmp 指令一样，是另一个 32 位整型的寄存器地址，以及对应的 2 的 16 进制值 0x2。mov 指令把 2 设置到对应的寄存器里去，相当于一个赋值操作。然后，PC 寄存器里的值继续自增，执行下一条 mov 指令。
5. 这条 mov 指令的第一个操作数 eax，代表累加寄存器，第二个操作数 0x0 则是 16 进制的 0 的表示。这条指令其实没有实际的作用，它的作用是一个占位符。
6. 我们回过头去看前面的 if 条件，如果满足的话，在赋值的 mov 指令执行完成之后，有一个 jmp 的无条件跳转指令。跳转的地址就是这一行的地址 51。我们的 main 函数没有设定返回值，而 mov eax, 0x0 其实就是给 main 函数生成了一个默认的为 0 的返回值到累加器里面。if 条件里面的内容执行完成之后也会跳转到这里，和 else 里的内容结束之后的位置是一样的。

### 如何通过 if…else 和 goto 来实现循环？ ###

	int a = 0;
    for (int i = 0; i < 3; i++){
        a += i;
    }


	int a = 0;
	   4:	c7 45 fc 00 00 00 00 	mov    DWORD PTR [rbp-0x4],0x0
	    for (int i = 0; i < 3; i++){
	   b:	c7 45 f8 00 00 00 00 	mov    DWORD PTR [rbp-0x8],0x0
	  12:	eb 0a                	jmp    1e <main+0x1e>
	        a += i;
	  14:	8b 45 f8             	mov    eax,DWORD PTR [rbp-0x8]
	  17:	01 45 fc             	add    DWORD PTR [rbp-0x4],eax
	    for (int i = 0; i < 3; i++){
	  1a:	83 45 f8 01          	add    DWORD PTR [rbp-0x8],0x1
	  1e:	83 7d f8 02          	cmp    DWORD PTR [rbp-0x8],0x2
	  22:	7e f0                	jle    14 <main+0x14>
	  24:	b8 00 00 00 00       	mov    eax,0x0
	    }

1. 对应的循环也是用 1e 这个地址上的 cmp 比较指令，和紧接着的 jle 条件跳转指令来实现的。
2. 主要的差别在于，这里的 jle 跳转的地址，在这条指令之前的地址 14，而非 if…else 编译出来的跳转指令之后。
3. 往前跳转使得条件满足的时候，PC 寄存器会把指令地址设置到之前执行过的指令位置，重新执行之前执行过的指令，直到条件不满足，顺序往下执行 jle 之后的指令，整个循环才结束。

### 总结延伸 ###

单条指令的基础上，学习了程序里的多条指令，除了简单地通过 PC 寄存器自增的方式顺序执行外，条件码寄存器会记录下当前执行指令的条件判断状态，然后通过跳转指令读取对应的条件码，修改 PC 寄存器内的下一条指令的地址，最终实现 if…else 以及 for/while 这样的程序控制流程。

回归到计算机可以识别的机器指令级别，其实都只是一个简单的地址跳转而已，也就是一个类似于 goto 的语句。

要在硬件层面实现这个 goto 语句，除了本身需要用来保存下一条指令地址，以及当前正要执行指令的 PC 寄存器、指令寄存器外，我们只需要再增加一个条件码寄存器，来保留条件判断的状态。这样简简单单的三个寄存器，就可以实现条件判断和循环重复执行代码的功能。

### 推荐阅读 ###

《深入理解计算机系统》的第 3 章，详细讲解了 C 语言和 Intel CPU 的汇编语言以及指令的对应关系，以及 Intel CPU 的各种寄存器和指令集。

Intel 指令集相对于之前的 MIPS 指令集要复杂一些：

1. 所有的指令是变长的，从 1 个字节到 15 个字节不等
2. 针对操作数据的长度不同有不同的后缀

### 课后思考 ###

除了 if…else 的条件语句和 for/while 的循环之外，大部分编程语言还有 switch…case 这样的条件跳转语句。switch…case 编译出来的汇编代码也是这样使用 jne 指令进行跳转吗？对应的汇编代码的性能和写很多 if…else 有什么区别呢？你可以试着写一个简单的 C 语言程序，编译成汇编代码看一看。

## 07 | 函数调用：为什么会发生stack overflow？ ##

栈溢出（stack overflow）

### 为什么我们需要程序栈？ ###

	// function_example.c
	#include <stdio.h>
	int static add(int a, int b)
	{
	    return a+b;
	}
	
	
	int main()
	{
	    int x = 5;
	    int y = 10;
	    int u = add(x, y);
	}

这个程序定义了一个简单的函数 add，接受两个参数 a 和 b，返回值就是 a+b。而 main 函数里则定义了两个变量 x 和 y，然后通过调用这个 add 函数，来计算 u=x+y，最后把 u 的数值打印出来。

	$ gcc -g -c function_example.c
	$ objdump -d -M intel -S function_example.o

### 如何构造一个 stack overflow？ ###

无论有多少层的函数调用，或者在函数 A 里调用函数 B，再在函数 B 里调用 A，这样的递归调用，我们都只需要通过维持 rbp 和 rsp，这两个维护栈顶所在地址的寄存器，就能管理好不同函数之间的跳转。不过，栈的大小也是有限的。如果函数调用层数太多，我们往栈里压入它存不下的内容，程序在执行的过程中就会遇到栈溢出的错误，这就是大名鼎鼎的“stack  overflow”。

除了无限递归，递归层数过深，在栈空间里面创建非常占内存的变量（比如一个巨大的数组），这些情况都很可能给你带来 stack  overflow。

### 如何利用函数内联进行性能优化？ ###

**函数内联**（Inline）。我们只要在 GCC 编译的时候，加上对应的一个让编译器自动优化的参数 -O，编译器就会在可行的情况下，进行这样的指令替换。

	#include <stdio.h>
	#include <time.h>
	#include <stdlib.h>
	
	int static add(int a, int b)
	{
	    return a+b;
	}
	
	int main()
	{
	    srand(time(NULL));
	    int x = rand() % 5
	    int y = rand() % 10;
	    int u = add(x, y)
	    printf("u = %d\n", u)
	}


$ gcc -g -c -O function_example_inline.c
$ objdump -d -M intel -S function_example_inline.o

内联并不是没有代价，内联意味着，我们把可以复用的程序指令在调用它的地方完全展开了。如果一个函数在很多地方都被调用了，那么就会展开很多次，整个程序占用的空间就会变大了。

没有调用其他函数，只会被调用的函数，我们一般称之为**叶子函数（或叶子过程）**。

### 总结延伸 ###

一个程序的函数间调用，在 CPU 指令层面是怎么执行的。**程序栈**的概念。通过压栈和出栈操作，使得程序在不同的函数调用过程中进行转移。而函数内联和栈溢出，一个是我们常常可以选择的优化方案，另一个则是我们会常遇到的程序 Bug。

通过加入了程序栈，在指令跳转中加入了记忆的功能（跳转去新的指令之后，再回到跳出去的位置）。为我们在程序开发的过程中，提供了“函数”这样一个抽象。

### 推荐阅读 ###

《深入理解计算机系统（第三版）》的 3.7 小节《过程》

通过搜索引擎搞清楚 function_example.c 每一行汇编代码的含义，这个能够帮你进一步深入了解程序栈、栈帧、寄存器以及 Intel CPU 的指令集。

### 课后思考 ###

## 08 | ELF和静态链接：为什么程序无法同时在Linux和Windows下运行？ ##

### 编译、链接和装载：拆解程序执行 ###

写好的 C 语言代码，可以通过编译器编译成汇编代码，然后汇编代码再通过汇编器变成 CPU 可以理解的机器码，于是 CPU 就可以执行这些机器码了

“C 语言代码 - 汇编代码 - 机器码"

第一部分由编译（Compile）、汇编（Assemble）以及链接（Link）三个阶段组成。在这三个阶段完成之后，生成了一个可执行文件。
第二部分，通过装载器（Loader）把可执行文件装载（Load）到内存中。CPU从内存中读取指令和数据，来开始真正执行程序。

![997341ed0fa9018561c7120c19cfa2a7.jpg](img/997341ed0fa9018561c7120c19cfa2a7.jpg)

### ELF 格式和链接：理解链接过程 ###

程序最终是通过装载器变成指令和数据的，所以其实我们生成的可执行代码也并不仅仅是一条条的指令。

因为在 Linux 下，可执行文件和目标文件所使用的都是一种叫 **ELF**（Execuatable and Linkable File Format）的文件格式，中文名字叫**可执行与可链接文件格式**。 ELF 文件里面，存储在一个叫作**符号表**（Symbols Table）的位置里。符号表相当于一个地址簿，把名字和地址关联了起来。

ELF 文件格式把各种信息，分成一个一个的 Section 保存起来。ELF 有一个基本的文件头（File Header），用来表示这个文件的基本属性，比如是否是可执行文件，对应的 CPU、操作系统等等。除了这些基本属性之外，大部分程序还有这么一些 Section：

1. 首先是.text Section，也叫作代码段或者指令段（Code Section），用来保存程序的代码和指令；
2. 接着是.data Section，也叫作数据段（Data Section），用来保存程序里面设置好的初始化数据信息；
3. 然后就是.rel.text Secion，叫作重定位表（Relocation Table）。重定位表里，保留的是当前的文件里面，哪些跳转地址其实是我们不知道的。比如上面的 link_example.o 里面，我们在 main 函数里面调用了 add 和 printf 这两个函数，但是在链接发生之前，我们并不知道该跳转到哪里，这些信息就会存储在重定位表里；
4. 最后是.symtab Section，叫作符号表（Symbol Table）。符号表保留了我们所说的当前文件里面定义的函数名称和对应地址的地址簿。

链接器会扫描所有输入的目标文件，然后把所有符号表里的信息收集起来，构成一个全局的符号表。然后再根据重定位表，把所有不确定要跳转地址的代码，根据符号表里面存储的地址，进行一次修正。最后，把所有的目标文件的对应段进行一次合并，变成了最终的可执行代码。这也是为什么，可执行文件里面的函数调用的地址都是正确的。

![f62da9b29aa53218f8907851df27f912.jpeg](img/f62da9b29aa53218f8907851df27f912.jpeg)

在链接器把程序变成可执行文件之后，要装载器去执行程序就容易多了。装载器不再需要考虑地址跳转的问题，只需要解析 ELF 文件，把对应的指令和数据，加载到内存里面供 CPU 执行就可以了。

### 总结延伸 ###

Linux 下的 ELF 文件格式，而 Windows 的可执行文件格式是一种叫作 PE（Portable Executable Format）的文件格式。Linux 下的装载器只能解析 ELF 格式而不能解析 PE 格式。

### 推荐阅读 ###

程序员的自我修养——链接、装载和库，ch1-ch4

### 课后思考 ###

可以通过 readelf 读取出今天演示程序的符号表，看看符号表里都有哪些信息；然后通过 objdump 读取出今天演示程序的重定位表，看看里面又有哪些信息。

## 09 | 程序装载：“640K内存”真的不够用么？ ##

### 程序装载面临的挑战 ###

如何通过链接器，把多个文件合并成一个最终可执行文件。在运行这些可执行文件的时候，我们其实是通过一个装载器，解析ELF或者PE格式的可执行文件。装载器会把对应的指令和数据加载到内存里面来，让CPU置执行。

1. **可执行程序加载后占用的内存空间应该是连续的**。
2. **我们需要同时加载很多哥程序，并且不能让程序自己规定在内存中加载的位置**。

要满足两个基本的要求，可以在内存里面，找到一段连续的内存空间，然后分配给装载的程序，然后把这段连续的内存空间地址，和整个程序指令里规定的内存地址做一个映射。

指令里用到的内存地址叫做虚拟内存地址（Virtual Memory Address），实际内存硬件里面的空间地址，叫物理内存地址（Physical Memory Address）

### 内存分段 ###

找出一段连续的物理内存和虚拟内存地址进行映射的方法，**分段**（Segmentation）。这里的段，就是指系统分配出来的那个连续的内存空间。

分段办法的不足：

1. 内存碎片


### 内存分页 ###

内存碎片

### 总结延伸 ###

程序员的自我修养——链接、装载和库，ch1和ch6 代码装载

## 10 | 动态链接：程序内部的“共享单车” ##

### 链接可以分动、静，共享运行省内存 ###

解决程序装载到内存的时候，最根本的就是**内存空间不够用**。

在动态链接的过程中，我们想要“链接”的，不是存储在硬盘上的目标文件代码，而是加载到内存中的**共享库**（Shared Libraries）。这里的共享库重在“共享“这两个字。

这个加载到内存中的共享库会被很多个程序的指令调用到。在 Windows 下，这些共享库文件就是.dll 文件，也就是 Dynamic-Link Libary（DLL，动态链接库）。在 Linux 下，这些共享库文件就是.so 文件，也就是 Shared Object（一般我们也称之为动态链接库）。这两大操作系统下的文件名后缀，一个用了“动态链接”的意思，另一个用了“共享”的意思，正好覆盖了两方面的含义。

### 地址无关很重要，相对地址解烦恼 ###

要想要在程序运行的时候共享代码，也有一定的要求，就是这些机器码必须是“**地址无关**”的。也就是说，我们编译出来的共享库文件的指令代码，是地址无关码（Position-Independent Code）。无论加载在哪个内存地址，都能够正常执行。如果不是这样的代码，就是地址相关的代码。

大部分函数库其实都可以做到地址无关，因为它们都接受特定的输入，进行确定的操作，然后给出返回结果就好了。无论是实现一个向量加法，还是实现一个打印的函数，这些代码逻辑和输入的数据在内存里面的位置并不重要。

而常见的地址相关的代码，比如绝对地址代码（Absolute Code）、利用重定位表的代码等等，都是地址相关的代码。你回想一下我们之前讲过的重定位表。在程序链接的时候，我们就把函数调用后要跳转访问的地址确定下来了，这意味着，如果这个函数加载到一个不同的内存地址，跳转就会失败。

对于所有动态链接共享库的程序来讲，虽然我们的共享库用的都是同一段物理内存地址，但是在不同的应用程序里，它所在的虚拟内存地址是不同的。我们没办法、也不应该要求动态链接同一个共享库的不同程序，必须把这个共享库所使用的虚拟内存地址变成一致。如果这样的话，我们写的程序就必须明确地知道内部的内存地址分配。

动态代码库内部的变量和函数调用都很容易解决，我们只需要使用相对地址（Relative Address）就好了。各种指令中使用到的内存地址，给出的不是一个绝对的地址空间，而是一个相对于当前指令偏移量的内存地址。因为整个共享库是放在一段连续的虚拟内存地址中的，无论装载到哪一段地址，不同指令之间的相对地址都是不变的。

### PLT 和 GOT，动态链接的解决方案 ###

要实现动态链接共享库

	call   400550 <show_me_the_money@plt>

@plt 的关键字，代表了我们需要从 PLT，也就是程序链接表（Procedure Link Table）里面找要调用的函数。对应的地址呢，则是 400550 这个地址。

	400550:       ff 25 12 05 20 00       jmp    QWORD PTR [rip+0x200512]        # 600a68 <_GLOBAL_OFFSET_TABLE_+0x18>

GLOBAL_OFFSET_TABLE+0x18。这里的 GLOBAL_OFFSET_TABLE，就是全局偏移表。

在动态链接对应的共享库，我们在共享库的 data section 里面，保存了一张全局偏移表（GOT，Global Offset Table）。虽然共享库的代码部分的物理内存是共享的，但是数据部分是各个动态链接它的应用程序里面各加载一份的。所有需要引用当前共享库外部的地址的指令，都会查询 GOT，来找到当前运行程序的虚拟内存里的对应位置。而 GOT 表里的数据，则是在我们加载一个个共享库的时候写进去的。

不同的进程，调用同样的 lib.so，各自 GOT 里面指向最终加载的动态链接库里面的虚拟内存地址是不同的。

这样，虽然不同的程序调用的同样的动态库，各自的内存地址是独立的，调用的又都是同一个动态库，但是不需要去修改动态库里面的代码所使用的地址，而是各个程序各自维护好自己的 GOT，能够找到对应的动态库就好了。

![1144d3a2d4f3f4f87c349a93429805c8.jpg](img/1144d3a2d4f3f4f87c349a93429805c8.jpg)

我们的 GOT 表位于共享库自己的数据段里。GOT 表在内存里和对应的代码段位置之间的偏移量，始终是确定的。这样，我们的共享库就是地址无关的代码，对应的各个程序只需要在物理内存里面加载同一份代码。而我们又要通过各个可执行程序在加载时，生成的各不相同的 GOT 表，来找到它需要调用到的外部变量和函数的地址。

这是一个典型的、不修改代码，而是通过修改“**地址数据**”来进行关联的办法。它有点像我们在 C 语言里面用函数指针来调用对应的函数，并不是通过预先已经确定好的函数名称来调用，而是利用当时它在内存里面的动态地址来调用。

### 总结延伸 ###

在静态链接和程序装载之后，利用动态链接把我们的内存利用到了极致。同样功能的代码生成的共享库，我们只要在内存里面保留一份就好了。这样，我们不仅能够做到代码在开发阶段的复用，也能做到代码在运行阶段的复用。

已经把程序怎么从源代码变成指令、数据，并装载到内存里面，由 CPU 一条条执行下去的过程讲完了。

### 推荐阅读 ###

想要更加深入地了解动态链接，我推荐你可以读一读《程序员的自我修养：链接、装载和库》的第 7 章，里面深入地讲解了，动态链接里程序内的数据布局和对应数据的加载关系。

### 课后思考 ###

像动态链接这样通过修改“地址数据”来进行间接跳转，去调用一开始不能确定位置代码的思路，你在应用开发中使用过吗？

### 精选留言 ###



## 11 | 二进制编码：“手持两把锟斤拷，口中疾呼烫烫烫”？ ##

### 理解二进制的“逢二进一” ###

**短除法**

**原码表示法**

* 缺点：0 可以用两个不同的编码来表示，1000 代表 0， 0000 也代表 0。

仍然通过最左侧第一位的 0 和 1，来判断这个数的正负。但是，我们不再把这一位当成单独的符号位，在剩下几位计算出的十进制前加上正负号，而是在计算整个二进制值的时候，在左侧最高位前面加个负号。

用补码来表示负数，使得我们的整数相加变得很容易，不需要做任何特殊处理。

### 字符串的表示，从编码到数字 ###

最典型的例子就是**字符串**（Character String）。

**不管是整数也好，浮点数也好，采用二进制序列化会比存储文本省下不少空间。**

**字符集**（Charset）和**字符编码**（Character Encoding）：

* 字符集：表示的可以是字符的一个集合。
* 字符编码：对于字符集里的这些字符，怎么一一用二进制表示出来的一个字典。

### 总结延伸 ###

二进制编码的方式，表示任意的信息。只要建立起字符集和字符编码，并且得到大家的认同，我们就可以在计算机里面表示这样的信息了。

我们在计算机组成里面，关心的不只是数值和字符的逻辑表示，更要弄明白，在硬件层面，这些数值和我们一直提的晶体管和电路有什么关系。

### 推荐阅读 ###

关于二进制和编码，我推荐你读一读《编码：隐匿在计算机软硬件背后的语言》。从电报机到计算机，这本书讲述了很多计算设备的历史故事，当然，也包含了二进制及其背后对应的电路原理。

## 12 | 理解电路：从电报机到门电路，我们如何做到“千里传信”？ ##

所有最终执行的程序其实都是使用“0”和“1”这样的二进制代码来表示的。

### 从信使到电报，我们怎么做到“千里传书”？ ###

从信息编码的角度来说，电报传输的信号有两种，一种是短促的点信号（dot 信号），一种是长一点的划信号（dash 信号）。

### 理解继电器，给跑不动的信号续一秒 ###

为了能够实现这样**接力传输信号**，在电路里面，工程师们造了一个叫作**继电器**（Relay）的设备。

继电器还有一个名字就叫作**电驿**，这个“驿”就是驿站的驿，可以说非常形象了。这个接力的策略不仅可以用在电报中，在通信类的科技产品中其实都可以用到。

通过这些线圈和开关，我们也可以很容易地创建出 “与（AND）”“或（OR）”“非（NOT）”这样的逻辑。我们在输入端的电路上，提供串联的两个开关，只有两个开关都打开，电路才接通，输出的开关也才能接通，这其实就是模拟了计算机里面的**“与”**操作。

我们在输入端的电路，提供两条独立的线路到输出端，两条线路上各有一个开关，那么任何一个开关打开了，到输出端的电路都是接通的，这其实就是模拟了计算机中的**“或”**操作。

当我们把输出端的“螺旋线圈 + 磁性开关”的组合，从默认关掉，只有通电有了磁场之后打开，换成默认是打开通电的，只有通电之后才关闭，我们就得到了一个计算机中的**“非”**操作。输出端开和关正好和输入端相反。这个在数字电路中，也叫作**反向器**（Inverter）。

### 总结延伸 ###

可以说，电报是现代计算机的一个最简单的原型。它和我们现在使用的现代计算机有很多相似之处。我们通过电路的“开”和“关”，来表示“1”和“0”。就像晶体管在不同的情况下，表现为导电的“1”和绝缘的“0”的状态。

我们通过电报机这个设备，看到了如何通过“螺旋线圈 + 开关”，来构造基本的逻辑电路，我们也叫门电路。一方面，我们可以通过继电器或者中继，进行长距离的信号传输。另一方面，我们也可以通过设置不同的线路和开关状态，实现更多不同的信号表示和处理方式，这些线路的连接方式其实就是我们在数字电路中所说的门电路。而这些门电路，也是我们创建 CPU 和内存的基本逻辑单元。我们的各种对于计算机二进制的“0”和“1”的操作，其实就是来自于门电路，叫作组合逻辑电路。

### 推荐阅读 ###

《编码：隐匿在计算机软硬件背后的语言》的第 6～11 章

## 13 | 加法器：如何像搭乐高一样搭电路（上） ##

在计算机硬件层面设计最基本的单元，门电路。门电路非常简单，只能做简单的 “与（AND）”“或（OR）”“NOT（非）”和“异或（XOR）”，这样最基本的单比特逻辑运算。

### 异或门和半加器 ###

基础门电路，输入都是两个单独的 bit，输出是一个单独的 bit。如果我们要对 2 个 8 位（bit）的数，计算与、或、非这样的简单逻辑运算，其实很容易。只要连续摆放 8 个开关，来代表一个 8 位数。这样的两组开关，从左到右，上下单个的位开关之间，都统一用“与门”或者“或门”连起来，就是两个 8 位数的 AND 或者 OR 的运算了。

讲与、或、非门的时候，我们很容易就能和程序里面的“AND（通常是 & 符号）”“ OR（通常是 | 符号）”和“ NOT（通常是 ! 符号）”对应起来。可能你没有想过，为什么我们会需要“异或（XOR）”。其实，异或门就是一个最简单的整数加法，所需要使用的基本门电路。

### 全加器 ###

半加器可以解决个位的加法问题，但是如果放到二位上来说

半加器（Half Adder），全加器（Full Adder）

**我们用两个半加器和一个或门，就能组合成一个全加器。**

这样，通过两个半加器和一个或门，我们就得到了一个，能够接受进位信号、加数和被加数，这样三个数组成的加法。这就是我们需要的全加器。

有了全加器，我们要进行对应的两个 8 bit 数的加法就很容易了。我们只要把 8 个全加器串联起来就好了。个位的全加器的进位信号作为二位全加器的输入信号，二位全加器的进位信号再作为四位的全加器的进位信号。

### 总结延伸 ###

通过门电路来搭建算术计算的一个小功能，就好像搭乐高积木一样。

用两个门电路，搭出一个半加器，就好像我们拿两块乐高，叠在一起，变成一个长方形的乐高，这样我们就有了一个新的积木组件，柱子。我们再用两个柱子和一个长条的积木组合一下，就变成一个积木桥。然后几个积木桥串接在一起，又成了积木楼梯。

这其实就是计算机中，无论软件还是硬件中一个很重要的设计思想，**分层**。

![8a7740f698236fda4e5f900d88fdf194.jpg](img/8a7740f698236fda4e5f900d88fdf194.jpg)

在硬件层面，我们通过门电路、半加器、全加器一层层搭出了加法器这样的功能组件。我们把这些用来做算术逻辑计算的组件叫作 ALU，也就是算术逻辑单元。当进一步打造强大的 CPU 时，我们不会再去关注最细颗粒的门电路，只需要把门电路组合而成的 ALU，当成一个能够完成基础计算的黑盒子就可以了。

### 补充阅读 ###

出于性能考虑，实际 CPU 里面使用的加法器，比起我们今天讲解的电路还有些差别，会更复杂一些。真实的加法器，使用的是一种叫作**超前进位加法器**的东西。你可以找到北京大学在 Coursera 上开设的《计算机组成》课程中的 Video-306 “加法器优化”一节，了解一下超前进位加法器的实现原理，以及我们为什么要使用它。

## 14 | 乘法器：如何像搭乐高一样搭电器（下） ##

### 顺序乘法的实现过程 ###

在这个乘法器的实现过程里，我们其实就是把乘法展开，变成了“**加法 + 位移**”来实现。我们用的是 4 位数，所以要进行 4 组“位移 + 加法”的操作。而且这 4 组操作还不能同时进行。因为**下一组的加法要依赖上一组的加法后的计算结果，下一组的位移也要依赖上一组的位移的结果。这样，整个算法是“顺序”的，每一组加法或者位移的运算都需要一定的时间。**

### 并行加速方法 ###

和软件开发里面改算法一样，在涉及 CPU 和电路的时候，我们可以改电路。

32 位数虽然是 32 次加法，但是我们可以让很多加法同时进行。回到这一讲开始，我们把位移和乘法的计算结果加到中间结果里的方法，32 位整数的乘法，其实就变成了 32 个整数相加。

### 电路并行 ###

之所以我们的计算会慢，核心原因其实是“顺序”计算，也就是说，要等前面的计算结果完成之后，我们才能得到后面的计算结果。

最典型的例子就是我们上一讲讲的加法器。每一个全加器，都要等待上一个全加器，把对应的进入输入结果算出来，才能算下一位的输出。位数越多，越往高位走，等待前面的步骤就越多，这个等待的时间有个专门的名词，叫作**门延迟**（Gate Delay）。

每通过一个门电路，我们就要等待门电路的计算结果，就是一层的门电路延迟，我们一般给它取一个“T”作为符号。一个全加器，其实就已经有了 3T 的延迟（进位需要经过 3 个门电路）。而 4 位整数，最高位的计算需要等待前面三个全加器的进位结果，也就是要等 9T 的延迟。如果是 64 位整数，那就要变成 63×3=189T 的延迟。

除了门延迟之外，还有一个问题就是**时钟频率**。在上面的顺序乘法计算里面，如果我们想要用更少的电路，计算的中间结果需要保存在寄存器里面，然后等待下一个时钟周期的到来，控制测试信号才能进行下一次移位和加法，这个延迟比上面的门延迟更可观。

**我们有什么办法可以解决这个问题呢？**实际上，在我们进行加法的时候，如果相加的两个数是确定的，那高位是否会进位其实也是确定的。计算机是连结的各种线路。我们不用让计算机模拟人脑的思考方式，来连结线路。

**那怎么才能把线路连结得复杂一点，让高位和低位的计算同时出结果呢？**怎样才能让高位不需要等待低位的进位结果，而是把低位的所有输入信号都放进来，直接计算出高位的计算结果和进位结果呢？

只要把进位部分的电路完全展开就好了。我们的半加器到全加器，再到加法器，都是用最基础的门电路组合而成的。门电路的计算逻辑，可以像我们做数学里面的多项式乘法一样完全展开。在展开之后呢，我们可以把原来需要较少的，但是有较多层前后计算依赖关系的门电路，展开成需要较多的，但是依赖关系更少的门电路。

![6e9f630389c566d72f57f5b196a711a8.jpeg](img/6e9f630389c566d72f57f5b196a711a8.jpeg)

通过把电路变复杂，就解决了延迟的问题。

这个优化，本质上是利用了电路天然的并行性。电路只要接通，输入的信号自动传播到了所有接通的线路里面，这其实也是硬件和软件最大的不同。

之前很多同学在我们讨论计算机的性能问题的时候，都提到，为什么晶体管的数量增加可以优化计算机的计算性能。实际上，这里的门电路展开和上面的并行计算乘法都是很好的例子。我们通过更多的晶体管，就可以拿到更低的门延迟，以及用更少的时钟周期完成一个计算指令。

### 总结延伸 ###

通过 ALU 和门电路，搭建出来了乘法器。如果愿意的话，我们可以把很多在生活中不得不顺序执行的事情，通过简单地连结一下线路，就变成并行执行了。这是因为，硬件电路有一个很大的特点，那就是信号都是实时传输的。

通过精巧地设计电路，用较少的门电路和寄存器，就能够计算完成乘法这样相对复杂的运算。是用更少更简单的电路，但是需要更长的门延迟和时钟周期；还是用更复杂的电路，但是更短的门延迟和时钟周期来计算一个复杂的指令，这之间的权衡，其实就是计算机体系结构中 RISC 和 CISC 的经典历史路线之争。

### 推荐阅读 ###

《计算机组成与设计：硬件 / 软件接口》的 3.3 节。

### 课后思考 ###


## 15 | 浮点数和定点数（上）：怎么用有限的Bit表示尽可能多的信息？ ##

### 浮点数的不精确性 ###

现在用的计算机通常用 16/32 个比特（bit）来表示一个数。答案很显然是不能。32 个比特，只能表示 2 的 32 次方个不同的数，差不多是 40 亿个。如果表示的数要超过这个数，就会有两个不同的数的二进制表示是一样的。那计算机可就会一筹莫展，不知道这个数到底是多少。

**我到底应该让这 40 亿个数映射到实数集合上的哪些数，在实际应用中才能最划得来呢？**

### 定点数的表示 ###

我们用 4 个比特来表示 0～9 的整数，那么 32 个比特就可以表示 8 个这样的整数。然后我们把最右边的 2 个 0～9 的整数，当成小数部分；把左边 6 个 0～9 的整数，当成整数部分。这样，我们就可以用 32 个比特，来表示从 0 到 999999.99 这样 1 亿个实数了。

![f5a0b0f2188ebe0d18f4424578a588b3.jpg](img/f5a0b0f2188ebe0d18f4424578a588b3.jpg)

这种用二进制来表示十进制的编码方式，叫作**BCD 编码**（Binary-Coded Decimal）。最常用的是在超市、银行这样需要用小数记录金额的情况里。

1. 这样的表示方式有点“浪费”
2. 这样的表示方式没办法同时表示很大的数字和很小的数字

### 浮点数的表示 ###

**那么，我们有没有一个办法，既能够表示很小的数，又能表示很大的数呢？**

**浮点数**（Floating Point），也就是**float类型**。

浮点数的科学计数法的表示，有一个 IEEE 的标准，它定义了两个基本的格式。

1. 一个是用 32 比特表示单精度的浮点数，也就是我们常常说的 float 或者 float32 类型。
2. 另外一个是用 64 比特表示双精度的浮点数，也就是我们平时说的 double 或者 float64 类型。

单精度的 32 个比特可以分成三部分。

1. 第一部分是一个**符号位**，用来表示是正数还是负数。我们一般用 **s** 来表示。所有的浮点数都是有符号的。
2. 接下来是一个 8 个比特组成的**指数位**。一般用**e**来表示。8 个比特能够表示的整数空间，就是 0～255。我们在这里用 1～254 映射到 -126～127 这 254 个有正有负的数上。
3. 最后，是一个 23 个比特组成的**有效数位**。我们用 **f** 来表示。

浮点数可以表示为：(−1)^s×1.f×2^e

要表示 0 和一些特殊的数，我们就要用上在 e 里面留下的 0 和 255 这两个表示，这两个表示其实是两个标记位。在 e 为 0 且 f 为 0 的时候，我们就把这个浮点数认为是 0。

![f922249a89667c4d10239eb8840dc94c.jpg](img/f922249a89667c4d10239eb8840dc94c.jpg)

### 总结延伸 ###

正是因为这个数对应的小数点的位置是“浮动”的，它才被称为浮点数。随着指数位 e 的值的不同，小数点的位置也在变动。对应的，前面的 BCD 编码的实数，就是小数点固定在某一位的方式，我们也就把它称为**定点数**。

### 推荐阅读 ###

计算机组成与设计：硬件 / 软件接口的 3.5.1 节，了解浮点数

### 课后思考 ###

对于 BCD 编码的定点数，如果我们用 7 个比特来表示连续两位十进制数，也就是 00～99，是不是可以让 32 比特表示更大一点的数据范围？如果我们还需要表示负数，那么一个 32 比特的 BCD 编码，可以表示的数据范围是多大？

## 16 | 浮点数和定点数（下）：深入理解浮点数到底有什么用？ ##

**浮点数的近似值究竟是怎么算出来的？浮点数的加法计算又是怎么回事儿？**

### 浮点数的二进制转化 ###

十进制的浮点数怎么表示成二进制。

我们输入一个任意的十进制浮点数，背后都会对应一个二进制表示。比方说，我们输入了一个十进制浮点数 9.1。那么按照之前的讲解，在二进制里面，我们应该把它变成一个“**符号位 s+ 指数位 e+ 有效位数 f**”的组合。

1. 我们要做的，就是把这个数变成二进制。
	* 我们把这个数的整数部分，变成一个二进制。这里的 9，换算之后就是 1001。
	* 把对应的小数部分也换算成二进制。小数怎么换成二进制呢？和整数的二进制表示采用“除以 2，然后看余数”的方式相比，小数部分转换成二进制是用一个相似的反方向操作，就是乘以 2，然后看看是否超过 1。如果超过 1，我们就记下 1，并把结果减去 1，进一步循环操作。在这里，我们就会看到，0.1 其实变成了一个无限循环的二进制小数，0.000110011。这里的“0011”会无限循环下去。
	* 然后，我们把整数部分和小数部分拼接在一起，9.1 这个十进制数就变成了 1001.000110011…这样一个二进制表示。
2. 浮点数其实是用二进制的科学计数法来表示的，所以我们可以把小数点左移三位，这个数就变成了：1.001000110011…×2^3



### 浮点数的加法和精度损失 ###

先对齐、再计算。

### Kahan Summation 算法 ###

### 总结延伸 ###

浮点数的表示、加法计算以及可能会遇到的精度损失问题。可以看到，虽然浮点数能够表示的数据范围变大了很多，但是在实际应用的时候，由于存在精度损失，会导致加法的结果和我们的预期不同，乃至于完全没有加上的情况。

* 一般情况下，在实践应用中，对于需要精确数值的，比如银行存款、电商交易，我们都会使用定点数或者整数类型。
* 而浮点数呢，则更适合我们不需要有一个非常精确的计算结果的情况。因为在真实的物理世界里，很多数值本来就不是精确的，我们只需要有限范围内的精度就好了。

### 推荐阅读 ###

《计算机组成与设计 硬件 / 软件接口》的 3.5.2 和 3.5.3 小节。

原理篇：处理器

## 17 | 建立数据通路（上）：指令+运算=CPU ##




## 18 | 建立数据通路（中）：指令+运算=CPU ##

## 19 | 建立数据通路（下）：指令+运算=CPU ##

通过一个时钟信号，我们可以实现计数器，这个会成为我们的 PC 寄存器。然后，我们还需要一个能够帮我们在内存里面寻找指定数据地址的译码器，以及解析读取到的机器指令的译码器。这样，我们就能把所有学习到的硬件组件串联起来，变成一个 CPU，实现我们在计算机指令的执行部分的运行步骤。

### PC寄存器所需要的计数 ###

PC寄存器（程序计数器），有了时钟信号，我们可以提供定时的输入；有了 D 型触发器，我们可以在时钟信号控制的时间点写入数据。我们把这两个功能组合起来，就可以实现一个自动的计数器了。

# 答疑与加餐 #

特别加餐 | 

## FAQ第一期 | 学与不学，知识就在那里，不如就先学好了 ##

## 特别加餐 | 我的一天怎么过？ ##

### 精选留言 ###

**Q**

👍 真实&典型的一天
有人的时间安排是以自我为中心，比较严格地去控制每件事占用的时间。老师的时间安排也是这样的吗？您怎么看这种做法？

**A**

工作中的事情，我大致会分成三类：

1. 一类是重要的需要大块时间的事情，比如产品的RoadMap，大的系统设计。
2. 一类是即时响应性的工作。比如来自各种内外部的邮件，或者随时有同事来问或者讨论的各类问题。
3. 一类是长期必须完成的工作，包括和同事1对1沟通，日常的周会，招聘。

工作安排时间希望尽量通过做第3点来减少第2点的时间。并且尽量能多花时间在1上。

每天都会列一些To-Do，然后尽量保障能够清掉2-3个。如果连续一段时间觉得自己在1上花得少，就需要反思手上有哪些工作是可以交给其他同事得。以及是否有些事情从整个公司团队层面就不该做。

**Q**

老师，几本操作系统推荐书

**A**

操作系统的推荐可以去看看刘超老师的Linux操作系统课。

当然，最经典的教材也还是《现代操作系统》

**Q**

不太理解 访存 为什么在 执行 后面

**A**

你好，这个5阶段通常是指MIPS这样的RISC的一个简化的模型。开始执行指令之后，才会知道要从内存的什么地址读取数据，这个时候才会进入访存阶段。其实访问读取完成之后会继续进行执行过程。

而很多EX计算的指令都直接从寄存器读取数据，所以不需要访存。

# 原理篇：存储与I/O系统 #

## 35 | 存储器层次结构全景：数据存储的大金字塔长什么样？ ##

### 理解存储器的层次结构 ###

我们常常把 CPU 比喻成计算机的“大脑”。我们思考的东西，就好比 CPU 中的寄存器（Register）。寄存器与其说是存储器，其实它更像是 CPU 本身的一部分，只能存放极其有限的信息，但是速度非常快，和 CPU 同步。

而我们大脑中的记忆，就好比 CPU Cache（CPU 高速缓存，我们常常简称为“缓存”）。CPU Cache 用的是一种叫作 SRAM（Static Random-Access Memory，静态随机存取存储器）的芯片。

### SRAM ###

在 CPU 里，通常会有 L1、L2、L3 这样三层高速缓存。每个 CPU 核心都有一块属于自己的 L1 高速缓存，通常分成**指令缓存**和**数据缓存**，分开存放 CPU 使用的指令和数据。

### DRAM ###

内存用的芯片和 Cache 有所不同，它用的是一种叫作 DRAM（Dynamic Random Access Memory，动态随机存取存储器）的芯片，比起 SRAM 来说，它的密度更高，有更大的容量，而且它也比 SRAM 芯片便宜不少。

DRAM 被称为“动态”存储器，是因为 DRAM 需要靠不断地“刷新”，才能保持数据被存储起来。DRAM 的一个比特，只需要一个晶体管和一个电容就能存储。所以，DRAM 在同样的物理空间下，能够存储的数据也就更多，也就是存储的“密度”更大。但是，因为数据是存储在电容里的，电容会不断漏电，所以需要定时刷新充电，才能保持数据不丢失。DRAM 的数据访问电路和刷新电路都比 SRAM 更复杂，所以访问延时也就更长。

### 存储器的层级结构 ###

这样，各个存储器只和相邻的一层存储器打交道，并且随着一层层向下，存储器的容量逐层增大，访问速度逐层变慢，而单位存储成本也逐层下降，也就构成了我们日常所说的存储器层次结构。

### 使用存储器的时候，该如何权衡价格和性能？ ###

存储器在不同层级之间的性能差异和价格差异，都至少在一个数量级以上。L1 Cache 的访问延时是 1 纳秒（ns），而内存就已经是 100 纳秒了。在价格上，这两者也差出了 400 倍。

### 总结延伸 ###

常常把 CPU 比喻成高速运转的大脑，那么和大脑同步的寄存器（Register），就存放着我们当下正在思考和处理的数据。而 L1-L3 的 CPU Cache，好比存放在我们大脑中的短期到长期的记忆。我们需要小小花费一点时间，就能调取并进行处理。

我们自己的书桌书架就好比计算机的内存，能放下更多的书也就是数据，但是找起来和看起来就要慢上不少。而图书馆更像硬盘这个外存，能够放下更多的数据，找起来也更费时间。从寄存器、CPU Cache，到内存、硬盘，这样一层层下来的存储器，速度越来越慢，空间越来越大，价格也越来越便宜。

这三个“越来越”的特性，使得我们在组装计算机的时候，要组合使用各种存储设备。越是快且贵的设备，实际在一台计算机里面的存储空间往往就越小。而越是慢且便宜的设备，在实际组装的计算机里面的存储空间就会越大。

### 补充阅读 ###

关于不同存储器的访问延时数据

1. 第一个是 Peter Novig 的Teach Yourself Programming in Ten Years。我推荐你在了解这些数据之后读一读这篇文章。这些数字随着摩尔定律的发展在不断缩小，但是在数量级上仍然有着很强的参考价值。
2. 第二个是 Jeff Dean 的Build Software Systems at Google and Lessons Learned。这份 PPT 中不仅总结了这些数字，还有大量的硬件故障、高可用和系统架构的血泪经验。尽管这是一份 10 年前的 PPT，但也非常值得阅读。

### 课后思考 ###

在上世纪 80～90 年代，3.5 寸的磁盘大行其道。它的存储空间只有 1.44MB，比起当时 40MB 的硬盘，它却被大家认为是“海量”存储的主要选择。你猜一猜这是为什么？

## 36 | 局部性原理：数据库性能跟不上，加个缓存就好了？ ##

平时进行服务端软件开发的时候，我们通常会把数据存储在数据库里。而服务端系统遇到的第一个性能瓶颈，往往就发生在访问数据库的时候。这个时候，大部分工程师和架构师会拿出一种叫作“缓存”的武器，通过使用 Redis 或者 Memcache 这样的开源软件，在数据库
前面提供一层缓存的数据，来缓解数据库面临的压力，提升服务端的程序性能。

那么，不知道你有没有想过，这种添加缓存的策略一定是有效的吗？或者说，这种策略在什么情况下是有效的呢？如果从理论角度去分析，添加缓存一定是我们的最佳策略么？进一步地，如果我们对于访问性能的要求非常高，希望数据在 1 毫秒，乃至 100 微秒内完成处理，我们还能用这个添加缓存的策略么？

### 理解局部性原理 ###

**我们能不能既享受 CPU Cache 的速度，又享受内存、硬盘巨大的容量和低廉的价格呢？**

想要同时享受到这三点，那就是，存储器中数据的**局部性原理**（Principle of Locality）。我们可以利用这个局部性原理，来制定管理和访问数据的策略。这个局部性原理包括**时间局部性**（temporal locality）和**空间局部性**（spatial locality）这两种策略。

* 时间局部性：如果一个数据被访问了，那么它在短时间内还会被再次访问。
* 空间局部性：这个策略是说，如果一个数据被访问了，那么和它相邻的数据也很快会被访问。

有了时间局部性和空间局部性，我们不用再把所有数据都放在内存里，也不用都放在 HDD 硬盘上，而是把访问次数多的数据，放在贵但是快一点的存储器里，把访问次数少的数据，放在慢但是大一点的存储器里。这样组合使用内存、SSD 硬盘以及 HDD 硬盘，使得我们可以用最低的成本提供实际所需要的数据存储、管理和访问的需求。

### 如何花最少的钱，装下亚马逊的所有商品？ ###

1. 假设亚马逊有6亿件商品
2. 每件商品需要4MB的存储空间
3. 一共需要2400TB（=6亿*4MB）
4. 把所有数据都放在内存中，需要3600万美元（=  2400TB/1MB  × 0.015 美元  =  3600 万美元）
	* 这 6 亿件商品中，不是每一件商品都会被经常访问
	* 如果我们只在内存里放前 1% 的热门商品，也就是 600 万件热门商品，而把剩下的商品，放在机械式的 HDD 硬盘上
	* 需要的存储成本就下降到 45.6 万美元（  =  3600 万美元 × 1% + 2400TB / 1MB × 0.00004 美元）
5. 一旦内存里面放不下了，我们就把最长时间没有在内存中被访问过的数据，从内存中移走，这个其实就是我们常用的 LRU（Least Recently Used）缓存算法。

只放 600 万件商品真的可以满足我们实际的线上服务请求吗？这个就要看 LRU 缓存策略的缓存命中率（Hit Rate/Hit Ratio）了，也就是访问的数据中，可以在我们设置的内存缓存中找到的，占有多大比例。

内存的随机访问请求需要 100ns。在极限情况下，内存可以支持 1000 万次随机访问。

内存的随机访问请求需要 100ns。这也就意味着，在极限情况下，内存可以支持 1000 万次随机访问。我们用了 24TB 内存，如果 8G 一条的话，意味着有 3000 条内存，可以支持每秒 300 亿次（  =  24TB/8GB  ×  1s/100ns）访问。以亚马逊 2017 年 3 亿的用户数来看，我们估算每天的活跃用户为 1 亿，这 1 亿用户每人平均会访问 100 个商品，那么平均每秒访问的商品数量，就是 12 万次。

但是如果数据没有命中内存，那么对应的数据请求就要访问到 HDD 磁盘了。刚才的图表中，我写了，一块 HDD 硬盘只能支撑每秒 100 次的随机访问，2400TB 的数据，以 4TB 一块磁盘来计算，有 600 块磁盘，也就是能支撑每秒 6 万次（  =  2400TB/4TB  × 1s/10ms  ）的随机访问。

这就意味着，所有的商品访问请求，都直接到了 HDD 磁盘，HDD 磁盘支撑不了这样的压力。我们至少要 50% 的缓存命中率，HDD 磁盘才能支撑对应的访问次数。不然的话，我们要么选择添加更多数量的 HDD 硬盘，做到每秒 12 万次的随机访问，或者将 HDD 替换成 SSD 硬盘，让单个硬盘可以支持更多的随机访问请求。

在实际的应用程序中，查看一个商品的数据可能意味着不止一次的随机内存或者随机磁盘的访问。对应的数据存储空间也不止要考虑数据，还需要考虑维护数据结构的空间，而缓存的命中率和访问请求也要考虑均值和峰值的问题。

通过这个估算过程，你需要理解，如何进行存储器的硬件规划。你需要考虑硬件的成本、访问的数据量以及访问的数据分布，然后根据这些数据的估算，来组合不同的存储器，能用尽可能低的成本支撑所需要的服务器压力。而当你用上了数据访问的局部性原理，组合起了多种存储器，你也就理解了怎么基于存储器层次结构，来进行硬件规划了。

### 总结延伸 ###

计算机存储器层次结构中最重要的一个优化思路，就是局部性原理。

在实际的计算机日常的开发和应用中，我们对于数据的访问总是会存在一定的局部性。有时候，这个局部性是时间局部性，就是我们最近访问过的数据还会被反复访问。有时候，这个局部性是空间局部性，就是我们最近访问过数据附近的数据很快会被访问到。

而局部性的存在，使得我们可以在应用开发中使用缓存这个有利的武器。比如，通过将热点数据加载并保留在速度更快的存储设备里面，我们可以用更低的成本来支撑服务器。

### 推荐阅读 ###

《计算机组成与设计：硬件 / 软件接口》

## 37 | 高速缓存（上）：“4毫秒”究竟值多少钱？ ##

看一个 3 行的小程序，这个程序里的循环 1 和循环 2，运行所花费的时间会差多少？

	int[] arr = new int[64 * 1024 * 1024];
	// 循环1
	for (int i = 0; i < arr.length; i++) arr[i] *= 3;
	// 循环2
	for (int i = 0; i < arr.length; i += 16) arr[i] *= 3

在这段 Java 程序中，我们首先构造了一个 64×1024×1024 大小的整型数组。在循环 1 里，我们遍历整个数组，将数组中每一项的值变成了原来的 3 倍；在循环 2 里，我们每隔 16 个索引访问一个数组元素，将这一项的值变成了原来的 3 倍。

按道理来说，循环 2 只访问循环 1 中 1/16 的数组元素，只进行了循环 1 中 1/16 的乘法计算，那循环 2 花费的时间应该是循环 1 的 1/16 左右。但是实际上，循环 1 在我的电脑上运行需要 50 毫秒，循环 2 只需要 46 毫秒。这两个循环花费时间之差在 15% 之内。

为什么会有这 15% 的差异呢？这和我们今天要讲的 CPU Cache 有关。之前我们看到了内存和硬盘之间存在的巨大性能差异。在 CPU 眼里，内存也慢得不行。于是，聪明的工程师们就在 CPU 里面嵌入了 CPU Cache（高速缓存），来解决这一问题。

### 我们为什么需要高速缓存? ###

能真实地把 CPU 的性能提升用起来，而不是让它在那儿空转，我们在现代 CPU 中引入了高速缓存。

从 CPU Cache 被加入到现有的 CPU 里开始，内存中的指令、数据，会被加载到 L1-L3 Cache 中，而不是直接由 CPU 访问内存去拿。在 95% 的情况下，CPU 都只需要访问 L1-L3 Cache，从里面读取指令和数据，而无需访问内存。要注意的是，这里我们说的 CPU Cache 或者 L1/L3 Cache，不是一个单纯的、概念上的缓存（比如之前我们说的拿内存作为硬盘的缓存），而是指特定的由 SRAM 组成的物理芯片。

运行程序的时间主要花在了将对应的数据从内存中读取出来，加载到 CPU Cache 里。CPU 从内存中读取数据到 CPU Cache 的过程中，是一小块一小块来读取数据的，而不是按照单个数组元素来读取数据的。这样一小块一小块的数据，在 CPU Cache 里面，我们把它叫作 Cache Line（缓存块）。

在我们日常使用的 Intel 服务器或者 PC 里，Cache Line 的大小通常是 64 字节。而在上面的循环 2 里面，我们每隔 16 个整型数计算一次，16 个整型数正好是 64 个字节。于是，循环 1 和循环 2，需要把同样数量的 Cache Line 数据从内存中读取到 CPU Cache 中，最终两个程序花费的时间就差别不大了。

### Cache 的数据结构和读取过程是什么样的？ ###



### 减少 4 毫秒，公司挣了多少钱? ###

高频交易公司

其实，只要 350 微秒的差异，就足够高频交易公司用来进行无风险套利了。而 350 微秒，如果用来进行 100 纳秒一次的内存访问，大约只够进行 3500 次。而引入 CPU Cache 之后，我们可以进行的数据访问次数，提升了数十倍，使得各种交易策略成为可能。

### 总结延伸 ###

程序的性能瓶颈，来自使用 DRAM 芯片的内存访问速度。

根据摩尔定律，自上世纪 80 年代以来，CPU 和内存的性能鸿沟越拉越大。于是，现代 CPU 的设计者们，直接在 CPU 中嵌入了使用更高性能的 SRAM 芯片的 Cache，来弥补这一性能差异。通过巧妙地将内存地址，拆分成“索引 + 组标记 + 偏移量”的方式，使得我们可以将很大的内存地址，映射到很小的 CPU Cache 地址里。而 CPU Cache 带来的毫秒乃至微秒级别的性能差异，又能带来巨大的商业利益，十多年前的高频交易行业就是最好的例子。

在搞清楚从内存加载数据到 Cache，以及从 Cache 里读取到想要的数据之后，我们又要面临一个新的挑战了。CPU 不仅要读数据，还需要写数据，我们不能只把数据写入到 Cache 里面就结束了。下一讲，我们就来仔细讲讲，CPU 要写入数据的时候，怎么既不牺牲性能，又能保证数据的一致性。

### 推荐阅读 ###

如果想深入了解 CPU 和内存之间的访问性能，你可以阅读What Every Programmer Should Know About Memory。

现代 CPU 已经很少使用直接映射 Cache 了，通常用的是组相连 Cache（set associative cache），想要了解组相连 Cache，你可以阅读《计算机组成与设计：硬件 / 软件接口》的 5.4.1 小节。

### 课后思考 ###

对于二维数组的访问，按行迭代和按列迭代的访问性能是一样的吗？你可以写一个程序测试一下，并思考一下原因。

## 38 | 高速缓存（下）：你确定你的数据更新了么？ ##

对于一些表示自己深入了解和擅长多线程的同学，我经常会问这样一个面试题：“**volatile 这个关键字有什么作用？**”

通常理解的错误：

* 一个是把 volatile 当成一种锁机制，认为给变量加上了 volatile，就好像是给函数加了 sychronized 关键字一样，不同的线程对于特定变量的访问会去加锁；
* 另一个是把 volatile 当成一种原子化的操作机制，认为加了 volatile 之后，对于一个变量的自增的操作就会变成原子性的了。

	// 一种错误的理解，是把volatile关键词，当成是一个锁，可以把long/double这样的数的操作自动加锁
	private volatile long synchronizedValue = 0;
	
	// 另一种错误的理解，是把volatile关键词，当成可以让整数自增的操作也变成原子性的
	private volatile int atomicInt = 0;
	amoticInt++;

volatile 关键字的最核心知识点，要关系到 Java 内存模型（JMM，Java Memory Model）上。

### “隐身”的变量 ###

	public class VolatileTest {
	    private static volatile int COUNTER = 0;
	
	    public static void main(String[] args) {
	        new ChangeListener().start();
	        new ChangeMaker().start();
	    }
	
	    static class ChangeListener extends Thread {
	        @Override
	        public void run() {
	            int threadValue = COUNTER;
	            while ( threadValue < 5){
	                if( threadValue!= COUNTER){
	                    System.out.println("Got Change for COUNTER : " + COUNTER + "");
	                    threadValue= COUNTER;
	                }
	            }
	        }
	    }
	
	    static class ChangeMaker extends Thread{
	        @Override
	        public void run() {
	            int threadValue = COUNTER;
	            while (COUNTER <5){
	                System.out.println("Incrementing COUNTER to : " + (threadValue+1) + "");
	                COUNTER = ++threadValue;
	                try {
	                    Thread.sleep(500);
	                } catch (InterruptedException e) { e.printStackTrace(); }
	            }
	        }
	    }
	}

## 39 | MESI协议：如何让多核CPU的高速缓存保持一致？ ##

多核 CPU 里的每一个 CPU 核，都有独立的属于自己的 L1 Cache 和 L2 Cache。多个 CPU 之间，只是共用 L3 Cache 和主内存。

CPU Cache 解决的是内存访问速度和 CPU 的速度差距太大的问题。而多核 CPU 提供的是，在主频难以提升的时候，通过增加 CPU 核心来提升 CPU 的吞吐率的办法。我们把多核和 CPU Cache 两者一结合，就给我们带来了一个新的挑战。因为 CPU 的每个核各有各的缓存，互相之间的操作又是各自独立的，就会带来**缓存一致性**（Cache Coherence）的问题。

### 缓存一致性问题 ###

一个有两个核心的 CPU，在这两个 CPU 核心里，1 号核心要写一个数据到内存里。这个怎么理解呢？

**这个问题，就是所谓的缓存一致性问题，1 号核心和 2 号核心的缓存，在这个时候是不一致的。**

### 总线嗅探机制和 MESI 协议 ###

要解决缓存一致性问题，首先要解决的是多个 CPU 核心之间的数据传播问题。最常见的一种解决方案呢，叫作总线嗅探（Bus Snooping）。(这个策略，本质上就是把所有的读写请求都通过总线（Bus）广播给所有的 CPU 核心，然后让各个核心去“嗅探”这些请求，再根据本地的情况进行响应。)

总线本身就是一个特别适合广播进行数据传输的机制，所以总线嗅探这个办法也是我们日常使用的 Intel CPU 进行缓存一致性处理的解决方案。

基于总线嗅探机制，其实还可以分成很多种不同的缓存一致性协议。不过其中最常用的，MESI 协议。

MESI 协议写失效（Write Invalidate）的协议。

* 在写失效协议里，只有一个 CPU 核心负责写入数据，其他的核心，只是同步读取到这个写入。在这个 CPU 核心写入 Cache 之后，它会去广播一个“失效”请求告诉所有其他的 CPU 核心。其他的 CPU 核心，只是去判断自己是否也有一个“失效”版本的 Cache Block，然后把这个也标记成失效的就好了。
* 相对于写失效协议，还有一种叫作写广播（Write Broadcast）的协议。在那个协议里，一个写入请求广播到所有的 CPU 核心，同时更新各个核心里的 Cache。
* 写广播在实现上自然很简单，但是写广播需要占用更多的总线带宽。写失效只需要告诉其他的 CPU 核心，哪一个内存地址的缓存失效了，但是写广播还需要把对应的数据传输给其他 CPU 核心。

* M：代表已修改（Modified）：“脏”的 Cache Block，Cache Block 里面的内容我们已经更新过了，但是还没有写回到主内存里面。
* E：代表独占（Exclusive）
* S：代表共享（Shared）
* I：代表已失效（Invalidated）：这个 Cache Block 里面的数据已经失效了，我们不可以相信这个 Cache Block 里面的数据

“独占”和“共享”：缓存里面的数据都是“干净”的。这个“干净”，自然对应的是前面所说的“脏”的，也就是说，这个时候，Cache Block 里面的数据和主内存里面的数据是一致的。
* 在独占状态下：对应的 Cache Line 只加载到了当前 CPU 核所拥有的 Cache 里。其他的 CPU 核，并没有加载对应的数据到自己的 Cache 里。这个时候，如果要向独占的 Cache Block 写入数据，我们可以自由地写入数据，而不需要告知其他 CPU 核。
* 在独占状态下的数据，如果收到了一个来自于总线的读取对应缓存的请求，它就会变成共享状态。这个共享状态是因为，这个时候，另外一个 CPU 核心，也把对应的 Cache Block，从内存里面加载到了自己的 Cache 里来。
* 在共享状态下，因为同样的数据在多个 CPU 核心的 Cache 里都有。所以，当我们想要更新 Cache 里面的数据的时候，不能直接修改，而是要先向所有的其他 CPU 核心广播一个请求，要求先把其他 CPU 核心里面的 Cache，都变成无效的状态，然后再更新当前 Cache 里面的数据。这个广播操作，一般叫作 RFO（Request For Ownership），也就是获取当前对应 Cache Block 数据的所有权。
* 有没有觉得这个操作有点儿像我们在多线程里面用到的读写锁。在共享状态下，大家都可以并行去读对应的数据。但是如果要写，我们就需要通过一个锁，获取当前写入位置的所有权。

整个 MESI 的状态，可以用一个有限状态机来表示它的状态流转。需要注意的是，对于不同状态触发的事件操作，可能来自于当前 CPU 核心，也可能来自总线里其他 CPU 核心广播出来的信号。

![fa98835c78c879ab69fd1f29193e54d1.jpeg](img/fa98835c78c879ab69fd1f29193e54d1.jpeg)

### 总结眼神 ###

CPU Cache的内容：

1. 缓存一致性
2. 是 MESI 协议。

想要实现缓存一致性，关键是要满足两点。

1. 第一个是写传播，也就是在一个 CPU 核心写入的内容，需要传播到其他 CPU 核心里。
2. 更重要的是第二点，保障事务的串行化，才能保障我们的数据是真正一致的，我们的程序在各个不同的核心上运行的结果也是一致的。这个特性不仅在 CPU 的缓存层面很重要，在数据库层面更加重要。

基于总线嗅探机制的 MESI 协议。MESI 协议是一种基于写失效的缓存一致性协议。写失效的协议的好处是，我们不需要在总线上传输数据内容，而只需要传输操作信号和地址信号就好了，不会那么占总线带宽。

MESI 协议，是已修改、独占、共享以及已失效这四个缩写的合称。独占和共享状态，就好像我们在多线程应用开发里面的读写锁机制，确保了我们的缓存一致性。而整个 MESI 的状态变更，则是根据来自自己 CPU 核心的请求，以及来自其他 CPU 核心通过总线传输过来的操作信号和地址信息，进行状态流转的一个有限状态机。

### 推荐阅读 ###

讨论缓存一致性问题，关于计算机底层原理的书，《大话计算机》，里面的 6.9 章节比较详细地讲解了多核 CPU 的访问存储数据的一致性问题。

### 课后思考 ###

## 40 | 理解内存（上）：虚拟内存和内存保护是什么 ##

计算机有五大组成部分，分别是：运算器、控制器、存储器、输入设备和输出设备。如果说计算机最重要的组件，是承担了运算器和控制器作用的 CPU，那内存就是我们第二重要的组件了。内存是五大组成部分里面的存储器，我们的指令和数据，都需要先加载到内存里面，才会被 CPU 拿去执行。

我们的内存需要被分成固定大小的页（Page），然后再通过虚拟内存地址（Virtual Address）到物理内存地址（Physical Address）的地址转换（Address Translation），才能到达实际存放数据的物理内存位置。而我们的程序看到的内存地址，都是虚拟内存地址。

### 简单页表 ###

想要把虚拟内存地址，映射到物理内存地址，最直观的办法，就是来建一张映射表。这个映射表，能够实现虚拟内存里面的页，到物理内存里面的页的一一映射。这个映射表，在计算机里面，就叫作**页表**（Page Table）。

页表这个地址转换的办法，会把一个内存地址分成**页号**（Directory）和**偏移量**（Offset）两个部分。

其实，前面的高位，就是内存地址的页号。后面的低位，就是内存地址里面的偏移量。做地址转换的页表，只需要保留虚拟内存地址的页号和物理内存地址的页号之间的映射关系就可以了。同一个页里面的内存，在物理层面是连续的。以一个页的大小是 4K 字节（4KB）为例，我们需要 20 位的高位，12 位的低位。

![22bb79129f6363ac26be47b35748500f.jpeg](img/22bb79129f6363ac26be47b35748500f.jpeg)

总结一下，对于一个内存地址转换，其实就是这样三个步骤：

1. 把虚拟内存地址，切分成页号和偏移量的组合；
2. 从页表里面，查询出虚拟页号，对应的物理页号；
3. 直接拿物理页号，加上前面的偏移量，就得到了物理内存地址。

![07cd4c3344690055240f215404a286dd.jpeg](img/07cd4c3344690055240f215404a286dd.jpeg)

32 位的内存地址空间，页表一共需要记录 2^20 个到物理页号的映射关系。这个存储关系，就好比一个 2^20 大小的数组。一个页号是完整的 32 位的 4 字节（Byte），这样一个页表就需要 4MB 的空间。

这个空间可不是只占用一份哦。我们每一个进程，都有属于自己独立的虚拟内存地址空间。这也就意味着，每一个进程都需要这样一个页表。不管我们这个进程，是个本身只有几 KB 大小的程序，还是需要几 GB 的内存空间，都需要这样一个页表。如果你用的是 Windows，你可以打开你自己电脑上的任务管理器看看，现在你的计算机里同时在跑多少个进程，用这样的方式，页表需要占用多大的内存。

### 多级页表 ###

仔细想一想，我们其实没有必要存下这 2^20 个物理页表啊。大部分进程所占用的内存是有限的，需要的页也自然是很有限的。我们只需要去存那些用到的页之间的映射关系就好了。如果你对数据结构比较熟悉，你可能要说了，那我们是不是应该用哈希表（Hash Map）这样的数据结构呢？

在实践中，我们其实采用的是一种叫作多级页表（Multi-Level Page Table）的解决方案。

一个进程的内存地址空间是怎么分配的。在整个进程的内存地址空间，通常是“两头实、中间空”。在程序运行的时候，内存地址从顶部往下，不断分配占用的栈的空间。而堆的空间，内存地址则是从底部往上，是不断分配占用的。在一个实际的程序进程里面，虚拟内存占用的地址空间，通常是两段连续的空间。而不是完全散落的随机的内存地址。而多级页表，就特别适合这样的内存地址分布。

以一个 4 级的多级页表为例，同样一个虚拟内存地址，偏移量的部分和上面简单页表一样不变，但是原先的页号部分，我们把它拆成四段，从高到低，分成 4 级到 1 级这样 4 个页表索引。

![614034116a840ef565feda078d73cb76.jpeg](img/614034116a840ef565feda078d73cb76.jpeg)

对应的，一个进程会有一个 4 级页表。我们先通过 4 级页表索引，找到 4 级页表里面对应的条目（Entry）。这个条目里存放的是一张 3 级页表所在的位置。4 级页面里面的每一个条目，都对应着一张 3 级页表，所以我们可能有多张 3 级页表。

找到对应这张 3 级页表之后，我们用 3 级索引去找到对应的 3 级索引的条目。3 级索引的条目再会指向一个 2 级页表。同样的，2 级页表里我们可以用 2 级索引指向一个 1 级页表。

而最后一层的 1 级页表里面的条目，对应的数据内容就是物理页号了。在拿到了物理页号之后，我们同样可以用“页号 + 偏移量”的方式，来获取最终的物理内存地址。

事实上，多级页表就像一个多叉树的数据结构，所以我们常常称它为**页表树**（Page Table Tree）。因为虚拟内存地址分布的连续性，树的第一层节点的指针，很多就是空的，也就不需要有对应的子树了。所谓不需要子树，其实就是不需要对应的 2 级、3 级的页表。找到最终的物理页号，就好像通过一个特定的访问路径，走到树最底层的叶子节点。

![5ba17a3ecf3f9ce4a65546de480fcc4e.jpeg](img/5ba17a3ecf3f9ce4a65546de480fcc4e.jpeg)

以这样的分成 4 级的多级页表来看，每一级如果都用 5 个比特表示。那么每一张某 1 级的页表，只需要 2^5=32 个条目。如果每个条目还是 4 个字节，那么一共需要 128 个字节。而一个 1 级索引表，对应 32 个 4KB 的也就是 128KB 的大小。一个填满的 2 级索引表，对应的就是 32 个 1 级索引表，也就是 4MB 的大小。

一个进程如果占用了 8MB 的内存空间，分成了 2 个 4MB 的连续空间。那么，它一共需要 2 个独立的、填满的 2 级索引表，也就意味着 64 个 1 级索引表，2 个独立的 3 级索引表，1 个 4 级索引表。一共需要 69 个索引表，每个 128 字节，大概就是 9KB 的空间。比起 4MB 来说，只有差不多 1/500。

多级页表虽然节约了我们的存储空间，却带来了时间上的开销，所以它其实是一个“以时间换空间”的策略。原本我们进行一次地址转换，只需要访问一次内存就能找到物理页号，算出物理内存地址。但是，用了 4 级页表，我们就需要访问 4 次内存，才能找到物理页号了。

内存访问其实比 Cache 要慢很多。

### 总结延伸 ###

最简单的进行虚拟页号一一映射的简单页表说起，仔细讲解了现在实际应用的多级页表。多级页表就像是一颗树。因为一个进程的内存地址相对集中和连续，所以采用这种页表树的方式，可以大大节省页表所需要的空间。而因为每个进程都需要一个独立的页表，这个空间的节省是非常可观的。

在优化页表的过程中，数组这样的紧凑的数据结构，以及树这样稀疏的数据结构，在时间复杂度和空间复杂度的差异。另外，纯粹理论软件的数据结构和硬件的设计也是高度相关的。

### 推荐阅读 ###

* 《计算机组成与设计：硬件 / 软件接口》的第 5.7 章节
* 觉得还不过瘾，可以进一步去读一读[《What Every Programmer Should Know About Memory》](https://people.freebsd.org/~lstewart/articles/cpumemory.pdf)的第 4 部分，也就是 Virtual Memory。

### 课后思考 ###

在实际的虚拟内存地址到物理内存地址的地址转换的过程里，我们没有采用哈希表，而是采用了多级页表的解决方案。

## 41 | 理解内存（下）：解析TLB和内存保护 ##

## 42 | 总线：计算机内部的高速公路 ##

## 43 | 输入输出设备：我们并不是只能用灯泡显示“0”和“1” ##

应用篇

## 52 | 设计大型DMP系统（上）：MongoDB并不是什么灵丹妙药 ##

### DMP：数据管理平台 ###

什么是 DMP 系统。DMP 系统的全称叫作数据管理平台（Data Management Platform），目前广泛应用在互联网的广告定向（Ad Targeting）、个性化推荐（Recommendation）这些领域。

DMP 系统会通过处理海量的互联网访问数据以及机器学习算法，给一个用户标注上各种各样的标签。然后，在我们做个性化推荐和广告投放的时候，再利用这些这些标签，去做实际的广告排序、推荐等工作。无论是 Google 的搜索广告、淘宝里千人千面的商品信息，还是抖音里面的信息流推荐，背后都会有一个 DMP 系统。

一个 DMP 系统应该怎么搭建呢？对于外部使用 DMP 的系统或者用户来说，可以简单地把 DMP 看成是一个键 - 值对（Key-Value）数据库。我们的广告系统或者推荐系统，可以通过一个客户端输入用户的唯一标识（ID），然后拿到这个用户的各种信息。

对于这个 KV 数据库，那就是：**低响应时间**（Low Response Time）、**高可用性**（High Availability）、**高并发**（High Concurrency）、**海量数据**（Big Data），同时我们需要**付得起对应的成本**（Affordable Cost）。

1. 低响应时间：一般的广告系统留给整个广告投放决策的时间也就是 10ms 左右，所以对于访问 DMP 获取用户数据，预期的响应时间都在 1ms 之内。
2. 高可用性：DMP 常常用在广告系统里面。DMP 系统出问题，往往就意味着我们整个的广告收入在不可用的时间就没了，所以我们对于可用性的追求可谓是没有上限的。
3. 高并发
4. 数据量：如果我们的产品针对中国市场，那么我们需要有 10 亿个 Key，对应的假设每个用户有 500 个标签，标签有对应的分数。标签和分数都用一个 4 字节（Bytes）的整数来表示，那么一共我们需要 10 亿 x 500 x (4 + 4) Bytes = 4 TB 的数据了。
5. 低成本：

### 总结延伸 ###

因为低延时、高并发、写少读多的 DMP 的 KV 数据库，最适合用 SSD 硬盘，并且采用专门的 KV 数据库是最合适的。

* AeroSpike
* 开源的 Cassandra

对于数据管道，因为主要是顺序读和顺序写，所以我们不一定要选用 SSD 硬盘，而可以用 HDD 硬盘。对于最大化吞吐量的需求，使用 zero-copy 和 DMA 是必不可少的，所以现在的数据管道的标准解决方案就是 Kafka 了。

对于数据仓库，我们通常是一次写入、多次读取。并且，由于存储的数据量很大，我们还要考虑成本问题。

1. 一方面，我们会用 HDD 硬盘而不是 SSD 硬盘；
2. 另一方面，我们往往会预先给数据规定好 Schema，使得单条数据的序列化，不需要像存 JSON 或者 MongoDB 的 BSON 那样，存储冗余的字段名称这样的元数据。

最常用的解决方案是，用 Hadoop 这样的集群，采用 Hive 这样的数据仓库系统，或者采用 Avro/Thrift/ProtoBuffer 这样的二进制序列化方案。

### 推荐阅读 ###

我推荐你去读一读《数据密集型应用系统设计》这本书，深入了解一下，设计数据系统需要关注的各个核心要点。

### 课后思考 ###

我们讲到了数据管道通常所使用的开源系统 Kafka，并且选择了使用机械硬盘。在 Kafka 的使用上，我们有没有必要使用 SSD 硬盘呢？如果用了 SSD 硬盘，又会带来哪些好处和坏处呢？

## 54 | 理解Disruptor（上）：带你体会CPU高速缓存的风驰电掣 ##

其实只要通晓硬件层面的原理，即使是像 Java 这样的高级语言，也能够把 CPU 的性能发挥到极限。

### Padding Cache Line，体验高速缓存的威力 ###

这段代码里，Disruptor 在 RingBufferPad 这个类里面定义了 p1，p2 一直到 p7 这样 7 个 long 类型的变量。

	abstract class RingBufferPad
	{
	    protected long p1, p2, p3, p4, p5, p6, p7;
	}

### 使用 RingBuffer，利用缓存和分支预测 ###

### 总结延伸 ###

Disruptor 这个框架的神奇之处：

1. CPU 从内存加载数据到 CPU Cache 里面的时候，不是一个变量一个变量加载的，而是加载固定长度的 Cache Line。如果是加载数组里面的数据，那么 CPU 就会加载到数组里面连续的多个数据。所以，数组的遍历很容易享受到 CPU Cache 那风驰电掣的速度带来的红利。
2. 对于类里面定义的单独的变量，就不容易享受到 CPU Cache 红利了。因为这些字段虽然在内存层面会分配到一起，但是实际应用的时候往往没有什么关联。于是，就会出现多个 CPU Core 访问的情况下，数据频繁在 CPU Cache 和内存里面来来回回的情况。而 Disruptor 很取巧地在需要频繁高速访问的变量，也就是 RingBufferFields 里面的 indexMask 这些字段前后，各定义了 7 个没有任何作用和读写请求的 long 类型的变量。
3. 无论在内存的什么位置上，这些变量所在的 Cache Line 都不会有任何写更新的请求。我们就可以始终在 Cache Line 里面读到它的值，而不需要从内存里面去读取数据，也就大大加速了 Disruptor 的性能。

55 | 理解Disruptor（下）：带你体会CPU高速缓存的风驰电掣

答疑和加餐

## 特别加餐 | 我在2019年F8大会的两日见闻录 ##

### Day 1：“The Future is Private” ###

### Day 2：科技改变世界 ###

### 推荐阅读 ###

## FAQ第一期 | 学与不学，知识就在那里，不如就先学好了 ##

### Q1：“要不要学”和“学不会怎么办”系列 ###

常常想，当时要是做了就好了。当时，不是就是现在么？学或者不学，知识就在那里，不如就先学好了啊。

### Q2：“计算机组成原理”和“操作系统”到底有啥不一样？ ###

操作系统也是一个“软件”

体系结构、操作系统、编译原理以及计算机网络，都可以认为是组成原理的后继课程。体系结构不是一个系统软件，它更多地是讲，如何量化地设计和研究体系结构和指令集。操作系统、编译原理和计算机网络都是基于体系结构之上的系统软件。

其实这几门基础学科，都是环环相扣，相互渗透的，每一门课都不可能独立存在。

### Q3：“图灵机”和“冯·诺依曼机”的区别 ###

* 图灵机：是Class
* 冯·诺依曼：是Object

### Q4：工作多年，如何保持对知识清晰、准确的认识？ ###

1. 很多工程师只是满足于工作的需求被满足了，没有真的深入去搞清楚一个问题的原理。从网络上搜索一段代码。比如说，我们现在要提升 RPC 和序列化的性能，找一个教程用一下 Thrift 这样的开源框架，解决眼下的问题就完事儿。
	1. 搞清楚里面的实现细节和原理
	2. 你对二进制存储、程序性能、网络性能，就会有一个更深刻的认识
2. 读书的时候我们认为一个东西掌握扎实了，有时候其实未必。书上所学和真正实践中所用完全是两码事。背出计算机的五大组成部分，似乎和我们的实际应用没有联系，但是在实际的系统开发过程中，无论是内存地址转换使用的页表树这样的数据结构，还是各个系统组件间通过总线进行通信的模式，其实都可以和我们自己的应用系统开发里的模式和思路联系起来。行为：
	1. 一方面，遇到疑难问题、复杂的系统时，必须要用更底层更本质的理解计算机运作的方式，去处理问题，自然会去回头把这些基础知识捡起来；
	2. 另一方面，时不时抽点时间回头看看一些“大部头”的教科书，对我自己而言，本身就很有自我满足感，而这种自我满足感也会促使我不断去读它们，从而形成一个良性循环。 

### Q5：六个最实用的、督促自己学习的办法 ###

1. 好奇心是一个优秀程序员必然要有的特质。多去想想“为什么是这样的”，有助于你更深入地掌握这些知识点。
2. 先了解知识面，再寻找自己有兴趣的点深入，学习也是个反复迭代的过程。
3. 带着问题去学习是最快的成长方式之一。彻底搞清楚实际在开发过程中遇到的困难的问题，而不是只满足于功能问题被实现和解决，是提升自己的必经之路。
4. “教别人”是一种非常高效的学习方式，自己有没有弄清楚，在教别人的过程中，会体会得明明白白。
5. 每个月给自己投资 100-200 块在专业学习上面，这样花了钱，通过外部约束，也是一个让自己坚持下去的好办法。
6. 坚持到底就是胜利✌️。把学习和成长变成一种习惯，这个习惯带来的惯性会让你更快地成长。

## 用户故事 | 赵文海：怕什么真理无穷，进一寸有一寸的欢喜 ##

### 为什么要学计算机组成原理？ ###



### 我是怎么学习专栏的 ###

徐老师每篇文章下的推荐阅读，是我个人最喜欢这个专栏的地方。徐老师每次都会在文章结尾列出相关书籍的对应章节、相关的博客或论文，这为我后面深入学习相关知识提供了很大的便利。

关于这一块内容我是这么打算的。我准备学完第一遍之后，仔细去读一读老师推荐的书籍。这样有了第一遍的铺垫，读起老师推荐的书和论文，不至于那么困难和恐惧。读书的时候还可以结合书中内容再复习一遍专栏，到时候肯定会有新的收获。毕竟，基础知识的学习是一个**长期积累、慢慢参悟、螺旋上升**的过程，我已经做好了打持久战的准备。。

### 学习专栏有什么收获？ ###

我工作之后一直在持续学习，在这个过程中，我发现最有效的方法，不是短时间冲刺，而是有节奏的坚持。

### 总结 ###

缓解焦虑的方式很简单，就是行动。担心自己长胖，那就去锻炼；担心自己被淘汰，那就去学习。