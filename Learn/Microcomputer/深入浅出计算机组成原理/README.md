# 深入浅出计算机组成原理 #

入门篇

## 开篇词 | 为什么你需要学习计算机组成原理 ##

如果越早去弄清楚计算机的底层原理，在你的知识体系中“储蓄”起这些知识，也就意味着你有越长的时间来收获学习知识的“利息”。虽然一开始可能不起眼，但是随着时间带来的复利效应，你的长线投资项目，就能让你在成长的过程中越走越快。

### 计算机底层知识的“第一课” ###

学习这门“第一课”的过程，会为你在整个软件开发领域中打开一扇扇窗和门，让你看到更加广阔的天地。比如说，明白了高级语言是如何对应着 CPU 能够处理的一条条指令，能为你打开编译原理这扇门；搞清楚程序是如何加载运行的，能够让你对操作系统有更深入的理解。

### 理论和实践相结合 ###

* 北京大学的《计算机组成》开放课程
* 计算机组成与设计：硬件 / 软件接口

组原难学的原因：

1. 广：组成原理中的概念非常多，每个概念的信息量也非常大。比如想要理解 CPU 中的算术逻辑单元（也就是 ALU）是怎么实现加法的，需要牵涉到如何把整数表示成二进制，还需要了解这些表示背后的电路、逻辑门、CPU 时钟、触发器等知识。
2. 深：组成原理中的很多概念，阐述开来就是计算机学科的另外一门核心课程。
3. 学不能致用：常常沉溺于概念和理论中，无法和自己日常的开发工作联系起来

我要把这些知识点和日常工作、生活以及整个计算机行业的发展史联系起来，教你真正看懂、学会、记住组成原理的核心内容，教你更多地从“为什么”这个角度，去理解这些知识点，而不是只是去记忆“是什么”。

专栏具体的设计：

1. 把组成原理里面的知识点，和我在应用开发和架构设计中遇到的实际案例，放到一起进行印证，通过代码和案例，让你消化理解。**比如，为什么 Disruptor 这个高性能队列框架里，要定义很多没有用的占位变量呢？其实这是为了确保我们唯一关心的参数，能够始终保留在 CPU 的高速缓存里面，而高速缓存比我们的内存要快百倍以上。**
2. **尽可能地多举一些我们日常生活里面的例子，让你理解计算机的各个组件是怎么运作的。**在真实的开发中，我们会遇到什么问题，这些问题产生的根源是什么。让你从知识到应用，最终又回到知识，让学习和实践之间形成一道闭环。计算机组成中很多组件的设计，都不是凭空发明出来，它们中的很多都来自现实生活中的想法和比喻。而底层很多硬件设计和开发的思路，其实也和你进行软件架构的开发设计和思路是一样的。比如说，在硬件上，我们是通过最基本的与、或、非、异或门这些最基础的门电路组合形成了强大的 CPU。而在面向对象和设计模式里，我们也常常是通过定义基本的 Command，然后组合来完成更复杂的功能；再比如说，CPU 里面的冒险和分支预测的策略，就好像在接力赛跑里面后面几棒的选手早点起跑，如果交接棒没有问题，自然占了便宜，但是如果没能交接上，就会吃个大亏。
3. **知识点和应用之外，我会多讲一些计算机硬件发展史上的成功和失败，让你明白很多设计的历史渊源，让你更容易记住“为什么”，更容易记住这些知识点。**

“人生如逆旅，我亦是行人”。学习总不会是一件太轻松的事情，希望在这个专栏里，你能和我多交流，坚持练完这一手内功。

## 01 | 冯·诺依曼体系结构：计算机组成的金字塔 ##

### 计算机的基本硬件组成 ###

1. CPU，叫中央处理器（Central Processing Unit）
2. 内存（Memory）
3. 主板（Motherboard）。主板的芯片组（Chipset）和总线（Bus）解决了 CPU 和内存之间如何通信的问题。芯片组控制了数据传输的流转，也就是数据从哪里到哪里的问题。总线则是实际数据传输的高速公路。因此，总线速度（Bus Speed）决定了数据能传输得多快。

鼠标、键盘以及硬盘，这些都是插在主板上的。作为外部 I/O 设备，它们是通过主板上的南桥（SouthBridge）芯片组，来控制和 CPU 之间的通信的。“南桥”芯片的名字很直观，一方面，它在主板上的位置，通常在主板的“南面”。另一方面，它的作用就是作为“桥”，来连接鼠标、键盘以及硬盘这些外部设备和 CPU 之间的通信。

### 冯·诺依曼体系结构 ###

SoC，System on a Chip（系统芯片）

### 总结延伸 ###

学习组成原理，其实就是学习控制器、运算器的工作原理，也就是 CPU 是怎么工作的，以及为何这样设计；学习内存的工作原理，从最基本的电路，到上层抽象给到 CPU 乃至应用程序的接口是怎样的；学习 CPU 是怎么和输入设备、输出设备打交道的。

学习组成原理，就是在理解从控制器、运算器、存储器、输入设备以及输出设备，从电路这样的硬件，到最终开放给软件的接口，是怎么运作的，为什么要设计成这样，以及在软件开发层面怎么尽可能用好它。

### 推荐阅读 ###

[First Draft of a Report on the EDVAC](https://en.wikipedia.org/wiki/First_Draft_of_a_Report_on_the_EDVAC)

### 课后思考 ###

## 02 | 给你一张知识地图，计算机组成原理应该怎么学 ##

*整个计算机组成原理，就是围绕着计算机是如何组织运作展开的*

### 计算机组成原理知识地图 ###

![12bc980053ea355a201e2b529048e2ff.jpg](img/12bc980053ea355a201e2b529048e2ff.jpg)

计算机的基本组成、计算机的指令和计算、处理器设计、以及存储器和I/O设备

1. 计算机的基本组成
	* 运算器、控制器、存储器、输入设备和输出设备这五大基本组件
	* 计算机的两个核心指标，性能和功耗
2. 计算机的指令和计算
	* 一条条指令执行的控制过程，就是由计算机五大组件之一的控制器来控制的
	* 

## 03 | 通过你的CPU主频，我们来谈谈“性能”究竟是什么？ ##



## 04 | 穿越功耗墙，我们该从哪些方面提升“性能”？ ##

# 原理篇：指令和运算 #

## 05 | 计算机指令：让我们试试用纸带编程 ##

### 在软硬件接口中，CPU 帮我们做了什么事？ ###

我们的个人电脑用的是 Intel 的 CPU，苹果手机用的是 ARM 的 CPU。这两者能听懂的语言就不太一样。类似这样两种 CPU 各自支持的语言，就是**两组不同的计算机指令集**，英文叫 Instruction Set。这里面的“Set”，其实就是数学上的集合，代表不同的单词、语法。

一个计算机程序，不可能只有一条指令，而是由成千上万条指令组成的。但是 CPU 里不能一直放着所有指令，所以计算机程序平时是存储在存储器中的。这种程序指令存储在存储器里面的计算机，我们就叫作**存储程序型计算机**（Stored-program Computer）。

### 从编译到汇编，代码怎么变成机器码？ ###

	// test.c
	int main()
	{
	  int a = 1; 
	  int b = 2;
	  a = a + b;
	}

C语言 => （Compile）汇编语言 => 汇编器（Assembler）机器码

	$ gcc -g -c test.c
	$ objdump -d -M intel -S test.o

汇编代码是“给程序员看的机器码”。机器码和汇编代码是一一对应的。

### 解析指令和机器码 ###

常见的指令可以分成五大类：

1. 算术类指令：加减乘除，变成一条条算术类指令。
2. 数据传输类指令：变量赋值、在内存里读写数据，用的都是数据传输类指令。
3. 逻辑类指令：逻辑上的与或非。
4. 条件分支类指令：if/else，其实都是条件分支类指令。
5. 无条件跳转指令：函数或者方法，在调用函数的时候，其实就是发起了一个无条件跳转指令。

![ebfd3bfe5dba764cdcf871e23b29f197.jpeg](img/ebfd3bfe5dba764cdcf871e23b29f197.jpeg)

MIPS 的指令是一个 32 位的整数，高 6 位叫操作码（Opcode），剩下的 26 位有三种格式，分别是 R、I 和 J。

* *R指令*是一般用来做算术和逻辑操作，里面有读取和写入数据的寄存器的地址。
* *I指令*通常是用在数据传输、条件分支，以及在运算的时候使用的并非变量还是常数的时候。
* *J指令*一个跳转指令，高 6 位之外的 26 位都是一个跳转后的地址。

### 总结延伸 ###

**一个 C 语言程序，是怎么被编译成为汇编语言，乃至通过汇编器再翻译成机器码的。**

其实最终都是由不同形式的程序，把我们写好的代码，转换成 CPU 能够理解的机器码来执行的。

只是解释型语言，是通过解释器在程序运行的时候逐句翻译，而 Java 这样使用虚拟机的语言，则是由虚拟机对编译出来的中间代码进行解释，或者即时编译成为机器码来最终执行。

### 推荐阅读 ###

《计算机组成与设计：软 / 硬件接口》第 5 版的 2.17 小节

### 课后思考 ###

## 06 | 指令跳转：原来if...else就是goto ##

### CPU 是如何执行指令的？ ###

写好的代码变成了指令之后，是一条一条*顺序*执行的就可以了。

CPU 其实就是由一堆寄存器组成的。而寄存器就是 CPU 内部，由多个触发器（Flip-Flop）或者锁存器（Latches）组成的简单电路。

一个CPU里面会有很多种不同功能的寄存器：

1. PC寄存器（Program Counter Register），指令地址寄存器（Instruction Address Register）。用来存放下一条需要执行的计算机指令的内存地址。
2. 指令寄存器（Instruction Register），用来存放当前正在执行的指令。
3. 条件码寄存器（Status Register），用里面的一个一个标记位（Flag），存放 CPU 进行算术或者逻辑计算的结果。

通常根据存放的数据内容来给它们取名字，比如整数寄存器、浮点数寄存器、向量寄存器和地址寄存器等等。有些寄存器既可以存放数据，又能存放地址，我们就叫它通用寄存器。

![ad91b005e97959d571bbd2a0fa30b48a.jpeg](img/ad91b005e97959d571bbd2a0fa30b48a.jpeg)

特殊指令：

* 跳转指令

### 从 if…else 来看程序的执行和跳转 ###


	// test.c
	#include <time.h>
	#include <stdlib.h>
	
	int main()
	{
	  srand(time(NULL));
	  int r = rand() % 2;
	  int a = 10;
	  if (r == 0)
	  {
	    a = 1;
	  } else {
	    a = 2;
	  } 
	}

	$ gcc -g -c test.c
	$ objdump -d -M intel -S test.o 


	    if (r == 0)
	  3b:   83 7d fc 00             cmp    DWORD PTR [rbp-0x4],0x0
	  3f:   75 09                   jne    4a <main+0x4a>
	    {
	        a = 1;
	  41:   c7 45 f8 01 00 00 00    mov    DWORD PTR [rbp-0x8],0x1
	  48:   eb 07                   jmp    51 <main+0x51>
	    }
	    else
	    {
	        a = 2;
	  4a:   c7 45 f8 02 00 00 00    mov    DWORD PTR [rbp-0x8],0x2
	  51:   b8 00 00 00 00          mov    eax,0x0
	    } 

对于 `r ==  0` 的条件判断，被编译成了 cmp 和 jne 这两条指令。cmp 指令比较了前后两个操作数的值，这里的 DWORD PTR 代表操作的数据类型是 32 位的整数，而[rbp-0x4]则是一个寄存器的地址。

1. 第一个操作数就是从寄存器里拿到的变量 r 的值。
2. 第二个操作数 0x0 就是我们设定的常量 0 的 16 进制表示。
3. cmp 指令的比较结果，会存入到*条件码寄存器*当中去。

跟着的 jne 指令，是 jump if not equal 的意思，它会查看对应的零标志位。

1. 如果为 0，会跳转到后面跟着的操作数 4a 的位置。这个 4a，对应这里汇编代码的行号，也就是上面设置的 else 条件里的第一条指令。
2. 当跳转发生的时候，PC 寄存器就不再是自增变成下一条指令的地址，而是被直接设置成这里的 4a 这个地址。
3. 这个时候，CPU 再把 4a 地址里的指令加载到指令寄存器中来执行。
4. 跳转到执行地址为 4a 的指令，实际是一条 mov 指令，第一个操作数和前面的 cmp 指令一样，是另一个 32 位整型的寄存器地址，以及对应的 2 的 16 进制值 0x2。mov 指令把 2 设置到对应的寄存器里去，相当于一个赋值操作。然后，PC 寄存器里的值继续自增，执行下一条 mov 指令。
5. 这条 mov 指令的第一个操作数 eax，代表累加寄存器，第二个操作数 0x0 则是 16 进制的 0 的表示。这条指令其实没有实际的作用，它的作用是一个占位符。
6. 我们回过头去看前面的 if 条件，如果满足的话，在赋值的 mov 指令执行完成之后，有一个 jmp 的无条件跳转指令。跳转的地址就是这一行的地址 51。我们的 main 函数没有设定返回值，而 mov eax, 0x0 其实就是给 main 函数生成了一个默认的为 0 的返回值到累加器里面。if 条件里面的内容执行完成之后也会跳转到这里，和 else 里的内容结束之后的位置是一样的。

### 如何通过 if…else 和 goto 来实现循环？ ###

	int a = 0;
    for (int i = 0; i < 3; i++){
        a += i;
    }


	int a = 0;
	   4:	c7 45 fc 00 00 00 00 	mov    DWORD PTR [rbp-0x4],0x0
	    for (int i = 0; i < 3; i++){
	   b:	c7 45 f8 00 00 00 00 	mov    DWORD PTR [rbp-0x8],0x0
	  12:	eb 0a                	jmp    1e <main+0x1e>
	        a += i;
	  14:	8b 45 f8             	mov    eax,DWORD PTR [rbp-0x8]
	  17:	01 45 fc             	add    DWORD PTR [rbp-0x4],eax
	    for (int i = 0; i < 3; i++){
	  1a:	83 45 f8 01          	add    DWORD PTR [rbp-0x8],0x1
	  1e:	83 7d f8 02          	cmp    DWORD PTR [rbp-0x8],0x2
	  22:	7e f0                	jle    14 <main+0x14>
	  24:	b8 00 00 00 00       	mov    eax,0x0
	    }

1. 对应的循环也是用 1e 这个地址上的 cmp 比较指令，和紧接着的 jle 条件跳转指令来实现的。
2. 主要的差别在于，这里的 jle 跳转的地址，在这条指令之前的地址 14，而非 if…else 编译出来的跳转指令之后。
3. 往前跳转使得条件满足的时候，PC 寄存器会把指令地址设置到之前执行过的指令位置，重新执行之前执行过的指令，直到条件不满足，顺序往下执行 jle 之后的指令，整个循环才结束。

### 总结延伸 ###

单条指令的基础上，学习了程序里的多条指令，除了简单地通过 PC 寄存器自增的方式顺序执行外，条件码寄存器会记录下当前执行指令的条件判断状态，然后通过跳转指令读取对应的条件码，修改 PC 寄存器内的下一条指令的地址，最终实现 if…else 以及 for/while 这样的程序控制流程。

回归到计算机可以识别的机器指令级别，其实都只是一个简单的地址跳转而已，也就是一个类似于 goto 的语句。

要在硬件层面实现这个 goto 语句，除了本身需要用来保存下一条指令地址，以及当前正要执行指令的 PC 寄存器、指令寄存器外，我们只需要再增加一个条件码寄存器，来保留条件判断的状态。这样简简单单的三个寄存器，就可以实现条件判断和循环重复执行代码的功能。

### 推荐阅读 ###

《深入理解计算机系统》的第 3 章，详细讲解了 C 语言和 Intel CPU 的汇编语言以及指令的对应关系，以及 Intel CPU 的各种寄存器和指令集。

Intel 指令集相对于之前的 MIPS 指令集要复杂一些：

1. 所有的指令是变长的，从 1 个字节到 15 个字节不等
2. 针对操作数据的长度不同有不同的后缀

### 课后思考 ###

除了 if…else 的条件语句和 for/while 的循环之外，大部分编程语言还有 switch…case 这样的条件跳转语句。switch…case 编译出来的汇编代码也是这样使用 jne 指令进行跳转吗？对应的汇编代码的性能和写很多 if…else 有什么区别呢？你可以试着写一个简单的 C 语言程序，编译成汇编代码看一看。

## 07 | 函数调用：为什么会发生stack overflow？ ##

栈溢出（stack overflow）

### 为什么我们需要程序栈？ ###

	// function_example.c
	#include <stdio.h>
	int static add(int a, int b)
	{
	    return a+b;
	}
	
	
	int main()
	{
	    int x = 5;
	    int y = 10;
	    int u = add(x, y);
	}

这个程序定义了一个简单的函数 add，接受两个参数 a 和 b，返回值就是 a+b。而 main 函数里则定义了两个变量 x 和 y，然后通过调用这个 add 函数，来计算 u=x+y，最后把 u 的数值打印出来。

	$ gcc -g -c function_example.c
	$ objdump -d -M intel -S function_example.o

### 如何构造一个 stack overflow？ ###

无论有多少层的函数调用，或者在函数 A 里调用函数 B，再在函数 B 里调用 A，这样的递归调用，我们都只需要通过维持 rbp 和 rsp，这两个维护栈顶所在地址的寄存器，就能管理好不同函数之间的跳转。不过，栈的大小也是有限的。如果函数调用层数太多，我们往栈里压入它存不下的内容，程序在执行的过程中就会遇到栈溢出的错误，这就是大名鼎鼎的“stack  overflow”。

除了无限递归，递归层数过深，在栈空间里面创建非常占内存的变量（比如一个巨大的数组），这些情况都很可能给你带来 stack  overflow。

### 如何利用函数内联进行性能优化？ ###

**函数内联**（Inline）。我们只要在 GCC 编译的时候，加上对应的一个让编译器自动优化的参数 -O，编译器就会在可行的情况下，进行这样的指令替换。

	#include <stdio.h>
	#include <time.h>
	#include <stdlib.h>
	
	int static add(int a, int b)
	{
	    return a+b;
	}
	
	int main()
	{
	    srand(time(NULL));
	    int x = rand() % 5
	    int y = rand() % 10;
	    int u = add(x, y)
	    printf("u = %d\n", u)
	}


$ gcc -g -c -O function_example_inline.c
$ objdump -d -M intel -S function_example_inline.o

内联并不是没有代价，内联意味着，我们把可以复用的程序指令在调用它的地方完全展开了。如果一个函数在很多地方都被调用了，那么就会展开很多次，整个程序占用的空间就会变大了。

没有调用其他函数，只会被调用的函数，我们一般称之为**叶子函数（或叶子过程）**。

### 总结延伸 ###

一个程序的函数间调用，在 CPU 指令层面是怎么执行的。**程序栈**的概念。通过压栈和出栈操作，使得程序在不同的函数调用过程中进行转移。而函数内联和栈溢出，一个是我们常常可以选择的优化方案，另一个则是我们会常遇到的程序 Bug。

通过加入了程序栈，在指令跳转中加入了记忆的功能（跳转去新的指令之后，再回到跳出去的位置）。为我们在程序开发的过程中，提供了“函数”这样一个抽象。

### 推荐阅读 ###

《深入理解计算机系统（第三版）》的 3.7 小节《过程》

通过搜索引擎搞清楚 function_example.c 每一行汇编代码的含义，这个能够帮你进一步深入了解程序栈、栈帧、寄存器以及 Intel CPU 的指令集。

### 课后思考 ###

## 08 | ELF和静态链接：为什么程序无法同时在Linux和Windows下运行？ ##

### 编译、链接和装载：拆解程序执行 ###

写好的 C 语言代码，可以通过编译器编译成汇编代码，然后汇编代码再通过汇编器变成 CPU 可以理解的机器码，于是 CPU 就可以执行这些机器码了

“C 语言代码 - 汇编代码 - 机器码"

第一部分由编译（Compile）、汇编（Assemble）以及链接（Link）三个阶段组成。在这三个阶段完成之后，生成了一个可执行文件。
第二部分，通过装载器（Loader）把可执行文件装载（Load）到内存中。CPU从内存中读取指令和数据，来开始真正执行程序。

![997341ed0fa9018561c7120c19cfa2a7.jpg](img/997341ed0fa9018561c7120c19cfa2a7.jpg)

### ELF 格式和链接：理解链接过程 ###

程序最终是通过装载器变成指令和数据的，所以其实我们生成的可执行代码也并不仅仅是一条条的指令。

因为在 Linux 下，可执行文件和目标文件所使用的都是一种叫 **ELF**（Execuatable and Linkable File Format）的文件格式，中文名字叫**可执行与可链接文件格式**。 ELF 文件里面，存储在一个叫作**符号表**（Symbols Table）的位置里。符号表相当于一个地址簿，把名字和地址关联了起来。

ELF 文件格式把各种信息，分成一个一个的 Section 保存起来。ELF 有一个基本的文件头（File Header），用来表示这个文件的基本属性，比如是否是可执行文件，对应的 CPU、操作系统等等。除了这些基本属性之外，大部分程序还有这么一些 Section：

1. 首先是.text Section，也叫作代码段或者指令段（Code Section），用来保存程序的代码和指令；
2. 接着是.data Section，也叫作数据段（Data Section），用来保存程序里面设置好的初始化数据信息；
3. 然后就是.rel.text Secion，叫作重定位表（Relocation Table）。重定位表里，保留的是当前的文件里面，哪些跳转地址其实是我们不知道的。比如上面的 link_example.o 里面，我们在 main 函数里面调用了 add 和 printf 这两个函数，但是在链接发生之前，我们并不知道该跳转到哪里，这些信息就会存储在重定位表里；
4. 最后是.symtab Section，叫作符号表（Symbol Table）。符号表保留了我们所说的当前文件里面定义的函数名称和对应地址的地址簿。

链接器会扫描所有输入的目标文件，然后把所有符号表里的信息收集起来，构成一个全局的符号表。然后再根据重定位表，把所有不确定要跳转地址的代码，根据符号表里面存储的地址，进行一次修正。最后，把所有的目标文件的对应段进行一次合并，变成了最终的可执行代码。这也是为什么，可执行文件里面的函数调用的地址都是正确的。

![f62da9b29aa53218f8907851df27f912.jpeg](img/f62da9b29aa53218f8907851df27f912.jpeg)

在链接器把程序变成可执行文件之后，要装载器去执行程序就容易多了。装载器不再需要考虑地址跳转的问题，只需要解析 ELF 文件，把对应的指令和数据，加载到内存里面供 CPU 执行就可以了。

### 总结延伸 ###

Linux 下的 ELF 文件格式，而 Windows 的可执行文件格式是一种叫作 PE（Portable Executable Format）的文件格式。Linux 下的装载器只能解析 ELF 格式而不能解析 PE 格式。

### 推荐阅读 ###

程序员的自我修养——链接、装载和库，ch1-ch4

### 课后思考 ###

可以通过 readelf 读取出今天演示程序的符号表，看看符号表里都有哪些信息；然后通过 objdump 读取出今天演示程序的重定位表，看看里面又有哪些信息。

## 09 | 程序装载：“640K内存”真的不够用么？ ##

### 程序装载面临的挑战 ###

如何通过链接器，把多个文件合并成一个最终可执行文件。在运行这些可执行文件的时候，我们其实是通过一个装载器，解析ELF或者PE格式的可执行文件。装载器会把对应的指令和数据加载到内存里面来，让CPU置执行。

1. **可执行程序加载后占用的内存空间应该是连续的**。
2. **我们需要同时加载很多哥程序，并且不能让程序自己规定在内存中加载的位置**。

要满足两个基本的要求，可以在内存里面，找到一段连续的内存空间，然后分配给装载的程序，然后把这段连续的内存空间地址，和整个程序指令里规定的内存地址做一个映射。

指令里用到的内存地址叫做虚拟内存地址（Virtual Memory Address），实际内存硬件里面的空间地址，叫物理内存地址（Physical Memory Address）

### 内存分段 ###

找出一段连续的物理内存和虚拟内存地址进行映射的方法，**分段**（Segmentation）。这里的段，就是指系统分配出来的那个连续的内存空间。

分段办法的不足：

1. 内存碎片


### 内存分页 ###

内存碎片

### 总结延伸 ###

程序员的自我修养——链接、装载和库，ch1和ch6 代码装载

## 10 | 动态链接：程序内部的“共享单车” ##

### 链接可以分动、静，共享运行省内存 ###

解决程序装载到内存的时候，最根本的就是**内存空间不够用**。

在动态链接的过程中，我们想要“链接”的，不是存储在硬盘上的目标文件代码，而是加载到内存中的**共享库**（Shared Libraries）。这里的共享库重在“共享“这两个字。

这个加载到内存中的共享库会被很多个程序的指令调用到。在 Windows 下，这些共享库文件就是.dll 文件，也就是 Dynamic-Link Libary（DLL，动态链接库）。在 Linux 下，这些共享库文件就是.so 文件，也就是 Shared Object（一般我们也称之为动态链接库）。这两大操作系统下的文件名后缀，一个用了“动态链接”的意思，另一个用了“共享”的意思，正好覆盖了两方面的含义。

### 地址无关很重要，相对地址解烦恼 ###

要想要在程序运行的时候共享代码，也有一定的要求，就是这些机器码必须是“**地址无关**”的。也就是说，我们编译出来的共享库文件的指令代码，是地址无关码（Position-Independent Code）。无论加载在哪个内存地址，都能够正常执行。如果不是这样的代码，就是地址相关的代码。

大部分函数库其实都可以做到地址无关，因为它们都接受特定的输入，进行确定的操作，然后给出返回结果就好了。无论是实现一个向量加法，还是实现一个打印的函数，这些代码逻辑和输入的数据在内存里面的位置并不重要。

而常见的地址相关的代码，比如绝对地址代码（Absolute Code）、利用重定位表的代码等等，都是地址相关的代码。你回想一下我们之前讲过的重定位表。在程序链接的时候，我们就把函数调用后要跳转访问的地址确定下来了，这意味着，如果这个函数加载到一个不同的内存地址，跳转就会失败。

对于所有动态链接共享库的程序来讲，虽然我们的共享库用的都是同一段物理内存地址，但是在不同的应用程序里，它所在的虚拟内存地址是不同的。我们没办法、也不应该要求动态链接同一个共享库的不同程序，必须把这个共享库所使用的虚拟内存地址变成一致。如果这样的话，我们写的程序就必须明确地知道内部的内存地址分配。

动态代码库内部的变量和函数调用都很容易解决，我们只需要使用相对地址（Relative Address）就好了。各种指令中使用到的内存地址，给出的不是一个绝对的地址空间，而是一个相对于当前指令偏移量的内存地址。因为整个共享库是放在一段连续的虚拟内存地址中的，无论装载到哪一段地址，不同指令之间的相对地址都是不变的。

### PLT 和 GOT，动态链接的解决方案 ###

要实现动态链接共享库

	call   400550 <show_me_the_money@plt>

@plt 的关键字，代表了我们需要从 PLT，也就是程序链接表（Procedure Link Table）里面找要调用的函数。对应的地址呢，则是 400550 这个地址。

	400550:       ff 25 12 05 20 00       jmp    QWORD PTR [rip+0x200512]        # 600a68 <_GLOBAL_OFFSET_TABLE_+0x18>

GLOBAL_OFFSET_TABLE+0x18。这里的 GLOBAL_OFFSET_TABLE，就是全局偏移表。

在动态链接对应的共享库，我们在共享库的 data section 里面，保存了一张全局偏移表（GOT，Global Offset Table）。虽然共享库的代码部分的物理内存是共享的，但是数据部分是各个动态链接它的应用程序里面各加载一份的。所有需要引用当前共享库外部的地址的指令，都会查询 GOT，来找到当前运行程序的虚拟内存里的对应位置。而 GOT 表里的数据，则是在我们加载一个个共享库的时候写进去的。

不同的进程，调用同样的 lib.so，各自 GOT 里面指向最终加载的动态链接库里面的虚拟内存地址是不同的。

这样，虽然不同的程序调用的同样的动态库，各自的内存地址是独立的，调用的又都是同一个动态库，但是不需要去修改动态库里面的代码所使用的地址，而是各个程序各自维护好自己的 GOT，能够找到对应的动态库就好了。

![1144d3a2d4f3f4f87c349a93429805c8.jpg](img/1144d3a2d4f3f4f87c349a93429805c8.jpg)

我们的 GOT 表位于共享库自己的数据段里。GOT 表在内存里和对应的代码段位置之间的偏移量，始终是确定的。这样，我们的共享库就是地址无关的代码，对应的各个程序只需要在物理内存里面加载同一份代码。而我们又要通过各个可执行程序在加载时，生成的各不相同的 GOT 表，来找到它需要调用到的外部变量和函数的地址。

这是一个典型的、不修改代码，而是通过修改“**地址数据**”来进行关联的办法。它有点像我们在 C 语言里面用函数指针来调用对应的函数，并不是通过预先已经确定好的函数名称来调用，而是利用当时它在内存里面的动态地址来调用。

### 总结延伸 ###

在静态链接和程序装载之后，利用动态链接把我们的内存利用到了极致。同样功能的代码生成的共享库，我们只要在内存里面保留一份就好了。这样，我们不仅能够做到代码在开发阶段的复用，也能做到代码在运行阶段的复用。

已经把程序怎么从源代码变成指令、数据，并装载到内存里面，由 CPU 一条条执行下去的过程讲完了。

### 推荐阅读 ###

想要更加深入地了解动态链接，我推荐你可以读一读《程序员的自我修养：链接、装载和库》的第 7 章，里面深入地讲解了，动态链接里程序内的数据布局和对应数据的加载关系。

### 课后思考 ###

像动态链接这样通过修改“地址数据”来进行间接跳转，去调用一开始不能确定位置代码的思路，你在应用开发中使用过吗？

### 精选留言 ###



## 11 | 二进制编码：“手持两把锟斤拷，口中疾呼烫烫烫”？ ##

### 理解二进制的“逢二进一” ###

**短除法**

**原码表示法**

* 缺点：0 可以用两个不同的编码来表示，1000 代表 0， 0000 也代表 0。

仍然通过最左侧第一位的 0 和 1，来判断这个数的正负。但是，我们不再把这一位当成单独的符号位，在剩下几位计算出的十进制前加上正负号，而是在计算整个二进制值的时候，在左侧最高位前面加个负号。

用补码来表示负数，使得我们的整数相加变得很容易，不需要做任何特殊处理。

### 字符串的表示，从编码到数字 ###

最典型的例子就是**字符串**（Character String）。

**不管是整数也好，浮点数也好，采用二进制序列化会比存储文本省下不少空间。**

**字符集**（Charset）和**字符编码**（Character Encoding）：

* 字符集：表示的可以是字符的一个集合。
* 字符编码：对于字符集里的这些字符，怎么一一用二进制表示出来的一个字典。

### 总结延伸 ###

二进制编码的方式，表示任意的信息。只要建立起字符集和字符编码，并且得到大家的认同，我们就可以在计算机里面表示这样的信息了。

我们在计算机组成里面，关心的不只是数值和字符的逻辑表示，更要弄明白，在硬件层面，这些数值和我们一直提的晶体管和电路有什么关系。

### 推荐阅读 ###

关于二进制和编码，我推荐你读一读《编码：隐匿在计算机软硬件背后的语言》。从电报机到计算机，这本书讲述了很多计算设备的历史故事，当然，也包含了二进制及其背后对应的电路原理。

## 12 | 理解电路：从电报机到门电路，我们如何做到“千里传信”？ ##

所有最终执行的程序其实都是使用“0”和“1”这样的二进制代码来表示的。

### 从信使到电报，我们怎么做到“千里传书”？ ###

从信息编码的角度来说，电报传输的信号有两种，一种是短促的点信号（dot 信号），一种是长一点的划信号（dash 信号）。

### 理解继电器，给跑不动的信号续一秒 ###

为了能够实现这样**接力传输信号**，在电路里面，工程师们造了一个叫作**继电器**（Relay）的设备。

继电器还有一个名字就叫作**电驿**，这个“驿”就是驿站的驿，可以说非常形象了。这个接力的策略不仅可以用在电报中，在通信类的科技产品中其实都可以用到。

通过这些线圈和开关，我们也可以很容易地创建出 “与（AND）”“或（OR）”“非（NOT）”这样的逻辑。我们在输入端的电路上，提供串联的两个开关，只有两个开关都打开，电路才接通，输出的开关也才能接通，这其实就是模拟了计算机里面的**“与”**操作。

我们在输入端的电路，提供两条独立的线路到输出端，两条线路上各有一个开关，那么任何一个开关打开了，到输出端的电路都是接通的，这其实就是模拟了计算机中的**“或”**操作。

当我们把输出端的“螺旋线圈 + 磁性开关”的组合，从默认关掉，只有通电有了磁场之后打开，换成默认是打开通电的，只有通电之后才关闭，我们就得到了一个计算机中的**“非”**操作。输出端开和关正好和输入端相反。这个在数字电路中，也叫作**反向器**（Inverter）。

### 总结延伸 ###

可以说，电报是现代计算机的一个最简单的原型。它和我们现在使用的现代计算机有很多相似之处。我们通过电路的“开”和“关”，来表示“1”和“0”。就像晶体管在不同的情况下，表现为导电的“1”和绝缘的“0”的状态。

我们通过电报机这个设备，看到了如何通过“螺旋线圈 + 开关”，来构造基本的逻辑电路，我们也叫门电路。一方面，我们可以通过继电器或者中继，进行长距离的信号传输。另一方面，我们也可以通过设置不同的线路和开关状态，实现更多不同的信号表示和处理方式，这些线路的连接方式其实就是我们在数字电路中所说的门电路。而这些门电路，也是我们创建 CPU 和内存的基本逻辑单元。我们的各种对于计算机二进制的“0”和“1”的操作，其实就是来自于门电路，叫作组合逻辑电路。

### 推荐阅读 ###

《编码：隐匿在计算机软硬件背后的语言》的第 6～11 章

## 13 | 加法器：如何像搭乐高一样搭电路（上） ##

在计算机硬件层面设计最基本的单元，门电路。门电路非常简单，只能做简单的 “与（AND）”“或（OR）”“NOT（非）”和“异或（XOR）”，这样最基本的单比特逻辑运算。

### 异或门和半加器 ###

基础门电路，输入都是两个单独的 bit，输出是一个单独的 bit。如果我们要对 2 个 8 位（bit）的数，计算与、或、非这样的简单逻辑运算，其实很容易。只要连续摆放 8 个开关，来代表一个 8 位数。这样的两组开关，从左到右，上下单个的位开关之间，都统一用“与门”或者“或门”连起来，就是两个 8 位数的 AND 或者 OR 的运算了。



### 全加器 ###

半加器（Half Adder），全加器（Full Adder）

**我们用两个半加器和一个或门，就能组合成一个全加器。**

### 总结延伸 ###

## 14 | 乘法器：如何像搭乐高一样搭电器（下） ##

## 15 | 浮点数和定点数（上）：怎么用有限的Bit表示尽可能多的信息？ ##

### 浮点数的不确定性 ###

用二进制来表示十进制的编码方式，叫作**BCD 编码（Binary-Coded Decimal）**。

1. 这样的表示方式有点“浪费”
2. 这样的表示方式没办法同时表示很大的数字和很小的数字

### 浮点数的表示 ###

浮点数（Floating Point），也就是float类型。

### 定点数的表示 ###

### 总结延伸 ###



### 推荐阅读 ###

计算机组成与设计：硬件 / 软件接口的 3.5.1 节，了解浮点数

### 课后思考 ###

对于 BCD 编码的定点数，如果我们用 7 个比特来表示连续两位十进制数，也就是 00～99，是不是可以让 32 比特表示更大一点的数据范围？如果我们还需要表示负数，那么一个 32 比特的 BCD 编码，可以表示的数据范围是多大？

原理篇：处理器

## 17 | 建立数据通路（上）：指令+运算=CPU ##




## 18 | 建立数据通路（中）：指令+运算=CPU ##

## 19 | 建立数据通路（下）：指令+运算=CPU ##

通过一个时钟信号，我们可以实现计数器，这个会成为我们的 PC 寄存器。然后，我们还需要一个能够帮我们在内存里面寻找指定数据地址的译码器，以及解析读取到的机器指令的译码器。这样，我们就能把所有学习到的硬件组件串联起来，变成一个 CPU，实现我们在计算机指令的执行部分的运行步骤。

### PC寄存器所需要的计数 ###

PC寄存器（程序计数器），有了时钟信号，我们可以提供定时的输入；有了 D 型触发器，我们可以在时钟信号控制的时间点写入数据。我们把这两个功能组合起来，就可以实现一个自动的计数器了。

# 答疑与加餐 #

特别加餐 | 

## FAQ第一期 | 学与不学，知识就在那里，不如就先学好了 ##

## 特别加餐 | 我的一天怎么过？ ##

### 精选留言 ###

**Q**

👍 真实&典型的一天
有人的时间安排是以自我为中心，比较严格地去控制每件事占用的时间。老师的时间安排也是这样的吗？您怎么看这种做法？

**A**

工作中的事情，我大致会分成三类：

1. 一类是重要的需要大块时间的事情，比如产品的RoadMap，大的系统设计。
2. 一类是即时响应性的工作。比如来自各种内外部的邮件，或者随时有同事来问或者讨论的各类问题。
3. 一类是长期必须完成的工作，包括和同事1对1沟通，日常的周会，招聘。

工作安排时间希望尽量通过做第3点来减少第2点的时间。并且尽量能多花时间在1上。

每天都会列一些To-Do，然后尽量保障能够清掉2-3个。如果连续一段时间觉得自己在1上花得少，就需要反思手上有哪些工作是可以交给其他同事得。以及是否有些事情从整个公司团队层面就不该做。

**Q**

老师，几本操作系统推荐书

**A**

操作系统的推荐可以去看看刘超老师的Linux操作系统课。

当然，最经典的教材也还是《现代操作系统》

**Q**

不太理解 访存 为什么在 执行 后面

**A**

你好，这个5阶段通常是指MIPS这样的RISC的一个简化的模型。开始执行指令之后，才会知道要从内存的什么地址读取数据，这个时候才会进入访存阶段。其实访问读取完成之后会继续进行执行过程。

而很多EX计算的指令都直接从寄存器读取数据，所以不需要访存。

# 原理篇：存储与I/O系统 #

## 35 | 存储器层次结构全景：数据存储的大金字塔长什么样？ ##

### 理解存储器的层次结构 ###

我们常常把 CPU 比喻成计算机的“大脑”。我们思考的东西，就好比 CPU 中的寄存器（Register）。寄存器与其说是存储器，其实它更像是 CPU 本身的一部分，只能存放极其有限的信息，但是速度非常快，和 CPU 同步。

而我们大脑中的记忆，就好比 CPU Cache（CPU 高速缓存，我们常常简称为“缓存”）。CPU Cache 用的是一种叫作 SRAM（Static Random-Access Memory，静态随机存取存储器）的芯片。

### SRAM ###

在 CPU 里，通常会有 L1、L2、L3 这样三层高速缓存。每个 CPU 核心都有一块属于自己的 L1 高速缓存，通常分成**指令缓存**和**数据缓存**，分开存放 CPU 使用的指令和数据。

### DRAM ###

内存用的芯片和 Cache 有所不同，它用的是一种叫作 DRAM（Dynamic Random Access Memory，动态随机存取存储器）的芯片，比起 SRAM 来说，它的密度更高，有更大的容量，而且它也比 SRAM 芯片便宜不少。

DRAM 被称为“动态”存储器，是因为 DRAM 需要靠不断地“刷新”，才能保持数据被存储起来。DRAM 的一个比特，只需要一个晶体管和一个电容就能存储。所以，DRAM 在同样的物理空间下，能够存储的数据也就更多，也就是存储的“密度”更大。但是，因为数据是存储在电容里的，电容会不断漏电，所以需要定时刷新充电，才能保持数据不丢失。DRAM 的数据访问电路和刷新电路都比 SRAM 更复杂，所以访问延时也就更长。

### 存储器的层级结构 ###

这样，各个存储器只和相邻的一层存储器打交道，并且随着一层层向下，存储器的容量逐层增大，访问速度逐层变慢，而单位存储成本也逐层下降，也就构成了我们日常所说的存储器层次结构。

### 使用存储器的时候，该如何权衡价格和性能？ ###

存储器在不同层级之间的性能差异和价格差异，都至少在一个数量级以上。L1 Cache 的访问延时是 1 纳秒（ns），而内存就已经是 100 纳秒了。在价格上，这两者也差出了 400 倍。

### 总结延伸 ###

常常把 CPU 比喻成高速运转的大脑，那么和大脑同步的寄存器（Register），就存放着我们当下正在思考和处理的数据。而 L1-L3 的 CPU Cache，好比存放在我们大脑中的短期到长期的记忆。我们需要小小花费一点时间，就能调取并进行处理。

我们自己的书桌书架就好比计算机的内存，能放下更多的书也就是数据，但是找起来和看起来就要慢上不少。而图书馆更像硬盘这个外存，能够放下更多的数据，找起来也更费时间。从寄存器、CPU Cache，到内存、硬盘，这样一层层下来的存储器，速度越来越慢，空间越来越大，价格也越来越便宜。

这三个“越来越”的特性，使得我们在组装计算机的时候，要组合使用各种存储设备。越是快且贵的设备，实际在一台计算机里面的存储空间往往就越小。而越是慢且便宜的设备，在实际组装的计算机里面的存储空间就会越大。

### 补充阅读 ###

关于不同存储器的访问延时数据

1. 第一个是 Peter Novig 的Teach Yourself Programming in Ten Years。我推荐你在了解这些数据之后读一读这篇文章。这些数字随着摩尔定律的发展在不断缩小，但是在数量级上仍然有着很强的参考价值。
2. 第二个是 Jeff Dean 的Build Software Systems at Google and Lessons Learned。这份 PPT 中不仅总结了这些数字，还有大量的硬件故障、高可用和系统架构的血泪经验。尽管这是一份 10 年前的 PPT，但也非常值得阅读。

### 课后思考 ###

在上世纪 80～90 年代，3.5 寸的磁盘大行其道。它的存储空间只有 1.44MB，比起当时 40MB 的硬盘，它却被大家认为是“海量”存储的主要选择。你猜一猜这是为什么？

## 36 | 局部性原理：数据库性能跟不上，加个缓存就好了？ ##

平时进行服务端软件开发的时候，我们通常会把数据存储在数据库里。而服务端系统遇到的第一个性能瓶颈，往往就发生在访问数据库的时候。这个时候，大部分工程师和架构师会拿出一种叫作“缓存”的武器，通过使用 Redis 或者 Memcache 这样的开源软件，在数据库
前面提供一层缓存的数据，来缓解数据库面临的压力，提升服务端的程序性能。

那么，不知道你有没有想过，这种添加缓存的策略一定是有效的吗？或者说，这种策略在什么情况下是有效的呢？如果从理论角度去分析，添加缓存一定是我们的最佳策略么？进一步地，如果我们对于访问性能的要求非常高，希望数据在 1 毫秒，乃至 100 微秒内完成处理，我们还能用这个添加缓存的策略么？

### 理解局部性原理 ###

**我们能不能既享受 CPU Cache 的速度，又享受内存、硬盘巨大的容量和低廉的价格呢？**

想要同时享受到这三点，那就是，存储器中数据的**局部性原理**（Principle of Locality）。我们可以利用这个局部性原理，来制定管理和访问数据的策略。这个局部性原理包括**时间局部性**（temporal locality）和**空间局部性**（spatial locality）这两种策略。

* 时间局部性：如果一个数据被访问了，那么它在短时间内还会被再次访问。
* 空间局部性：这个策略是说，如果一个数据被访问了，那么和它相邻的数据也很快会被访问。

有了时间局部性和空间局部性，我们不用再把所有数据都放在内存里，也不用都放在 HDD 硬盘上，而是把访问次数多的数据，放在贵但是快一点的存储器里，把访问次数少的数据，放在慢但是大一点的存储器里。这样组合使用内存、SSD 硬盘以及 HDD 硬盘，使得我们可以用最低的成本提供实际所需要的数据存储、管理和访问的需求。

### 如何花最少的钱，装下亚马逊的所有商品？ ###

1. 假设亚马逊有6亿件商品
2. 每件商品需要4MB的存储空间
3. 一共需要2400TB（=6亿*4MB）
4. 把所有数据都放在内存中，需要3600万美元（=  2400TB/1MB  × 0.015 美元  =  3600 万美元）
	* 这 6 亿件商品中，不是每一件商品都会被经常访问
	* 如果我们只在内存里放前 1% 的热门商品，也就是 600 万件热门商品，而把剩下的商品，放在机械式的 HDD 硬盘上
	* 需要的存储成本就下降到 45.6 万美元（  =  3600 万美元 × 1% + 2400TB / 1MB × 0.00004 美元）
5. 一旦内存里面放不下了，我们就把最长时间没有在内存中被访问过的数据，从内存中移走，这个其实就是我们常用的 LRU（Least Recently Used）缓存算法。

只放 600 万件商品真的可以满足我们实际的线上服务请求吗？这个就要看 LRU 缓存策略的缓存命中率（Hit Rate/Hit Ratio）了，也就是访问的数据中，可以在我们设置的内存缓存中找到的，占有多大比例。

内存的随机访问请求需要 100ns。在极限情况下，内存可以支持 1000 万次随机访问。

内存的随机访问请求需要 100ns。这也就意味着，在极限情况下，内存可以支持 1000 万次随机访问。我们用了 24TB 内存，如果 8G 一条的话，意味着有 3000 条内存，可以支持每秒 300 亿次（  =  24TB/8GB  ×  1s/100ns）访问。以亚马逊 2017 年 3 亿的用户数来看，我们估算每天的活跃用户为 1 亿，这 1 亿用户每人平均会访问 100 个商品，那么平均每秒访问的商品数量，就是 12 万次。

但是如果数据没有命中内存，那么对应的数据请求就要访问到 HDD 磁盘了。刚才的图表中，我写了，一块 HDD 硬盘只能支撑每秒 100 次的随机访问，2400TB 的数据，以 4TB 一块磁盘来计算，有 600 块磁盘，也就是能支撑每秒 6 万次（  =  2400TB/4TB  × 1s/10ms  ）的随机访问。

这就意味着，所有的商品访问请求，都直接到了 HDD 磁盘，HDD 磁盘支撑不了这样的压力。我们至少要 50% 的缓存命中率，HDD 磁盘才能支撑对应的访问次数。不然的话，我们要么选择添加更多数量的 HDD 硬盘，做到每秒 12 万次的随机访问，或者将 HDD 替换成 SSD 硬盘，让单个硬盘可以支持更多的随机访问请求。

在实际的应用程序中，查看一个商品的数据可能意味着不止一次的随机内存或者随机磁盘的访问。对应的数据存储空间也不止要考虑数据，还需要考虑维护数据结构的空间，而缓存的命中率和访问请求也要考虑均值和峰值的问题。

通过这个估算过程，你需要理解，如何进行存储器的硬件规划。你需要考虑硬件的成本、访问的数据量以及访问的数据分布，然后根据这些数据的估算，来组合不同的存储器，能用尽可能低的成本支撑所需要的服务器压力。而当你用上了数据访问的局部性原理，组合起了多种存储器，你也就理解了怎么基于存储器层次结构，来进行硬件规划了。

### 总结延伸 ###

计算机存储器层次结构中最重要的一个优化思路，就是局部性原理。

在实际的计算机日常的开发和应用中，我们对于数据的访问总是会存在一定的局部性。有时候，这个局部性是时间局部性，就是我们最近访问过的数据还会被反复访问。有时候，这个局部性是空间局部性，就是我们最近访问过数据附近的数据很快会被访问到。

而局部性的存在，使得我们可以在应用开发中使用缓存这个有利的武器。比如，通过将热点数据加载并保留在速度更快的存储设备里面，我们可以用更低的成本来支撑服务器。

### 推荐阅读 ###

《计算机组成与设计：硬件 / 软件接口》

## 37 | 高速缓存（上）：“4毫秒”究竟值多少钱？ ##



## 38 | 高速缓存（下）：你确定你的数据更新了么？ ##