# MySQL实战45讲 #

# 开篇词 #

## 开篇词 | 这一次，让我们一起来搞懂MySQL ##

平时使用数据库时高频出现的知识，如事务、索引、锁等内容构成专栏的主线。


点->线->面，形成自己的MySQL知识网络

# 基础篇 #

## 01 | 基础架构：一条SQL查询语句是如何执行的？ ##

mysql> select * from T where ID = 10;

![0d2070e8f84c4801adbfa03bda1f98d9](img/0d2070e8f84c4801adbfa03bda1f98d9.png)

MySQL分为Server层和存储引擎层两部分。

Server层包括连接器、查询缓存、分析器、优化器、执行器等，涵盖MySQL的大部分核心服务功能，以及所有的内置函数（如日期、时间、数学和加密函数等），所有跨存储引擎的功能都在这一层实现，比如存储过程、触发器、视图等。

存储引擎层负责数据的存储和提取。其架构模式是插件式的，支持 InnoDB、MyISAM、Memory 等多个存储引擎。现在最常用的存储引擎是 InnoDB，它从 MySQL 5.5.5 版本开始成为了默认存储引擎。

不同的存储引擎共用一个Server层，也就是从连接器到执行器的部分。

### 连接器 ###

1. 先连接到这个数据库上，这时候接待你的就是连接器。连接器负责跟客户端建立连接、获取权限、维持和管理连接。`mysql -h$ip -P$port -u$user -p`（连接命令中的 mysql 是客户端工具，用来跟服务端建立连接。在完成经典的 TCP 握手后，连接器就要开始认证你的身份，这个时候用的就是你输入的用户名和密码。）

建立连接的过程通常比较复杂的，所以在使用中要尽量减少建立连接的动作，也尽量使用长连接。但全部使用长连接后，有时候MySQL占用内存涨得特别快，因为MySQL在执行过程中临时使用的内存是管理在连接对象里面的。=>内存占用太大，被系统强行杀掉（OOM）=>MySQL异常重启

解决方案：

1. 定期断开长连接。使用一段时间后，或者程序里面判断执行过一个占用内存的大查询后，断开连接，之后要查询再重连。
2. 如果是 MySQL 5.7 或更新版本，可以在每次执行一个比较大的操作后，通过执行 mysql_reset_connection 来重新初始化连接资源。这个过程不需要重连和重新做权限验证，但是会将连接恢复到刚刚创建完时的状态。

    wait_timeout默认值是8小时

### 查询缓存 ###

*但是大多数情况下我会建议你不要使用查询缓存，为什么呢？因为查询缓存往往弊大于利。*

查询缓存的失效非常频繁，只要有对一个表的更新，这个表上所有的查询缓存都会被清空。除非你的业务就是有一张静态表，很长时间才会更新一次。比如，一个系统配置表，那这张表上的查询才适合使用查询缓存。

8.0以后没有这块功能。

### 分析器 ###

如果没有命中查询缓存

1. MySQL需要对SQL语句做解析
2. 词法分析， `select * from T where ID = 10;` 比如select，识别出来是一个查询语句，把字符串“T”识别成“表名T”，把字符串“ID”识别成“列ID”。
3. 语法分析，语法分析器会根据语法判断，判断是否满足MySQL语法。

### 优化器 ###

优化器是在表里面有多个索引的时候，决定使用哪个索引；或者在一个语句有多表关联（join）的时候，决定各个表的连接顺序。比如你执行下面这样的语句，这个语句是执行两个表的 join：

	mysql> select * from t1 join t2 using(ID) where t1.c=10 and t2.d=20;

* 既可以先从表 t1 里面取出 c=10 的记录的 ID 值，再根据 ID 值关联到表 t2，再判断 t2 里面 d 的值是否等于 20。
* 也可以先从表 t2 里面取出 d=20 的记录的 ID 值，再根据 ID 值关联到 t1，再判断 t1 里面 c 的值是否等于 10。

逻辑结果一样，但执行的效率不同，优化器的作用就是决定选择使用哪个方案。

优化器阶段完成后，这个语句的执行方案就确定下来了。

### 执行器 ###

执行器阶段，开始执行语句。

开始执行的时候，要先判断一下你对这个表 T 有没有执行查询的权限，如果没有，就会返回没有权限的错误，如下所示 (在工程实现上，如果命中查询缓存，会在查询缓存返回结果的时候，做权限验证。查询也会在优化器之前调用 precheck 验证权限)。

如果有权限，就打开表继续执行。打开表的时候，执行器就会根据表的引擎定义，去使用这个引擎提供的接口。

比如我们这个例子中的表 T 中，ID 字段没有索引，那么执行器的执行流程是这样的：

1. 调用InnoDB引擎接口取这个表的第一行，判断ID值是不是10，如果不是则跳过，如果是则将这行存在结果集中；
2. 调用引擎接口取“下一行”，重复相同的判断逻辑，直到取到这个表的最后一行。
3. 执行器将上述遍历过程中所有满足条件的行组成的记录集作为结果集返回给客户端。

数据库的慢查询日志中看到一个 rows_examined 的字段，表示这个语句执行过程中扫描了多少行。这个值就是在执行器每次调用引擎获取数据行的时候累加的。

引擎扫描行数跟 rows_examined 并不是完全相同的。

### 小结 ###

MySQL的逻辑架构，对一个SQL语句完整执行流程的各个阶段有一个初步的印象。由于篇幅的限制，只是用一个查询的例子将各个环节过了一遍。

### 精选留言 ###

#### Q ####

如果表 T 中没有字段 k，而你执行了这个语句 select * from T where k=1, 那肯定是会报“不存在这个列”的错误： “Unknown column ‘k’ in ‘where clause’”。你觉得这个错误是在我们上面提到的哪个阶段报出来的呢？

#### A ####

* Oracle会在分析阶段判断语句是否正确，表是否存在，列是否存在等。

* 对权限的检查不在优化器之前做，，SQL语句要操作的表不只是SQL字面上那些。比如如果有个触发器，得在执行器阶段（过程中）才能确定。优化器阶段前是无能为力的。

分析器，优化器是对SQL语句进行择优处理，执行器需要判断选取表中的权限。

#### Q ####

为什么对权限的检查不在优化器之前做？

#### A ####

SQL语句要操作的表不只是SQL字面上那些。比如如果有个触发器，得在执行器阶段（过程中）才能确定。

#### Q ####

加粗内容为未掌握

1. MySQL的框架有几个组件, 各是什么作用? 
**2. Server层和存储引擎层各是什么作用?**
**3. you have an error in your SQL syntax 这个保存是在词法分析里还是在语法分析里报错?**
4. 对于表的操作权限验证在哪里进行?
**5. 执行器的执行查询语句的流程是什么样的? **

#### A ####

1. Server层和存储引擎，Server层分连接器、查询缓存、分析器、优化器、执行器。
**2. Server层：涵盖MySQL的大多数核心服务功能，以及内置函数；存储引擎：负责数据的存储和提取**
3. 语法分析——**词法分析：识别里面的字符串分别是什么，代表什么；语法分析：**
4. 执行器，在做执行器之前，通过获取一张表的数据，验证该用户是否有表的操作权限
**5. 执行器进行检索流程如下：
	* 调用InnoDB引擎接口取这个表的第一行
	* 调用引擎接口取“下一行”，重复相同的判断逻辑，直到取到这个表的最后一行
	* 执行器将上述遍历过程中所有满足条件的行组成的记录集作为结果集返回给客户端 **

#### A ####

## 02 | 日志系统：一条SQL更新语句是如何执行的？ ##

一条查询语句的执行过程一般是经过连接器、分析器、优化器、执行器等功能模块，最后到达存储引擎。

查询语句对策那一套流程，更新语句也是同样走一遍。

1. 连接器：执行语句前要先连接数据库
2. 查询缓存：一个表上有更新的时候，跟这个表有关的查询缓存会失效。所以这条语句就会把表T上所有缓存结果都清空（不建议使用查询缓存的原因）
3. 分析器：通过词法和语法解析知道这是一条更新语句
4. 优化器：使用ID索引
5. 执行器：负责具体执行，找到这一行，更新

### 重要的日志模块：redo log ###

WAL的全称是Write-Ahead Logging，关键点是先写日志，再写磁盘。

当有一条记录需要更新的时候，InnoDB引擎就会先把记录写到redo log（粉板）里面，并更新内存。InnoDB引擎会在适当的时候，将这个操作记录更新到磁盘里面（系统比较空闲的时候处理）。如果写满了，需要腾出空间才行。

InnoDB 的 redo log 是固定大小的，比如可以配置为一组 4 个文件，每个文件的大小是 1GB，那么这块“粉板”总共就可以记录 4GB 的操作。从头开始写，写到末尾就又回到开头循环写。

![16a7950217b3f0f4ed02db5db59562a7.png](img/16a7950217b3f0f4ed02db5db59562a7.png)

* write pos是当前记录的位置，一边写一边后移，写到第3号文件末尾后就回到0号文件开头。
* checkpoint是当前要擦除的位置，也是往后推移并且循环，擦除记录前要把记录更新到数据文件。
* write pos和checkpoint之间的是“粉版”上还空着的部分，可以用来记录新的操作。如果 write pos 追上 checkpoint，表示“粉板”满了，这时候不能再执行新的更新，得停下来先擦掉一些记录，把 checkpoint 推进一下。

crash-safe：因为redo log，InnoDB 就可以保证即使数据库发生异常重启，之前提交的记录都不会丢失

### 重要的日志模块：binlog ###

MySQL整体来看，有两块

1.  Server 层，主要做的是MySQL功能层面的事情
2.  引擎层，负责存储相关的具体事宜

---

1. redo log 是 InnoDB 引擎特有的；binlog 是 MySQL 的 Server 层实现的，所有引擎都可以使用。
2. redo log 是物理日志，记录的是“在某个数据页上做了什么修改”；binlog 是逻辑日志，记录的是这个语句的原始逻辑，比如“给 ID=2 这一行的 c 字段加 1 ”。
3. redo log 是循环写的，空间固定会用完；binlog 是可以追加写入的。“追加写”是指 binlog 文件写到一定大小后会切换到下一个，并不会覆盖以前的日志。

执行器和 InnoDB 引擎在执行这个简单的 update 语句时的内部流程：`mysql> update T set c=c+1 where ID=2;`

![2e5bff4910ec189fe1ee6e2ecc7b4bbe.png](img/2e5bff4910ec189fe1ee6e2ecc7b4bbe.png)

1. 
	* 执行器：执行器先找引擎取 ID=2 这一行。
	* 引擎：ID 是主键，引擎直接用树搜索找到这一行。如果 ID=2 这一行所在的数据页本来就在内存中，就直接返回给执行器；否则，需要先从磁盘读入内存，然后再返回。
2. 执行器：执行器拿到引擎给的行数据，把这个值加上 1，比如原来是 N，现在就是 N+1，得到新的一行数据，再调用引擎接口写入这行新数据。 
3. 引擎：引擎将这行新数据更新到内存中，同时将这个更新操作记录到 redo log 里面，此时 redo log 处于 prepare 状态。然后告知执行器执行完成了，随时可以提交事务。
4. 执行器：执行器生成这个操作的 binlog，并把 binlog 写入磁盘。
5. 引擎：执行器调用引擎的提交事务接口，引擎把刚刚写入的 redo log 改成提交（commit）状态，更新完成。

redo log的写入拆成了两个步骤：prepare和commit，两阶段提交

### 两阶段提交 ###

两阶段提交的原因是为了让两份日志的逻辑一致。=>怎样让数据库恢复到半个月内任意一秒的状态？

1. binlog会记录所有的逻辑操作
2. 系统定期做整库备份
3. 找到最近一天的全量备份，然后通过这个备份恢复到临时库
4. 从备份时间开始，将备份的binlog依次取出，重新进行到误操作时间点的那个时候

如果不使用两阶段提交。

1. 先写 redo log 后写 binlog。
	* redo log 写完之后，系统即使崩溃，仍然能够把数据恢复回来，所以恢复后这一行 c 的值是 1。
	* 但是由于 binlog 没写完就 crash 了，这时候 binlog 里面就没有记录这个语句。如果需要用这个 binlog 来恢复临时库的话，由于这个语句的 binlog 丢失，这个临时库就会少了这一次更新，恢复出来的这一行 c 的值就是 0，与原库的值不同。
2. 先写 binlog 后写 redo log。
	* 如果在 binlog 写完之后 crash，由于 redo log 还没写，崩溃恢复以后这个事务无效，所以这一行 c 的值是 0。但是 binlog 里面已经记录了“把 c 从 0 改成 1”这个日志。所以，在之后用 binlog 来恢复的时候就多了一个事务出来，恢复出来的这一行 c 的值就是 1，与原库的值不同。

不只是操作，还有扩容的时候。通常的做法是用全量备份加上应用 binlog 来实现的。

简单说，redo log 和 binlog 都可以用于表示事务的提交状态，而两阶段提交就是让这两个状态保持逻辑上的一致。

### 小结 ###

* 两个日志，即物理日志redo log和逻辑日志binlog。
* redo log用于保证crash-safe能力。
	*  innodb_flush_log_at_trx_commit 这个参数设置成 1 的时候，表示每次事务的 redo log 都直接持久化到磁盘。这个参数我建议你设置成 1，这样可以保证 MySQL 异常重启之后数据不丢失。
* sync_binlog  
	* sync_binlog 这个参数设置成 1 的时候，表示每次事务的 binlog 都持久化到磁盘。这个参数我也建议你设置成 1，这样可以保证 MySQL 异常重启之后 binlog 不丢失。 
* 与 MySQL 日志系统密切相关的“两阶段提交”：两阶段提交是跨系统维持数据逻辑一致性时常用的一个方案，即使你不做数据库内核开发，日常开发中也有可能会用到。   

前面我说到定期全量备份的周期“取决于系统重要性，有的是一天一备，有的是一周一备”。那么在什么场景下，一天一备会比一周一备更有优势呢？或者说，它影响了这个数据库系统的哪个指标？

最长恢复时间更短

### 精选留言 ###

#### Q ####

说到定期全量备份的周期“取决于系统重要性，有的是一天一备，有的是一周一备”。那么在什么场景下，一天一备会比一周一备更有优势呢？或者说，它影响了这个数据库系统的哪个指标？

#### A ####

一天一备，只需要根据前一天的redo log的记录把binarylog 复现一遍即可，不容易无操作，隔断时间比较少。但这样redolog的记录会比较大。

Binlog两种模式

1. statement格式的话是记sql语句
2. row格式会记录行的内容，记两条，更新前和更新后的内容

redo是物理的，binlog是逻辑的；现在由于redo是属于InnoDB引擎，所以必须要有binlog（可以使用别的引擎）保证数据库的一致性，必须要保证2份日志一致。使用的2阶段式提交；其他感觉像事务，不是成功就是失败，不能让中间环节出现，也就是一个成功，一个失败。

#### Q ####

只用InonoDB引擎的时候还保留Binlog这种设计的原因

#### A ####

1. redolog只有InnoDB有，别的引擎没有
2. redolog是循环写的，不持久保存，binlog的“归档”这个功能，redolog是不具备的。

## 03 | 事务隔离：为什么你改了我还看不见？ ##

事务就是要保证一组数据库操作，要么全部成功，要么全部失败。

### 隔离性与隔离级别 ###

ACID（Atomicity，Consistency，Isolation，Durability，即原子性、一致性、隔离性和持久性）

当数据库上有多个事务同时执行的时候=>可能出现脏读（dirty read）、不可重复读（non-repeatable read）、幻读（phantom read）的问题=>有了“隔离级别”的概念，去解决这些问题

#### 隔离级别 ####

隔离级别越高，效率越低=>需要找到平衡点

SQL标准的事务隔离级别：

1. 读未提交（read uncommitted）：一个事务还没提交时，它做的变更就能被别的事务看到。
2. 读提交（read committed）：一个事务提交之后，它做的变更才会被其他事务看到。
3. 可重复读（repeatable read）：一个事务执行过程中看到的数据，总是跟这个事务在启动时看到的数据是一致的。当然在可重复读隔离级别下，未提交变更对其他事务也是不可见的。
4. 串行化（serializable）：对于同一行记录，“写”会加“写锁”，“读”会加“读锁”。当出现读写锁冲突的时候，后访问的事务必须等前一个事务执行完成，才能继续执行。

![7dea45932a6b722eb069d2264d0066f8](img/7dea45932a6b722eb069d2264d0066f8.png)

根据事务隔离级别的不同，V1、V2、V3返回值也会分别不同。

* 读未提交：V2、V3=2，B的事务还没提交，但做个变更就能被别的事务看到，所以V1也是2
* 读提交：V2、V3=2，事务 B 的更新在提交后才能被 A 看到，所以V1也是1
* 可重复读：事务在执行期间看到的数据前后必须是一致的，所以v1，v2=1，v3=2
* 串行化：则在事务 B 执行“将 1 改成 2”的时候，会被锁住。直到事务 A 提交后，事务 B 才可以继续执行。v1、v2值是1，v3的值是2

在实现上，数据库里面会创建一个视图，访问的时候以视图的逻辑结果为准。

* “读提交”隔离级别：在每个 SQL 语句开始执行的时候创建的
* “可重复读”隔离级别：在事务启动时创建的，整个事务存在期间都用这个视图

“读未提交”隔离级别：直接返回记录上的最新值，没有视图概念；而“串行化”隔离级别下直接用加锁的方式来避免并行访问。

在不同的隔离级别下，数据库行为是有所不同的。Oracle 数据库的默认隔离级别其实就是“读提交”，因此对于一些从 Oracle 迁移到 MySQL 的应用，为保证数据库隔离级别的一致，你一定要记得将 MySQL 的隔离级别设置为“读提交”。

	show variables like 'transaction_isolation'

根据业务场景选择不同的隔离级别

### 事务隔离实现 ###

假设一个值从1被按顺序改成2、3、4，在回滚日志会有如下的记录：

![d9c313809e5ac148fc39feff532f0fee](img/d9c313809e5ac148fc39feff532f0fee.png)

回滚日志删除条件：系统判断，当前没有事务再需要用到这些回滚日志时，回滚日志会被删除。当系统里没有比这个回滚日志更早的 read-view 的时候会被正式清除。

在 MySQL 中，实际上每条记录在更新的时候都会同时记录一条回滚操作。记录上的最新值，通过回滚操作，都可以得到前一个状态的值。

长事务意味着系统里面会存在很老的事务视图。由于这些事务随时可能访问数据库里面的任何数据，所以这个事务提交之前，数据库里面它可能用到的回滚记录都必须保留，这就会导致大量占用存储空间。在 MySQL 5.5 及以前的版本，回滚日志是跟数据字典一起放在 ibdata 文件里的，即使长事务最终提交，回滚段被清理，文件也不会变小。（我见过数据只有 20GB，而回滚段有 200GB 的库。最终只好为了清理回滚段，重建整个库。）还有占用锁资源，拖垮整个库的问题。

### 事务的启动方式 ###

MySQL 的事务启动方式有以下几种：

1. 显式启动事务语句， begin 或 start transaction。配套的提交语句是 commit，回滚语句是 rollback。
2. set autocommit=0，这个命令会将这个线程的自动提交关掉。意味着如果你只执行一个 select 语句，这个事务就启动了，而且并不会自动提交。这个事务持续存在直到你主动执行 commit 或 rollback 语句，或者断开连接。

在 autocommit 为 1 的情况下，用 begin 显式启动的事务，如果执行 commit 则提交事务。如果执行 commit work and chain，则是提交事务并自动启动下一个事务，这样也省去了再次执行 begin 语句的开销。同时带来的好处是从程序开发的角度明确地知道每个语句是否处于事务中。

information_schema 库的 innodb_trx 这个表中查询长事务，比如下面这个语句，用于查找持续时间超过 60s 的事务。

	select * from information_schema.innodb_trx where TIME_TO_SEC(timediff(now(),trx_started)) > 60

### 小结 ###

1. 介绍了 MySQL 的事务隔离级别的现象和实现
2. 根据实现原理分析了长事务存在的风险
3. 如何用正确的方式避免长事务

我给你留一个问题吧。你现在知道了系统里面应该避免长事务，如果你是业务开发负责人同时也是数据库负责人，你会有什么方案来避免出现或者处理这种情况呢？

## 04 | 深入浅出牵引（上） ##

索引的出现其实就是为了提高数据查询的效率，就像书的目录一样。对于数据库的表而言，索引其实就是它的“目录”。

### 索引的常见模型 ###

索引的出现是为了提高查询效率，三种常见的模型：

1. 哈希表
2. 有序数组
3. 搜索树

#### 哈希表 ####

哈希表是一种以键 - 值（key-value）存储数据的结构，输入待查找的值即 key，就可以找到其对应的值即 Value。哈希的思路很简单，把值放在数组里，用一个哈希函数把 key 换算成一个确定的位置，然后把 value 放在数组的这个位置。

多个key值经过哈希函数的换算，会出现同一个值的情况。处理这种情况的一种方法是，拉出一个链表。

哈希表这种结构适用于只有等值查询的场景

#### 有序数组 ####

有序数组在等值查询和范围查询场景中的性能就都非常优秀。

仅仅看查询效率，有序数组就是最好的数据结构。但需要更新数据的时候就往中间插入一个记录必需挪动后面所有的记录，成本很高。

有序数组索引只适用于静态存储引擎。

#### 搜索树 ####

二叉搜索树：每个节点的左儿子小于父节点，父节点又小于右儿子。

树可以有二叉，也可以有多叉。多叉树就是每个节点有多个儿子，儿子之间的大小保证从左到右递增。二叉树是搜索效率最高的，但是实际上大多数的数据库存储却并不使用二叉树。其原因是，索引不止存在内存中，还要写到磁盘上。

在MySQL中，索引是在存储引擎层实现的，所以并没有统一的索引标准，即不同存储的引擎的工作方式并不一样。而即使多个存储引擎支持同一种类型的索引，其底层的实现也可能不同。

#### 实战 ####

在 MySQL 中，索引是在存储引擎层实现的，所以并没有统一的索引标准，即不同存储引擎的索引的工作方式并不一样。而即使多个存储引擎支持同一种类型的索引，其底层的实现也可能不同。由于 InnoDB 存储引擎在 MySQL 数据库中使用最为广泛，所以下面我就以 InnoDB 为例，和你分析一下其中的索引模型。

### InnoDB的索引模型 ###

在 InnoDB 中，表都是根据主键顺序以索引的形式存放的，这种存储方式的表称为索引组织表。InnoDB 使用了 B+ 树索引模型，所以数据都是存储在 B+ 树中的。

每一个索引在 InnoDB 里面对应一棵 B+ 树。

* 主键索引的叶子节点存的是整行数据。在 InnoDB 里，主键索引也被称为聚簇索引（clustered index）。
* 非主键索引的叶子节点内容是主键的值。在 InnoDB 里，非主键索引也被称为二级索引（secondary index）。

#### 基于主键索引和普通索引的查询有什么区别？ ####

* 如果语句是 select * from T where ID=500，即主键查询方式，则只需要搜索 ID 这棵 B+ 树；
* 如果语句是 select * from T where k=5，即普通索引查询方式，则需要先搜索 k 索引树，得到 ID 的值为 500，再到 ID 索引树搜索一次。这个过程称为回表。
* 基于非主键索引的查询需要多扫描一棵索引树。因此，我们在应用中应该尽量使用主键查询。

### 索引维护 ###

B+ 树为了维护索引有序性，在插入新值的时候需要做必要的维护。

自增主键是指自增列上定义的主键，在建表语句中一般是这么定义的： NOT NULL PRIMARY KEY AUTO_INCREMENT。

显然，主键长度越小，普通索引的叶子节点就越小，普通索引占用的空间也就越小。

从性能和存储空间方面考量，自增主键往往是更合理的选择。

1. 只有一个索引；
2. 该索引必须是唯一索引
3. 类似典型的KV场景
4. “尽量使用主键查询”原则，直接将这个索引设置为主键，可以避免每次查询需要搜索两棵树

### 小结 ###

InnoDB采用的B+树结构，以及InnoDB要选择B+树的原因。B+ 树能够很好地配合磁盘的读写特性，减少单次查询的磁盘访问次数。由于 InnoDB 是索引组织表，一般情况下我会建议你创建一个自增主键，这样非主键索引占用的空间最小。但事无绝对，我也跟你讨论了使用业务逻辑字段做主键的应用场景。

### 精选留言 ###

#### Q ####

重建索引k

	alter table T drop index k;
	alter table T add index(k);

如果重建主键索引

	alter table T drop primary key;
	alter table T add primary key(id);

#### A ####

#### Q ####

没有主键的表，有一个普通索引。怎么回表？

#### A ####

没有主键的表，innodb会给默认创建一个Rowid做主键


## 05 | 深入浅出牵引（下） ##

`select * from T where k between 3 and 5`，需要执行几次树的搜索操作，会扫描多少行？

    mysql> create table T (
    ID int primary key,
    k int NOT NULL DEFAULT 0, 
    s varchar(16) NOT NULL DEFAULT '',
    index k(k))
    engine=InnoDB;
    
    insert into T values(100,1, 'aa'),(200,2,'bb'),(300,3,'cc'),(500,5,'ee'),(600,6,'ff'),(700,7,'gg');

这条SQL查询查询语句的执行流程：

1. 在k索引树上找到k=3的记录，取得ID=300;
2. 再到ID索引树查到ID=300对应的R3;
3. 在k索引树取下一个值k=5，取得ID=500;
4. 再回到ID索引树查到ID=500对应的R4;
5. 在k索引树取下一个值k=6，不满足条件，循环结束。

回到主键索引树搜索的过程，我们称为回表。


### 覆盖索引 ###

由于覆盖索引可以减少树的搜索次数，显著提升查询性能，所以使用覆盖索引是一个常用的性能优化手段。

索引字段的维护总是有代价的。在建立冗余索引来支持覆盖索引时就需要权衡考虑了。

### 最左前缀原则 ###

B+ 树这种索引结构，可以利用索引的“最左前缀”，来定位记录。用(name, age)这个联合索引来分析。

在建立联合索引的时候，如何安排索引内的字段顺序。

1. 如果通过调整顺序，可以少维护一个索引，那么这个索引往往就是需要优先考虑采用。
	* 既有联合查询，又有基于a、b各自的查询：只有b的语句，是无法使用（a，b）这个联合索引，这时候不得不维护另外一个索引，同时维护（a，b）、（b）这两个索引
2. 考虑的原则就是空间

### 索引下推 ###

	mysql> select * from tuser where name like '张%' and age=10 and ismale=1;

* 在MySQL5.6之前，只能从ID3开始一个个回表。到主键索引上找出数据行，再对比字段值。
	* 在(name, age)索引里面，InnoDB并不会去看age的值，只是按顺序把“name第一个字是‘张’”的记录一条条取出来回表，需要回表4次。 
* MySQL5.6引入的索引下推优化（index condition pushdown），可以在索引遍历过程中，对索引中包含的字段先做判断，直接过滤掉不满足条件的记录，减少回表次数。
	*  InnoDB在（name， age）索引内部就判断了age是否等于10，对于不等于10的记录，直接判断并跳过。只需要对 ID4、ID5 这两条记录回表取数据判断，就只需要回表 2 次。

### 小结 ###

数据库索引的概念，包括了覆盖索引、前缀索引、索引下推。在满足语句需求的情况下， 尽量少地访问资源是数据库设计的重要原则之一。在使用数据库的时候，尤其是在设计表结构时，也要以减少资源消耗作为目标。


## 06 | 全局锁和表锁：给表加个字段怎么有这么多阻碍 ##

数据库锁设计的初衷是处理并发问题，作为多用户共享的资源，当出现并发访问的时候，数据库需要合理地控制资源的访问规则。而锁就是用来实现这些访问规则的重要数据结构。

*根据加锁的范围，MySQL 里面的锁大致可以分成全局锁、表级锁和行锁三类。*

### 全局锁 ###

全局锁就是对整个数据库实例加锁。通过 Flush tables with read lock(FTWRL) 确保不会有其他线程对数据库做更新，然后对整个库做备份。注意，在备份过程中整个库完全处于只读状态。unlock tables解锁。

*全局锁的典型使用场景是，做全库逻辑备份。*（整库每个表都select出来存成文本）。

* 如果你在主库上备份，那么在备份期间都不能执行更新，业务基本上就得停摆；
* 如果你在从库上备份，那么备份期间从库不能执行主库同步过来的 binlog，会导致主从延迟。

如果不加锁的话，备份系统备份的得到的库不是一个逻辑时间点，这个视图是逻辑不一致的。

*一致性读是好，但前提是引擎要支持这个隔离级别。*对于MyISAM不支持事务的引擎，备份过程中有更新，总是只能取到最新的数据，破坏了备份的一致性。——FTWRL命令

single-transaction方法只适用于所有的表适用事务引擎的库。如果有的表适用了不支持事务的引擎，那么备份就只能通过FTWRL方法。（DBA要求业务开发人员适用InnoDB代替MyISAM的原因之一）。

*既然要全库只读，为什么不使用 set global readonly=true 的方式呢？*

1. 在有些系统中，readonly 的值会被用来做其他逻辑，比如用来判断一个库是主库还是备库。因此，修改 global 变量的方式影响面更大，我不建议你使用。
2. 在异常处理机制上有差异。如果执行 FTWRL 命令之后由于客户端发生异常断开，那么 MySQL 会自动释放这个全局锁，整个库回到可以正常更新的状态。而将整个库设置为 readonly 之后，如果客户端发生异常，则数据库就会一直保持 readonly 状态，这样会导致整个库长时间处于不可写状态，风险较高。

### 表级锁 ###

MySQL 里面表级别的锁有两种：

1. 表锁：`lock tabels...read/write`。
	*  可以用 unlock tables 主动释放锁，也可以在客户端断开的时候自动释放。lock tables 语法除了会限制别的线程的读写外，也限定了本线程接下来的操作对象
2. 元数据锁（meta data lock，MDL)。

对于InnoDB支持行锁的引擎，一般不使用lock tables命令来控制并发，毕竟锁住整个表的影响面很大。

*另一类表级的锁是 MDL（metadata lock)*。MDL不需要显式使用，在访问一个表的时候会自动加上。

MySQL5.5版本中引入了MDL，当对一个表做增删改查操作的时候，加上MDL读锁；当要对表做结构变更操作的时候，加MDL写锁。

* 读锁之间不互斥，因此可以有多个线程同时对一张表增删改查。
* 读写锁之间、写锁之间是互斥的，用来保证变更表结构操作的安全性。因此，如果有两个线程要同时给一个表加字段，其中一个要等另一个执行玩才能开始执行。

给一个表加字段，或者修改字段，或者加索引，需要扫描全表的数据。

#### 如何安全地给小表加字段？ ####

1. 解决长事务，事务不提交，就会一直占着MDL锁。
	* 在 MySQL 的 information_schema 库的 innodb_trx 表中，查到当前执行中的事务。
	* 如果你要做 DDL 变更的表刚好有长事务在执行，要考虑先暂停 DDL，或者 kill 掉这个长事务。
2. 变更的表是一个热点表（数据量不大，但是上面的请求很频繁，不得不加字段）
	* kill未必管用，新的请求马上就来了
	* 在alert table语句里面设定等待时间（如果在这个指定的等待时间里面能够拿到 MDL 写锁最好，拿不到也不要阻塞后面的业务语句，先放弃）
	* repeat

具体语句：

	ALTER TABLE tbl_name NOWAIT add column ...
	ALTER TABLE tbl_name WAIT N add column ... 

### 小结 ###

介绍了 MySQL 的全局锁和表级锁

全局锁主要用在逻辑备份过程中。对于全部是 InnoDB 引擎的库，我建议你选择使用–single-transaction 参数。

表锁一般是在数据库引擎不支持行锁的时候才会被用到的。lock tables

* 要么是你的系统现在还在用 MyISAM 这类不支持事务的引擎，那要安排升级换引擎；
* lock tables 和 unlock tables 改成 begin 和 commit

MDL 会直到事务提交才释放，在做表结构变更的时候，你一定要小心不要导致锁住线上查询和更新。

## 07 | 行锁功过：怎么减少行锁对性能的影响？ ##

MySQL 的行锁是在引擎层由各个引擎自己实现的。但并不是所有的引擎都支持行锁，比如 MyISAM 引擎就不支持行锁。不支持行锁意味着并发控制只能使用表锁，对于这种引擎的表，同一张表上任何时刻只能有一个更新在执行，这就会影响到业务并发度。InnoDB 是支持行锁的，这也是 MyISAM 被 InnoDB 替代的重要原因之一。

InnoDB 的行锁，以及如何通过减少锁冲突来提升业务并发度。行锁就是针对数据表中行记录的锁。比如事务 A 更新了一行，而这时候事务 B 也要更新同一行，则必须等事务 A 的操作完成后才能进行更新。

### 死锁和死锁检测 ###

当并发系统中不同线程出现循环资源依赖，涉及的线程都在等待别的线程释放资源时，就会导致这几个线程都进入无限等待的状态，称为死锁。

在InnoDB事务中，行锁是在需要的时候才加上的，但并不是不需要就立刻释放，而是要等到事务结束才释放。这个就是两阶段锁协议。所以*如果事务中需要锁多个行，要把最可能造成锁冲突、最可能影响并发度的锁尽量往后放*。

### 死锁和死锁检测 ###

当并发系统中不同线程出现循环资源依赖，涉及的线程在等待别的线程释放资源时，就会导致这几个线程都进入无限等待的状态，称为死锁。

1. 直接进入等待，直到超时。这个超时时间可以通过参数 innodb_lock_wait_timeout 来设置。
2. 发起死锁检测，发现死锁后，主动回滚死锁链条中的某一个事务，让其他事务得以继续执行。将参数 innodb_deadlock_detect 设置为 on，表示开启这个逻辑。

在InnoDB中，innodb_lock_wait_timeout的默认值是50s（如果采用第一个策略，当出现死锁以后，第一个被锁住的线程要过 50s 才会超时退出，然后其他线程才有可能继续执行）——对于在线服务来说，这个等待时间往往是无法接受的。

### 小结 ###

MySQL的行锁，涉及了两阶段锁协议、死锁和死锁检测这两大部分内容。

1. 两阶段协议为起点
2. 如果事务中需要多个行，把最可能造成锁冲突、最可能影响并发度的锁的申请时机尽量往后放
3. 调整语句顺序并不能完全避免死锁
	* 死锁
	* 死锁的检测

## 08 | 事务到底是隔离的还是不隔离的？ ##

	mysql> CREATE TABLE `t08` (
	  `id` int(11) NOT NULL,
	  `k` int(11) DEFAULT NULL,
	  PRIMARY KEY (`id`)
	) ENGINE=InnoDB;
	insert into t08(id, k) values(1,1),(2,2);

在MySQL里，有两个“视图”的概念：

* view，是一个用查询语句定义的虚拟表，在调用的时候执行查询语句并生成结果。创建视图的语法是`create view...`，而它的查询方法与表一样。
* InnoDB在实现MVCC时用到的一致性读视图，即consitent read view，用于支持RC（Read Committed，读操作）和RR（Repeatable Read，可重复读）隔离级别的实现。

没有物理结构，作用的是事务执行期间用来定义“我能看到什么数据”。

### “快照”在MVCC里怎么工作的？ ###

在可重复读隔离级别下，事务在启动的时候就“拍了个快照”。注意，这个快照是基于整库的。

* transaction id，InnoDB 里面每个事务有一个唯一的事务 ID，在事务开始的时候向 InnoDB 的事务系统申请的，是按申请顺序严格递增的。
* row trx_id，每行数据也都是有很多版本的。每次事务更新数据的时候，都会生成一个新的数据版本；旧的数据版本要保留，并且在新的数据版本中，能够有信息可以直接拿到；数据表中的一行记录，其实可能有多个版本（row），每个版本有自己的row_trx_id。

#### undo log在哪呢？ ####

对于当前事务的启动瞬间来说，一个数据版本的row_trx_id，有以下几种可能：

1. 已提交的事务或者当前的事务自己生成的，这个数据是可见的
2. 将来启动的事务生成的，是肯定不可兼得
3. 未提交事务集合
	* 若 row trx_id 在数组中，表示这个版本是由还没提交的事务生成的，不可见；
	* 若 row trx_id 不在数组中，表示这个版本是已经提交了的事务生成的，可见。 

*InnoDB 利用了“所有数据都有多个版本”的这个特性，实现了“秒级创建快照”的能力。*

InnnoDB为每个事务构造了一个数组，用来保存这个事务启动瞬间，当前正在“活跃”的所有事务ID。“活跃”指的就是，启动了但还没提交。数组里面事务ID的最小值记为低水位，当前系统里面已经创建过的事务ID的最大值加1记为高水位。

### 更新逻辑 ###

*事务 B 的 update 语句，如果按照一致性读，好像结果不对哦？*

*更新数据都是先读后写的，而这个读，只能读当前的值，称为“当前读”（current read）。*

读提交的逻辑和可重复读的逻辑类似，最主要的区别是：

1. 在可重复读隔离级别下，只需要在事务开始的时候创建一致性视图，之后事务里的其他查询都共用这个一致性视图；
2. 在读提交隔离级别下，每一个语句执行前都会重新算出一个新的视图。

### 小结 ###

InnoDB 的行数据有多个版本，每个数据版本有自己的 row trx_id，每个事务或者语句有自己的一致性视图。普通查询语句是一致性读，一致性读会根据 row trx_id 和一致性视图确定数据版本的可见性。

* 对于可重复读，查询只承认在事务启动前就已经提交完成的数据；
* 对于读提交，查询只承认在语句启动前就已经提交完成的数据；

### 精选留言 ###

#### Q ####

下面的表结构和初始化语句作为试验环境，事务隔离级别是可重复读。现在，要把所有“字段c和id值相等的行”的c值清零，但是却发现了一个 “诡异”的、改不掉的情况

	mysql> CREATE TABLE `t08_home` (
	  `id` int(11) NOT NULL,
	  `c` int(11) DEFAULT NULL,
	  PRIMARY KEY (`id`)
	) ENGINE=InnoDB;
	insert into t08_home(id, c) values(1,1),(2,2),(3,3),(4,4);

1. 使用sessionA和sessionB
2. 通过sessionA进行线程更新，通过使用线程A进行更新数据为0，并使用sleep
3. 通过线程B进行数据检索
4. 因为事务隔离级别是可重复读，当他sessionA使用更新，会创建view，而sessionB是不可见的，所以sessionB检索时只能查到1，2，3，4原始值，而不是0，0，0，0

#### A ####

思考题，RR下，用另外一个事物在update执行之前，先把所有c值修改，应该就可以。比如update t set c = id + 1。
这个实际场景还挺常见——所谓的“乐观锁”。时常我们会基于version字段对row进行cas式的更新，类似update ...set ... where id = xxx and version = xxx。如果version被其他事务抢先更新，则在自己事务中更新失败，trx_id没有变成自身事务的id，同一个事务中再次select还是旧值，就会出现“明明值没变可就是更新不了”的“异象”（anomaly）。解决方案就是每次cas更新不管成功失败，结束当前事务。如果失败则重新起一个事务进行查询更新。

#### Q ####

请教一个问题，业务上有这样的需求，A、B两个用户，如果互相喜欢，则成为好友。设计上是有两张表，一个是like表，一个是friend表，like表有user_id、liker_id两个字段，我设置为复合唯一索引即uk_user_id_liker_id。语句执行顺序是这样的：
以A喜欢B为例：
1、先查询对方有没有喜欢自己（B有没有喜欢A）
select * from like where user_id = B and liker_id = A
2、如果有，则成为好友
insert into friend
3、没有，则只是喜欢关系
insert into like

如果A、B同时喜欢对方，会出现不会成为好友的问题。因为上面第1步，双方都没喜欢对方。第1步即使使用了排他锁也不行，因为记录不存在，行锁无法生效。请问这种情况，在mysql锁层面有没有办法处理

#### A ####



# 实践篇 #

## 09 | 普通索引和唯一索引，应该怎么选择？ ##

	select name from CUser where id_card = 'xxxxxxxyyyyyyzzzzz';

id_card字段比较大，不建议当主键，可以有两个选择，要么给id_card字段创建唯一索引，要么创建普通索引。

### 查询过程 ###

查询语句在索引树上查找的过程

1. 通过 B+ 树从树根开始
2. 按层搜索到叶子节点
3. 数据页通过二分法定位记录

### 更新过程 ###

当需要更新一个数据页时，如果数据页在内存中就直接更新，而如果这个数据页还没有在内存中的话，在不影响数据一致性的前提下，InooDB 会将这些更新操作缓存在 change buffer 中，这样就不需要从磁盘中读入这个数据页了。在下次查询需要访问这个数据页的时候，将数据页读入内存，然后执行 change buffer 中与这个页有关的操作。通过这种方式就能保证这个数据逻辑的正确性。

change buffer：可以持久化的数据，在内存中有拷贝，也会被写入到磁盘上。

将change buffer中的操作应用到原数据页，得到最新结果的过程称为merge。除了访问这个数据页会触发merge外，系统有后台线程会定期merge。在数据库正常关闭（shutdown）的过程中，也会执行 merge 操作。

将更新操作先记录在 change buffer，减少读磁盘，语句的执行速度会得到明显的提升。而且，数据读入内存是需要占用 buffer pool 的，所以这种方式还能够避免占用内存，提高内存利用率。

*什么条件下可以使用 change buffer 呢？*

一索引的更新就不能使用 change buffer，实际上也只有普通索引可以使用。

1. 记录要在更新的目标页在内存中
	* 对于唯一索引来说，找到 3 和 5 之间的位置，判断到没有冲突，插入这个值，语句执行结束；
	* 对于普通索引来说，找到 3 和 5 之间的位置，插入这个值，语句执行结束。
	* 普通索引和唯一索引对更新语句性能影响的差别，只是一个判断，只会耗费微小的 CPU 时间。
2. 记录要更新的目标页不在内存中
	* 对于唯一索引来说，需要将数据页读入内存，判断到没有冲突，插入这个值，语句执行结束；
	* 对于普通索引来说，则是将更新记录在 change buffer，语句执行就结束了。

将数据从磁盘读入内存涉及随机 IO 的访问，是数据库里面成本最高的操作之一。change buffer 因为减少了随机磁盘访问，所以对更新性能的提升是会很明显的。

#### change buffer 的使用场景 ####

使用 change buffer 对更新过程的加速作用，也清楚了 change buffer 只限于用在普通索引的场景下，而不适用于唯一索引。

对于写多读少的业务来说，页面在写完以后马上被访问到的概率比较小，此时 change buffer 的使用效果最好。

#### 索引选择和实践 ####

普通索引和唯一索引应该怎么选择。这两类索引在查询能力上是没差别的，主要考虑的是对更新性能的影响。尽量选择普通索引。

如果所有的更新后面，都马上伴随着对这个记录的查询，那么你应该关闭 change buffer。而在其他情况下，change buffer 都能提升更新性能。

普通索引和 change buffer 的配合使用，对于数据量大的表的更新优化还是很明显的。

### change buffer 和 redo log ###

* redo log 主要节省的是随机写磁盘的 IO 消耗（转成顺序写）
* change buffer 主要节省的则是随机读磁盘的 IO 消耗。

### 小结 ###

1. 从普通索引和唯一索引的选择开始
2. 分享了数据的查询和更新过程
3. change buffer 的机制以及应用场景
4. 索引选择的实践

由于唯一索引用不上 change buffer 的优化机制，因此如果业务可以接受，从性能角度出发我建议你优先考虑非唯一索引。最后，又到了思考题时间。

## 10 | MySQL为什么有时候会选错索引？ ##

表定义：

	CREATE TABLE `t10` (
	  `id` int(11) NOT NULL,
	  `a` int(11) DEFAULT NULL,
	  `b` int(11) DEFAULT NULL,
	  PRIMARY KEY (`id`),
	  KEY `a` (`a`),
	  KEY `b` (`b`)
	) ENGINE=InnoDB;

存储过程定义：

	delimiter ;;
	create procedure idata()
	begin
	  declare i int;
	  set i=1;
	  while(i<=100000)do
	    insert into t values(i, i, i);
	    set i=i+1;
	  end while;
	end;;
	delimiter ;
	call idata();

SQL语句：

	mysql> select * from t10 where a between 10000 and 20000;

explain命令看到的这条语句的执行情况：

	mysql> explain select * from t10 where a between 10000 and 20000;

key这个字段值是'a'，表示优化器选择了索引a。

如果准备好的包含了10万行

### 优化器的逻辑 ###

选择索引是优化器的工作。而优化器选择索引的目的，是找到一个最优的执行方案，并用最小的代价去执行语句。在数据库里面，扫描行数是影响执行代价的因素之一。扫描的行数越少，意味着访问磁盘数据的次数越少，消耗的 CPU 资源越少。扫描行数也不是唯一的判断标准，优化器还会结合是否使用临时表、是否排序等因素进行综合判断。

MySQL选错索引在判断扫描行数的时候出了问题，*扫描行数是怎么判断的？*

基数：一个索引上不同的值，这个索引的区分度就越好。而一个索引上不同的值的个数。基数越大，索引的区分度越好。

使用`show index`方法，看到索引的基数。

### 索引选择异常和处理 ###

1. 采用 force index 强行选择一个索引。MySQL会根据语法解析的结果分析可能使用的索引作为候选项，然后在候选列表中依次判断每个索引需要扫面多少行
2. 修改语句，引导MySQL使用期望的索引
3. 在有些场景下，我们可以新建一个更合适的索引，来提供给优化器做选择，或删掉误用的索引

### 小结 ###

索引统计的更新机制，优化器存在选错索引的可能性。


## 11 | 怎么给字符串字段加索引？ ##

### 前缀索引对覆盖索引的影响 ###


## 12 | 为什么我的MySQL会“抖”一下？ ##

*你的SQL语句为什么变“慢”了*

当内存数据页跟磁盘数据页内容不一致的时候，我们称这个内存页为“脏页”。内存数据写入到磁盘后，内存和磁盘上的数据页的内容就一致了，称为“干净页”。

### InnoDB 刷脏页的控制策略 ###

### 小结 ###

解释了这个机制后续需要的刷脏页操作和执行时机。利用 WAL 技术，数据库将随机写转换成了顺序写，大大提升了数据库的性能。

由此也带来了内存脏页的问题。脏页会被后台线程自动 flush，也会由于数据页淘汰而触发 flush，而刷脏页的过程由于会占用资源，可能会让你的更新和查询语句的响应时间长一些。



## 13 | 为什么表数据删掉一半，表文件大小没变 ##

数据库表的空间回收

一个 InnoDB 表包含两部分，即：表结构定义和数据。在 MySQL 8.0 版本以前，表结构是存在以.frm 为后缀的文件里。而 MySQL 8.0 版本，则已经允许把表结构定义放在系统数据表中了。因为表结构定义占用的空间很小。

### 参数innodb_file_per_table ###

数据既可以存在共享表空间里，也可以是单独的文件。这个行为是由参数 innodb_file_per_table 控制的：

1. 这个参数设置为 OFF 表示的是，表的数据放在系统共享表空间，也就是跟数据字典放在一起；即使表删掉了，空间也是不会回收的
2. 这个参数设置为 ON 表示的是，每个 InnoDB 表数据存储在一个以 .ibd 为后缀的文件中；通过drop table命令，系统就会直接删除这个文件。

从MySQL5.6.6版本开始，它的默认值就是ON了。

*将 innodb_file_per_table 设置为 ON，是推荐做法，我们接下来的讨论都是基于这个设置展开的。*

删除表的时候，可以使用drop table命令来回收空间；但删除数据的场景往往是删除某些行（表空间却没有被回收）

### 数据删除流程 ###

InnoDB 里的数据都是用 B+ 树的结构组织的。

## 14 | count(*)这么慢，我该怎么办？ ##

### count(*)的实现方式 ###

在不同的MySQL引擎中，count(*)有不同的实现方式。

* MyISAM引擎把一个表的总行数存在了磁盘上，因此执行count(*)的时候会直接返回这个数，效率很高；
* 而InnoDB引擎执行count(*)的时候，需要把数据一行一行地从引擎里面读出来，然后累积计数。

事务支持、并发能力还是在数据安全方面，InnoDB都优于MyISAM

#### 为什么InnoDB不跟MyISAM一样，也把数字存起来呢？ ####

因为即使在同一时刻的多个查询，由于多版本并发控制（MVCC）的原因，InnoDB表“应该返回多少行”也是不确定的。

假如设计三个用户并行的会话，可能出现拿到的结果却不同。因为InnoDB的事务设计有关系，可重复读是默认的隔离级别。每一行记录都要判断自己是否对这个会话可见，因此对于count(*)请求来说，InnoDB只好把数据一行一行读出来依次判断，可见的行才能够用于计算“基于这个查询”的表的总行数

MySQL在执行count(*)操作的时候做了优化。InnoDB是索引组织表，主键索引树的叶子节点是数据，而普通索引树比主键索引树少很多。对于count(*)这样的操作，遍历哪个索引数得到的结果逻辑上都是一样的。MySQL优化器会找到最小的那棵树来遍历。在保证逻辑正确的前提下，尽量减少扫描的数据量，是数据库系统设计的通用法则之一。

show table status，输出结果里面有一个TABLE_ROWS用于显示这个表当前有多少行。（show table status命令显示的行数也不能直接使用）

### 小结 ###

* MyISAM表虽然count(*)很快，但是不支持事务；
* show table status 命令虽然返回很快，但是不准确；
* InnoDB 表直接 count(*) 会遍历全表，虽然结果准确，但会导致性能问题。

### 用缓存系统保存计数 ###

将计数保存在缓存系统中的方式，还不只是丢失更新的问题。即使Redis正常工作，这个值还是逻辑上不精确的。

1. 缓存可能会丢失更新，
2. 逻辑上值不精确

### 在数据库保存计数 ###

把计数直接放到数据库单独里的一张计数表C中

1. 解决了崩溃丢失的问题，InnoDB是支持崩溃恢复不丢数据的
2. 利用InnoDB的事务特性解决查计数值和“最近100条记录”看到的结果

### 不同的count用法 ###

count(*)、count(主键 id)、count(字段) 和 count(1) 等不同用法的性能

1. count()是一个聚合函数，对于返回的结果集，一行行地判断，如果count函数的参数不是NULL，累计值就加1，否则不加。最后返回累计值。

至于分析性能差别，可以记住这几个原则：

1. server层要什么就给什么；
2. InnoDB 只给必要的值；
3. 现在的优化器只优化了 count(*) 的语义为“取行数”，其他“显而易见”的优化并没有做。

---

* 对于 count(主键 id) 来说：
* 对于 count(1) 来说：
* 对于 count(字段) 来说：
	1. 如果这个“字段”是定义为not null的话，一行行地从记录里面读出这个字段，判断不能为null，按行累加；
	2. 如果这个“字段”定义为null，那么执行的时候，判断到有可能是null，还要把值取出来再判断一下，不是null才累加。
* 但是 count(*) 是例外：不会把全部字段取出来，专门做了优化，不取值。count(*)肯定不是null，按行累加。

按照效率排序的话，count(字段)<count(主键 id)<count(1)≈count(*)，所以我建议你，尽量使用 count(*)。

### 小结 ###

1. 不同引擎中count(*)的实现方式不一样
2. 用缓存系统来存储计数值存在问题——这两个不同的存储构成的系统，不支持分布式事务，无法拿到精确一致的视图
3. 把计数值放在MySQL中，解决一致性视图问题

InnoDB引擎支持事务，利用好事务的原子性和隔离性，简化在业务开发时的逻辑。


## 15 | 答疑文章（一）：日志和索引相关问题 ##

## 16 | "order by"是怎么工作的？ ##

表定义

	CREATE TABLE `t` (
	  `id` int(11) NOT NULL,
	  `city` varchar(16) NOT NULL,
	  `name` varchar(16) NOT NULL,
	  `age` int(11) NOT NULL,
	  `addr` varchar(128) DEFAULT NULL,
	  PRIMARY KEY (`id`),
	  KEY `city` (`city`)
	) ENGINE=InnoDB;


	select city,name,age from t where city='杭州' order by name limit 1000 ;

### 全字段排序 ###

为避免全表扫描，需要再city字段加上索引。在city字段上创建索引之后，用explain命令来看看这个语句的执行情况。

1. 初始化sort_buffer，确定放入name、city、age这三个字段；
2. 从索引city找到第一个满足city='杭州'条件的主键id，也就是图中的ID_X；
3. 到主键id索引取出整行，取name、city、age这三个字段的值，存入sort_buffer中；
4. 从索引city取下一个记录的主键id；
5. 重复步骤 3、4 直到 city 的值不满足查询条件为止，对应的主键 id 也就是图中的 ID_Y；
6. 对 sort_buffer 中的数据按照字段 name 做快速排序；
7. 按照排序结果取前 1000 行返回给客户端。

sort_buffer_size，就是MySQL为排序开辟的内存（sort_buffer）的大小。如果要排序的数据量小于sort_buffer_size，排序就在内存中完成。但如果排序数据量太大，内存放不下，则不得不利用磁盘临时文件辅助排序。

MySQL将需要排序的数据分成12份，每一份单独排序后存在这些临时文件种。然后把12个有序文件再合并成一个有序的大文件。

OPTIMIZER_TRACE 

* filesort_summary
	* rows:
	* examined_rows：表示参与排序的的行数
	* number_of_tmp_files
	* sort_buffer_size
		* sort_buffer_size超过了需要排序的数据量的大小，number_of_tmp_files就是0，表示排序可以直接在内存中完成
		* 否则需要放在临时文件加中排序。sort_buffer_size 越小，需要分成的份数越多，number_of_tmp_files 的值就越大
	* sort_mode 

### rowid排序 ###

如果MySQL认为排序的单行长度太长会怎么做呢？

	SET max_length_for_sort_data = 16;

max_length_for_sort_data，是MySQL中专门控制用于排序的行数据的长度的一个参数。

1. 初始化sort_buffer，确定放入name和id；
2. 从索引city找到第一个满足city='杭州'条件的主键id，也就是图中的ID_X；
3. 到主键id索引取出整行，取name、id这两个字段的值，存入sort_buffer中；
4. 从索引city取下一个记录的主键id；
5. 重复步骤 3、4 直到 city='杭州' 条件为止，也就是 ID_Y；
6. 对 sort_buffer 中的数据按照字段 name 做快速排序；
7. 按照排序结果取前 1000 行返回给客户端。

### 全字段排序 VS rowid 排序 ###

1. 如果MySQL担心排序内存太小，会影响排序效率，才会采用rowid排序算法。这样排序过程一次可以排序更多行，但是需要再回到原表里取数据。
2. 如果MySQL认为内存足够大，会优先选择全字段排序，把需要的字段都放到sort_buffer中，这样排序后会直接从内存里面返回查询结果了，不用再回到原表中去取数据。

如果内存够，就要多利用内存，尽量减少磁盘访问。对于InnoDB表来说，rowid排序会要求回表多造成磁盘读，因此不会被优先选择。

覆盖索引是指，索引上的信息足够满足查询请求，不需要再回到主键索引上去取数据。

每个查询都用上覆盖索引，就要把语句中涉及的字段都建上联合索引，毕竟索引还是有维护代价。需要权衡决定。

## 17 | 如何正确地显示随机消息 ##

### 内存临时表 ###

	mysql> select word from words order by rand() limit 3;

Extra 字段显示 Using temporary，表示的是需要使用临时表；Using filesort，表示的是需要执行排序操作。(需要临时表，并且需要在临时表上排序)

对于 InnoDB 表来说，执行全字段排序会减少磁盘访问，因此会被优先选择；对于内存表，回表过程只是简单地根据数据行的位置，直接访问内存得到数据，根本不会导致多访问磁盘。

1. 创建一个临时表。这个临时表使用的是memory引擎，表里有两个字段

### 磁盘临时表 ###

## 18 | 为什么这些SQL语句逻辑相同，性能却差异巨大？ ##

## 19 | 为什么我只查一行的语句，也执行这么慢？ ##

## 20 | 幻读是什么，幻读有什么问题？ ##

## 21 | 为什么我只改一行的语句锁这么多 ##

我总结的加锁规则里面，包含了两个“原则”、两个“优化”和一个“bug”。

1. 原则1：加锁的基本单位是next-key lock。
2. 原则2：查找过程中访问到的对象才会加锁。
3. 优化1：索引上的等值查询，给唯一索引加锁的时候，next-key lock退化为行锁。
4. 优化2：索引上的等值查询，向右遍历时且最后一个值不满足等值条件

## 22 | MySQL有哪些“饮鸩止渴”提高性能的方法？ ##

## 24 | MySQL有哪些“饮鸩止渴”提高性能的方法？ ##

binlog可以用来归档，也可以用来主备同步。

### MySQL主备的基本原理 ###

在状态 1 中，客户端的读写都直接访问节点 A，而节点 B 是 A 的备库，只是将 A 的更新都同步过来，到本地执行。这样可以保持节点 B 和 A 的数据是相同的。

当需要切换的时候，就切成状态 2。这时候客户端读写访问的都是节点 B，而节点 A 是 B 的备库。

在状态 1 中，虽然节点 B 没有被直接访问，但是我依然建议你把节点 B（也就是备库）设置成只读（readonly）模式。这样做，有以下几个考虑：

1. 有时候一些运营类的查询语句会被放到备库上去查，设置为只读可以防止误操作；
2. 防止切换逻辑有 bug，比如切换过程中出现双写，造成主备不一致；
3. 可以用 readonly 状态，来判断节点的角色。

### binlog的三种格式对比 ###

### 为什么会右mixed格式的binlog？ ###

### 循环复制问题 ###

### 小结 ###

介绍了MySQL binlog的格式和一些基本机制，时后面要介绍读写分离等系列文章的背景知识

MySQL高可用方案的基础，演化出了诸如多节点、半同步、MySQL group replication等相对复杂的方案。

### 精选留言 ###

我们说 MySQL 通过判断 server id 的方式，断掉死循环。但是，这个机制其实并不完备，在某些场景下，还是有可能出现死循环。

## 25 | MySQL时怎么保证高可用的？ ##

## 26 | 备库为什么会延迟好几个小时？ ##

## 27 | 主库出问题了，从库怎么办？ ##

## 28 | 读写分离有哪些坑？ ##

## 29 | 如何判断一个数据库是不是出问题了？ ##

在一主一备的双M架构里，主备切换只需要客户端流量切到备库；而在一主多从架构里，主备且换除了要把客户端流量切换到备库外，还需要把从库接到新主库上。

## 30 | 答疑文章（二）：用动态的观点看加锁 ##

## 31 | 误删数据后除了跑路，还能怎么办？ ##

## 32 | 为什么还有kill不掉的语句？ ##

## 33 | 我查这么多数据，会不会数据库内存打爆？ ##

### 全表扫描对server层的影响 ###

	mysql -h$host -P$port -u$user -p$pwd -e "select * from db1.t" > $target_file

InnoDB的数据是保存在主键索引上的，所以全表扫描实际上是直接扫描表t的主键索引。

服务端不需要保存一个完整的结果集。取数据和发数据的流程是：

1. 获取一行，写到net_buffer中，这个内存的大小是由参数net_buffer_length定义的，默认是16k。
2. 重复获取行，直到net_buffer写满，调用网络接口发出去。
3. 如果发送成功，就清空net_buffer，然后继续取下一行，并写入net_buffer
4. 如果发送函数返回 EAGAIN 或 WSAEWOULDBLOCK，就表示本地网络栈（socket send buffer）写满了，进入等待。直到网络栈重新可写，再继续发送。

可以看出

1. 一个查询在发送过程中，占用的MySQL内部的内存最大就是net_buffer_length这么大，并不会达到200G；
2. socket send buffer也不可能达到200G（默认定义 /proc/sys/net/core/wmem_default），如果socket send buffer被写满，就会暂停读数据的流程。

MySQL是“边读变发的”。如果客户端接收的慢，会导致MySQL服务端由于结果发布出去，这个事务的执行时间变长。

	show processlist;

对于正常的线上业务来说，如果一个查询的返回结果不会很多的话，使用mysql_store_result这个接口，直接把查询结果保存到本地内存。

### 全表扫描对InnoDB的影响 ###

分析了 InnoDB 内存的一个作用，是保存更新的结果，再配合 redo log，就避免了随机写盘。

内存的数据页是在Buffer Pool(BP)中管理的，在WAL里Buffer Pool 

### 小结 ###

由于 MySQL 采用的是边算边发的逻辑，因此对于数据量很大的查询结果来说，不会在 server 端保存完整的结果集。所以，如果客户端读结果不及时，会堵住 MySQL 的查询过程，但是不会把内存打爆。

对于 InnoDB 引擎内部，由于有淘汰策略，大查询也不会导致内存暴涨。并且，由于 InnoDB 对 LRU 算法做了改进，冷数据的全表扫描，对 Buffer Pool 的影响也能做到可控。

全表扫描还是比较耗费 IO 资源的，所以业务高峰期还是不能直接在线上主库执行全表扫描的。

## 34 | 到底可不可以使用join？ ##

## 35 | join语句怎么优化？ ##

## 36 | 为什么临时表可以重名？ ##

## 37 | 什么时候会使用内部临时表？ ##

## 38 | 都说InnoDB好，还要不要使用Memory引擎 ##

## 39 | 自增主键为什么不是连续的？ ##

	CREATE TABLE `t39` (
	  `id` int(11) NOT NULL AUTO_INCREMENT,
	  `c` int(11) DEFAULT NULL,
	  `d` int(11) DEFAULT NULL,
	  PRIMARY KEY (`id`),
	  UNIQUE KEY `c` (`c`)
	) ENGINE=InnoDB;

### 自增值保存在哪儿？ ###

`insert into t39 values(null, 1, 1);`插入一行数据
`show create table t39`看到表定义里面出现了AUTO_INCREMENT=2，表示下一次插入数据时，如果需要自动生成自增值，会生成id=2

表的结构定义存放在后缀名为.frm 的文件中，但是并不会保存自增值。不同的引擎对于自增值的保存策略不同：

* MyISAM引擎的自增值保存在数据文件中
* InnoDB引擎的自增值，其实时保存在了内存里，并且到了MySQL8.0版本后，才有了“自增值持久化”的能力，也就是才实现了“如果发生重启，表的自增值可以恢复为MySQL重启前的值”，具体情况是：
	* 在 MySQL 5.7 及之前的版本，自增值保存在内存里，并没有持久化。每次重启后，第一次打开表的时候，都会去找自增值的最大值 max(id)，然后将 max(id)+1 作为这个表当前的自增值。﻿
	* 在 MySQL 8.0 版本，将自增值的变更记录在了 redo log 中，重启的时候依靠 redo log 恢复重启之前的值。

### 自增值修改机制 ###

在MySQL里面，如果字段id被定义为AUTO_INCREMENT，在插入一行数据的时候，自增值的行为如下：



### 自增值的修改时机 ###

### 自增锁的优化 ###

### 小结 ###

“自增主键为什么会出现不连续的值”，这个问题开始，讨论了自增值的存储。

* 在 MyISAM 引擎里面，自增值是被写在数据文件上的。
* 在 InnoDB 中，自增值是被记录在内存的。
	* MySQL 直到 8.0 版本，才给 InnoDB 表的自增值加上了持久化的能力，确保重启前后一个表的自增值不变。

## 40 | insert语句的锁为什么这么多？ ##

## 41 | 怎么最快地复制一张表 ##

## 42 | grant之后要跟着flush privileges吗 ##

## 43 | 要不要使用分区表？ ##

## 44 | 答疑文章（三）：说一说这些好问题 ##

## 45 | 自增id用完怎么办？ ##


## 直播回顾 | 林晓斌：我的 MySQL 心路历程 ##

## 结束语 | 点线网面，一起构建MySQL知识网络 ##

### 1. 路径千万条，实践第一条 ###

* 跟着专栏中的案例做实验，继续坚持下去。在阅读其他技术文章、图书的时候，也是同样的道理。如果你觉得自己理解了一个知识点，也一定要尝试设计一个例子来验证它。
* 在设计案例的时候，我建议你也设计一个对照的反例，从而达到知识融汇贯通的目的。

### 2. 原理说不清，双手白费劲 ###

1. 先实践再搞清楚原理
2. 先明白原理再通过实践去验证

怎么证明是是不是真的把原理弄清楚了呢？——说出来、写出来

如果有人请教你这个问题：

1. 验证自己搞懂了这个知识点
2. 提升自己的技术表达能力

“写出来”又是一个更高的境界。因为，你在写的过程中，就会发现这个“明白”很可能只是一个假象。所以，在专栏下面写下自己对本章知识点的理解，也是一个不错的夯实学习成果的方法。

### 3. 知识没体系，转身就忘记 ###

知识点“写下来”，还有一个好处，就是你会发现这个知识点的关联知识点。深究下去，点就连成线，然后再跟别的线找交叉。

### 4. 手册补全面，案例扫盲点 ###

一开始就看手册？看手册的时机，应该是知识网络构建得差不多的时候。

“差不多”的标准：

* 能否解释清楚错误日志（error log）、慢查询日志（slow log）中每一行的意思？
* 能否快速评估出一个表结构或者一条 SQL 语句，设计得是否合理？
* 能否通过 explain 的结果，来“脑补”整个执行过程（我们已经在专栏中练习几次了）？
* 到网络上找 MySQL 的实践建议，对于每一条做一次分析：
	* 如果觉得不合理，能否给出自己的意见？
	* 如果觉得合理，能否给出自己的解释？
	* 找有经验的人讨论

# 特别放送 #

## MySQL中6个常见的日志问题 ##

MySQL里有两个日志，即：重做日志（redo log）和归档日志（binlog）。

* binlog可以给

### 问题 ###

问题1：MySQL怎么知道binlog是完整的？

回答：一个事务的binlog是有完整格式的：

* statement格式的binlog，最后会有COMMIT；
* row格式的binlog，最后会有一个XID event。

在MySQL5.6.2版本以后，还引入了binlog-checksum参数，用来验证binlog内容的正确性。对于binlog日志由于磁盘原因，可能会在日志中间出错的情况，MySQl可以通过校验checksum的结果来发现。所以，MySQL还是有版本验证事务binlog的完整性的。

问题2：redo log和binlog是怎么关联起来的？

回答：他们有共同的数据字段，叫XID。崩溃恢复的时候，会按顺序扫描redo log：

* 如果碰到既有prepare，又有commit的redo log，就直接提交。
* 如果碰到只有prepare，而没有commit的redo log，就拿着XID去binlog找对应的事务。

问题3：处于prepare阶段的redo log加上完整binlog，重启就能恢复，MySQL为什么要这么设计？

问题4：如果这样的话，为什么还要两阶段提交呢？干脆先redo log写完，再写binlog。崩溃恢复的时候，必须得两个日志都完整才可以。是不是一样得逻辑？

答：两阶段提交是经典的分布式系统问题，并不是MySQL独有的。

两阶段提交就是为了给所有人一个几回，当每个人都说“我ok”的时候，再一起提交。

问题5：不引入两个日志，也就没有两阶段提交的必要了。只用binlog来支持崩溃恢复，又能支持归档，不就可以了？

答：binlog没有能力恢复“数据页”

问题6：那能不能反过来，只用redo log，不要binlog？

# 直播回顾 #

## 直播回顾 | 林晓斌：我的 MySQL 心路历程 ##

