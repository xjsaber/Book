# MySQL实战45讲 #

# 开篇词 #

## 开篇词 | 这一次，让我们一起来搞懂MySQL ##

平时使用数据库时高频出现的知识，如事务、索引、锁等内容构成专栏的主线。

点->线->面，形成自己的MySQL知识网络

# 基础篇 #

## 01 | 基础架构：一条SQL查询语句是如何执行的？ ##

mysql> select * from T where ID = 10;

![0d2070e8f84c4801adbfa03bda1f98d9](img/0d2070e8f84c4801adbfa03bda1f98d9.png)

MySQL分为Server层和存储引擎层两部分。

Server层包括连接器、查询缓存、分析器、优化器、执行器等，涵盖MySQL的大部分核心服务功能，以及所有的内置函数（如日期、时间、数学和加密函数等），所有跨存储引擎的功能都在这一层实现，比如存储过程、触发器、视图等。

存储引擎层负责数据的存储和提取。其架构模式是插件式的，支持 InnoDB、MyISAM、Memory 等多个存储引擎。现在最常用的存储引擎是 InnoDB，它从 MySQL 5.5.5 版本开始成为了默认存储引擎。

不同的存储引擎共用一个Server层，也就是从连接器到执行器的部分。

### 连接器 ###

1. 先连接到这个数据库上，这时候接待你的就是连接器。连接器负责跟客户端建立连接、获取权限、维持和管理连接。`mysql -h$ip -P$port -u$user -p`（连接命令中的 mysql 是客户端工具，用来跟服务端建立连接。在完成经典的 TCP 握手后，连接器就要开始认证你的身份，这个时候用的就是你输入的用户名和密码。）

建立连接的过程通常比较复杂的，所以在使用中要尽量减少建立连接的动作，也尽量使用长连接。但全部使用长连接后，有时候MySQL占用内存涨得特别快，因为MySQL在执行过程中临时使用的内存是管理在连接对象里面的。=>内存占用太大，被系统强行杀掉（OOM）=>MySQL异常重启

解决方案：

1. 定期断开长连接。使用一段时间后，或者程序里面判断执行过一个占用内存的大查询后，断开连接，之后要查询再重连。
2. 如果是 MySQL 5.7 或更新版本，可以在每次执行一个比较大的操作后，通过执行 mysql_reset_connection 来重新初始化连接资源。这个过程不需要重连和重新做权限验证，但是会将连接恢复到刚刚创建完时的状态。

    wait_timeout默认值是8小时

### 查询缓存 ###

*但是大多数情况下我会建议你不要使用查询缓存，为什么呢？因为查询缓存往往弊大于利。*

查询缓存的失效非常频繁，只要有对一个表的更新，这个表上所有的查询缓存都会被清空。除非你的业务就是有一张静态表，很长时间才会更新一次。比如，一个系统配置表，那这张表上的查询才适合使用查询缓存。

8.0以后没有这块功能。

### 分析器 ###

如果没有命中查询缓存

1. MySQL需要对SQL语句做解析
2. 词法分析， `select * from T where ID = 10;` 比如select，识别出来是一个查询语句，把字符串“T”识别成“表名T”，把字符串“ID”识别成“列ID”。
3. 语法分析，语法分析器会根据语法判断，判断是否满足MySQL语法。

### 优化器 ###

优化器是在表里面有多个索引的时候，决定使用哪个索引；或者在一个语句有多表关联（join）的时候，决定各个表的连接顺序。比如你执行下面这样的语句，这个语句是执行两个表的 join：

	mysql> select * from t1 join t2 using(ID) where t1.c=10 and t2.d=20;

* 既可以先从表 t1 里面取出 c=10 的记录的 ID 值，再根据 ID 值关联到表 t2，再判断 t2 里面 d 的值是否等于 20。
* 也可以先从表 t2 里面取出 d=20 的记录的 ID 值，再根据 ID 值关联到 t1，再判断 t1 里面 c 的值是否等于 10。

逻辑结果一样，但执行的效率不同，优化器的作用就是决定选择使用哪个方案。

优化器阶段完成后，这个语句的执行方案就确定下来了。

### 执行器 ###

执行器阶段，开始执行语句。

开始执行的时候，要先判断一下你对这个表 T 有没有执行查询的权限，如果没有，就会返回没有权限的错误，如下所示 (在工程实现上，如果命中查询缓存，会在查询缓存返回结果的时候，做权限验证。查询也会在优化器之前调用 precheck 验证权限)。

如果有权限，就打开表继续执行。打开表的时候，执行器就会根据表的引擎定义，去使用这个引擎提供的接口。

比如我们这个例子中的表 T 中，ID 字段没有索引，那么执行器的执行流程是这样的：

1. 调用InnoDB引擎接口取这个表的第一行，判断ID值是不是10，如果不是则跳过，如果是则将这行存在结果集中；
2. 调用引擎接口取“下一行”，重复相同的判断逻辑，直到取到这个表的最后一行。
3. 执行器将上述遍历过程中所有满足条件的行组成的记录集作为结果集返回给客户端。

数据库的慢查询日志中看到一个 rows_examined 的字段，表示这个语句执行过程中扫描了多少行。这个值就是在执行器每次调用引擎获取数据行的时候累加的。

引擎扫描行数跟 rows_examined 并不是完全相同的。

### 小结 ###

MySQL的逻辑架构，对一个SQL语句完整执行流程的各个阶段有一个初步的印象。由于篇幅的限制，只是用一个查询的例子将各个环节过了一遍。

### 精选留言 ###

#### Q ####

如果表 T 中没有字段 k，而你执行了这个语句 select * from T where k=1, 那肯定是会报“不存在这个列”的错误： “Unknown column ‘k’ in ‘where clause’”。你觉得这个错误是在我们上面提到的哪个阶段报出来的呢？

#### A ####

* Oracle会在分析阶段判断语句是否正确，表是否存在，列是否存在等。

* 对权限的检查不在优化器之前做，，SQL语句要操作的表不只是SQL字面上那些。比如如果有个触发器，得在执行器阶段（过程中）才能确定。优化器阶段前是无能为力的。

分析器，优化器是对SQL语句进行择优处理，执行器需要判断选取表中的权限。

#### Q ####

为什么对权限的检查不在优化器之前做？

#### A ####

SQL语句要操作的表不只是SQL字面上那些。比如如果有个触发器，得在执行器阶段（过程中）才能确定。

#### Q ####

加粗内容为未掌握

1. MySQL的框架有几个组件, 各是什么作用? 
**2. Server层和存储引擎层各是什么作用?**
**3. you have an error in your SQL syntax 这个保存是在词法分析里还是在语法分析里报错?**
4. 对于表的操作权限验证在哪里进行?
**5. 执行器的执行查询语句的流程是什么样的? **

#### A ####

1. Server层和存储引擎，Server层分连接器、查询缓存、分析器、优化器、执行器。
**2. Server层：涵盖MySQL的大多数核心服务功能，以及内置函数；存储引擎：负责数据的存储和提取**
3. 语法分析——**词法分析：识别里面的字符串分别是什么，代表什么；语法分析：**
4. 执行器，在做执行器之前，通过获取一张表的数据，验证该用户是否有表的操作权限
**5. 执行器进行检索流程如下：
	* 调用InnoDB引擎接口取这个表的第一行
	* 调用引擎接口取“下一行”，重复相同的判断逻辑，直到取到这个表的最后一行
	* 执行器将上述遍历过程中所有满足条件的行组成的记录集作为结果集返回给客户端 **

#### A ####

## 02 | 日志系统：一条SQL更新语句是如何执行的？ ##

一条查询语句的执行过程一般是经过连接器、分析器、优化器、执行器等功能模块，最后到达存储引擎。

查询语句对策那一套流程，更新语句也是同样走一遍。

1. 连接器：执行语句前要先连接数据库
2. 查询缓存：一个表上有更新的时候，跟这个表有关的查询缓存会失效。所以这条语句就会把表T上所有缓存结果都清空（不建议使用查询缓存的原因）
3. 分析器：通过词法和语法解析知道这是一条更新语句
4. 优化器：使用ID索引
5. 执行器：负责具体执行，找到这一行，更新

### 重要的日志模块：redo log ###

WAL的全称是Write-Ahead Logging，关键点是先写日志，再写磁盘。

当有一条记录需要更新的时候，InnoDB引擎就会先把记录写到redo log（粉板）里面，并更新内存。InnoDB引擎会在适当的时候，将这个操作记录更新到磁盘里面（系统比较空闲的时候处理）。如果写满了，需要腾出空间才行。

InnoDB 的 redo log 是固定大小的，比如可以配置为一组 4 个文件，每个文件的大小是 1GB，那么这块“粉板”总共就可以记录 4GB 的操作。从头开始写，写到末尾就又回到开头循环写。

![16a7950217b3f0f4ed02db5db59562a7.png](img/16a7950217b3f0f4ed02db5db59562a7.png)

* write pos是当前记录的位置，一边写一边后移，写到第3号文件末尾后就回到0号文件开头。
* checkpoint是当前要擦除的位置，也是往后推移并且循环，擦除记录前要把记录更新到数据文件。
* write pos和checkpoint之间的是“粉版”上还空着的部分，可以用来记录新的操作。如果 write pos 追上 checkpoint，表示“粉板”满了，这时候不能再执行新的更新，得停下来先擦掉一些记录，把 checkpoint 推进一下。

crash-safe：因为redo log，InnoDB 就可以保证即使数据库发生异常重启，之前提交的记录都不会丢失

### 重要的日志模块：binlog ###

MySQL整体来看，有两块

1.  Server 层，主要做的是MySQL功能层面的事情
2.  引擎层，负责存储相关的具体事宜

---

1. redo log 是 InnoDB 引擎特有的；binlog 是 MySQL 的 Server 层实现的，所有引擎都可以使用。
2. redo log 是物理日志，记录的是“在某个数据页上做了什么修改”；binlog 是逻辑日志，记录的是这个语句的原始逻辑，比如“给 ID=2 这一行的 c 字段加 1 ”。
3. redo log 是循环写的，空间固定会用完；binlog 是可以追加写入的。“追加写”是指 binlog 文件写到一定大小后会切换到下一个，并不会覆盖以前的日志。

执行器和 InnoDB 引擎在执行这个简单的 update 语句时的内部流程：`mysql> update T set c=c+1 where ID=2;`

![2e5bff4910ec189fe1ee6e2ecc7b4bbe.png](img/2e5bff4910ec189fe1ee6e2ecc7b4bbe.png)

1. 
	* 执行器：执行器先找引擎取 ID=2 这一行。
	* 引擎：ID 是主键，引擎直接用树搜索找到这一行。如果 ID=2 这一行所在的数据页本来就在内存中，就直接返回给执行器；否则，需要先从磁盘读入内存，然后再返回。
2. 执行器：执行器拿到引擎给的行数据，把这个值加上 1，比如原来是 N，现在就是 N+1，得到新的一行数据，再调用引擎接口写入这行新数据。 
3. 引擎：引擎将这行新数据更新到内存中，同时将这个更新操作记录到 redo log 里面，此时 redo log 处于 prepare 状态。然后告知执行器执行完成了，随时可以提交事务。
4. 执行器：执行器生成这个操作的 binlog，并把 binlog 写入磁盘。
5. 引擎：执行器调用引擎的提交事务接口，引擎把刚刚写入的 redo log 改成提交（commit）状态，更新完成。

redo log的写入拆成了两个步骤：prepare和commit，两阶段提交

### 两阶段提交 ###

两阶段提交的原因是为了让两份日志的逻辑一致。=>怎样让数据库恢复到半个月内任意一秒的状态？

1. binlog会记录所有的逻辑操作
2. 系统定期做整库备份
3. 找到最近一天的全量备份，然后通过这个备份恢复到临时库
4. 从备份时间开始，将备份的binlog依次取出，重新进行到误操作时间点的那个时候

如果不使用两阶段提交。

1. 先写 redo log 后写 binlog。
	* redo log 写完之后，系统即使崩溃，仍然能够把数据恢复回来，所以恢复后这一行 c 的值是 1。
	* 但是由于 binlog 没写完就 crash 了，这时候 binlog 里面就没有记录这个语句。如果需要用这个 binlog 来恢复临时库的话，由于这个语句的 binlog 丢失，这个临时库就会少了这一次更新，恢复出来的这一行 c 的值就是 0，与原库的值不同。
2. 先写 binlog 后写 redo log。
	* 如果在 binlog 写完之后 crash，由于 redo log 还没写，崩溃恢复以后这个事务无效，所以这一行 c 的值是 0。但是 binlog 里面已经记录了“把 c 从 0 改成 1”这个日志。所以，在之后用 binlog 来恢复的时候就多了一个事务出来，恢复出来的这一行 c 的值就是 1，与原库的值不同。

不只是操作，还有扩容的时候。通常的做法是用全量备份加上应用 binlog 来实现的。

简单说，redo log 和 binlog 都可以用于表示事务的提交状态，而两阶段提交就是让这两个状态保持逻辑上的一致。

### 小结 ###

* 两个日志，即物理日志redo log和逻辑日志binlog。
* redo log用于保证crash-safe能力。
	*  innodb_flush_log_at_trx_commit 这个参数设置成 1 的时候，表示每次事务的 redo log 都直接持久化到磁盘。这个参数我建议你设置成 1，这样可以保证 MySQL 异常重启之后数据不丢失。
* sync_binlog  
	* sync_binlog 这个参数设置成 1 的时候，表示每次事务的 binlog 都持久化到磁盘。这个参数我也建议你设置成 1，这样可以保证 MySQL 异常重启之后 binlog 不丢失。 
* 与 MySQL 日志系统密切相关的“两阶段提交”：两阶段提交是跨系统维持数据逻辑一致性时常用的一个方案，即使你不做数据库内核开发，日常开发中也有可能会用到。   

前面我说到定期全量备份的周期“取决于系统重要性，有的是一天一备，有的是一周一备”。那么在什么场景下，一天一备会比一周一备更有优势呢？或者说，它影响了这个数据库系统的哪个指标？

最长恢复时间更短

### 精选留言 ###

#### Q ####

说到定期全量备份的周期“取决于系统重要性，有的是一天一备，有的是一周一备”。那么在什么场景下，一天一备会比一周一备更有优势呢？或者说，它影响了这个数据库系统的哪个指标？

#### A ####

一天一备，只需要根据前一天的redo log的记录把binlog 复现一遍即可，不容易无操作，隔断时间比较少。但这样redolog的记录会比较大。

Binlog两种模式

1. statement格式的话是记sql语句
2. row格式会记录行的内容，记两条，更新前和更新后的内容

redo是物理的，binlog是逻辑的；现在由于redo是属于InnoDB引擎，所以必须要有binlog（可以使用别的引擎）保证数据库的一致性，必须要保证2份日志一致。使用的2阶段式提交；其他感觉像事务，不是成功就是失败，不能让中间环节出现，也就是一个成功，一个失败。

#### Q ####

只用InonoDB引擎的时候还保留Binlog这种设计的原因

#### A ####

1. redolog只有InnoDB有，别的引擎没有
2. redolog是循环写的，不持久保存，binlog的“归档”这个功能，redolog是不具备的。

#### Q ####

后知后觉，误删除一段时间了，才发现误删除，此时，我把之前误删除的binlog导入，再把误删除之后binlog导入，会出现问题，比如主键冲突，而且binlog导数据，不同模式下时间也有不同，但是一般都是row模式

#### A ####

其实恢复数据只能恢复到误删之前到一刻，误删之后的，不能只靠binlog来做，因为业务逻辑可能因为误删操作的行为，插入了逻辑错误的语句，所以之后的，跟业务一起，从业务快速补数据的。只靠binlog补出来的往往不完整

#### Q ####

如果在重启后，需要通过检查binlog来确认redo log中处于prepare的事务是否需要commit，那是否不需要二阶段提交，直接以binlog的为准，如果binlog中不存在的，就认为是需要回滚的。

#### A ####

1 prepare阶段 2 写binlog 3 commit

当在2之前崩溃时
重启恢复：后发现没有commit，回滚。备份恢复：没有binlog 。
一致
当在3之前崩溃
重启恢复：虽没有commit，但满足prepare和binlog完整，所以重启后会自动commit。备份：有binlog. 一致

#### Q ####

1. redo log的概念是什么? 为什么会存在.
*2. 什么是WAL(write-ahead log)机制, 好处是什么.*
3. redo log 为什么可以保证crash safe机制.
4. binlog的概念是什么, 起到什么作用, 可以做crash safe吗?
5. binlog和redolog的不同点有哪些?
6. 物理一致性和逻辑一直性各应该怎么理解?
7. 执行器和innoDB在执行update语句时候的流程是什么样的?
8. 如果数据库误操作, 如何执行数据恢复?
9. 什么是两阶段提交, 为什么需要两阶段提交, 两阶段提交怎么保证数据库中两份日志间的逻辑一致性(什么叫逻辑一致性)?
10. 如果不是两阶段提交, 先写redo log和先写bin log两种情况各会遇到什么问题?

#### A ####

1. redo log是innoDB存储引擎特有的存储引擎，物理存储日志
*2. WAL技术，全称Write-Ahead Logging，关键点是先写日志，再写磁盘*
3. 31
4. 
4. binlog是
5. 123
6. 123
7. * 检索update的行是否在数据页
8.  * 通过使用误操作之前最近的时间节点的redo log进行全量恢复数据库
	* 并在此基础上使用binlog进行操作
	* 恢复到误操作时的数据库
9. 什么是
10.  

## 03 | 事务隔离：为什么你改了我还看不见？ ##

事务就是要保证一组数据库操作，要么全部成功，要么全部失败。

### 隔离性与隔离级别 ###

ACID（Atomicity，Consistency，Isolation，Durability，即原子性、一致性、隔离性和持久性）

当数据库上有多个事务同时执行的时候=>可能出现脏读（dirty read）、不可重复读（non-repeatable read）、幻读（phantom read）的问题=>有了“隔离级别”的概念，去解决这些问题

#### 隔离级别 ####

隔离级别越高，效率越低=>需要找到平衡点

SQL标准的事务隔离级别：

1. 读未提交（read uncommitted）：一个事务还没提交时，它做的变更就能被别的事务看到。
2. 读提交（read committed）：一个事务提交之后，它做的变更才会被其他事务看到。
3. 可重复读（repeatable read）：一个事务执行过程中看到的数据，总是跟这个事务在启动时看到的数据是一致的。当然在可重复读隔离级别下，未提交变更对其他事务也是不可见的。
4. 串行化（serializable）：对于同一行记录，“写”会加“写锁”，“读”会加“读锁”。当出现读写锁冲突的时候，后访问的事务必须等前一个事务执行完成，才能继续执行。

![7dea45932a6b722eb069d2264d0066f8](img/7dea45932a6b722eb069d2264d0066f8.png)

根据事务隔离级别的不同，V1、V2、V3返回值也会分别不同。

* 读未提交：V2、V3=2，B的事务还没提交，但做个变更就能被别的事务看到，所以V1也是2
* 读提交：V2、V3=2，事务 B 的更新在提交后才能被 A 看到，所以V1也是1
* 可重复读：事务在执行期间看到的数据前后必须是一致的，所以v1，v2=1，v3=2
* 串行化：则在事务 B 执行“将 1 改成 2”的时候，会被锁住。直到事务 A 提交后，事务 B 才可以继续执行。v1、v2值是1，v3的值是2

在实现上，数据库里面会创建一个视图，访问的时候以视图的逻辑结果为准。

* “读提交”隔离级别：在每个 SQL 语句开始执行的时候创建的
* “可重复读”隔离级别：在事务启动时创建的，整个事务存在期间都用这个视图

“读未提交”隔离级别：直接返回记录上的最新值，没有视图概念；而“串行化”隔离级别下直接用加锁的方式来避免并行访问。

在不同的隔离级别下，数据库行为是有所不同的。Oracle 数据库的默认隔离级别其实就是“读提交”，因此对于一些从 Oracle 迁移到 MySQL 的应用，为保证数据库隔离级别的一致，你一定要记得将 MySQL 的隔离级别设置为“读提交”。

	show variables like 'transaction_isolation'

根据业务场景选择不同的隔离级别

### 事务隔离实现 ###

假设一个值从1被按顺序改成2、3、4，在回滚日志会有如下的记录：

![d9c313809e5ac148fc39feff532f0fee](img/d9c313809e5ac148fc39feff532f0fee.png)

回滚日志删除条件：系统判断，当前没有事务再需要用到这些回滚日志时，回滚日志会被删除。当系统里没有比这个回滚日志更早的 read-view 的时候会被正式清除。

在 MySQL 中，实际上每条记录在更新的时候都会同时记录一条回滚操作。记录上的最新值，通过回滚操作，都可以得到前一个状态的值。

长事务意味着系统里面会存在很老的事务视图。由于这些事务随时可能访问数据库里面的任何数据，所以这个事务提交之前，数据库里面它可能用到的回滚记录都必须保留，这就会导致大量占用存储空间。在 MySQL 5.5 及以前的版本，回滚日志是跟数据字典一起放在 ibdata 文件里的，即使长事务最终提交，回滚段被清理，文件也不会变小。（我见过数据只有 20GB，而回滚段有 200GB 的库。最终只好为了清理回滚段，重建整个库。）还有占用锁资源，拖垮整个库的问题。

### 事务的启动方式 ###

MySQL 的事务启动方式有以下几种：

1. 显式启动事务语句， begin 或 start transaction。配套的提交语句是 commit，回滚语句是 rollback。
2. set autocommit=0，这个命令会将这个线程的自动提交关掉。意味着如果你只执行一个 select 语句，这个事务就启动了，而且并不会自动提交。这个事务持续存在直到你主动执行 commit 或 rollback 语句，或者断开连接。

在 autocommit 为 1 的情况下，用 begin 显式启动的事务，如果执行 commit 则提交事务。如果执行 commit work and chain，则是提交事务并自动启动下一个事务，这样也省去了再次执行 begin 语句的开销。同时带来的好处是从程序开发的角度明确地知道每个语句是否处于事务中。

information_schema 库的 innodb_trx 这个表中查询长事务，比如下面这个语句，用于查找持续时间超过 60s 的事务。

	select * from information_schema.innodb_trx where TIME_TO_SEC(timediff(now(),trx_started)) > 60

### 小结 ###

1. 介绍了 MySQL 的事务隔离级别的现象和实现
2. 根据实现原理分析了长事务存在的风险
3. 如何用正确的方式避免长事务

我给你留一个问题吧。你现在知道了系统里面应该避免长事务，如果你是业务开发负责人同时也是数据库负责人，你会有什么方案来避免出现或者处理这种情况呢？

### 精选留言 ###

#### Q ####

现在知道了系统里面应该避免长事务，如果你是业务开发负责人同时也是数据库负责人，你会有什么方案来避免出现或者处理这种情况呢？

#### A ####

少用长事务，尽量将高并发的表放在事务的后面。


## 04 | 深入浅出牵引（上） ##

索引的出现其实就是为了提高数据查询的效率，就像书的目录一样。对于数据库的表而言，索引其实就是它的“目录”。

### 索引的常见模型 ###

索引的出现是为了提高查询效率，三种常见的模型：

1. 哈希表
2. 有序数组
3. 搜索树

#### 哈希表 ####

哈希表是一种以键 - 值（key-value）存储数据的结构，输入待查找的值即 key，就可以找到其对应的值即 Value。哈希的思路很简单，把值放在数组里，用一个哈希函数把 key 换算成一个确定的位置，然后把 value 放在数组的这个位置。

多个key值经过哈希函数的换算，会出现同一个值的情况。处理这种情况的一种方法是，拉出一个链表。

哈希表这种结构适用于只有等值查询的场景

#### 有序数组 ####

有序数组在等值查询和范围查询场景中的性能就都非常优秀。

仅仅看查询效率，有序数组就是最好的数据结构。但需要更新数据的时候就往中间插入一个记录必需挪动后面所有的记录，成本很高。

有序数组索引只适用于静态存储引擎。

#### 搜索树 ####

二叉搜索树：每个节点的左儿子小于父节点，父节点又小于右儿子。

树可以有二叉，也可以有多叉。多叉树就是每个节点有多个儿子，儿子之间的大小保证从左到右递增。二叉树是搜索效率最高的，但是实际上大多数的数据库存储却并不使用二叉树。其原因是，索引不止存在内存中，还要写到磁盘上。

在MySQL中，索引是在存储引擎层实现的，所以并没有统一的索引标准，即不同存储的引擎的工作方式并不一样。而即使多个存储引擎支持同一种类型的索引，其底层的实现也可能不同。

为了让一个查询尽量少地读磁盘，就必须让查询过程访问尽量少的数据块。那么，我们就不应该使用二叉树，而是要使用“N 叉”树。这里，“N 叉”树中的“N”取决于数据块的大小。

#### 实战 ####

在 MySQL 中，索引是在存储引擎层实现的，所以并没有统一的索引标准，即不同存储引擎的索引的工作方式并不一样。而即使多个存储引擎支持同一种类型的索引，其底层的实现也可能不同。由于 InnoDB 存储引擎在 MySQL 数据库中使用最为广泛，所以下面我就以 InnoDB 为例，和你分析一下其中的索引模型。

### InnoDB的索引模型 ###

	mysql> create table t04(
	id int primary key, 
	k int not null, 
	name varchar(16),
	index (k))engine=InnoDB;

	insert into t04(id, k) values (100,1),(200,2),(300,3),(500,5),(600,6);

在 InnoDB 中，表都是根据主键顺序以索引的形式存放的，这种存储方式的表称为索引组织表。InnoDB 使用了 B+ 树索引模型，所以数据都是存储在 B+ 树中的。

每一个索引在 InnoDB 里面对应一棵 B+ 树。

* 主键索引的叶子节点存的是整行数据。在 InnoDB 里，主键索引也被称为聚簇索引（clustered index）。
* 非主键索引的叶子节点内容是主键的值。在 InnoDB 里，非主键索引也被称为二级索引（secondary index）。

#### 基于主键索引和普通索引的查询有什么区别？ ####

* 如果语句是 select * from T where ID=500，即主键查询方式，则只需要搜索 ID 这棵 B+ 树；
* 如果语句是 select * from T where k=5，即普通索引查询方式，则需要先搜索 k 索引树，得到 ID 的值为 500，再到 ID 索引树搜索一次。这个过程称为回表。
* 基于非主键索引的查询需要多扫描一棵索引树。因此，我们在应用中应该尽量使用主键查询。

### 索引维护 ###

B+ 树为了维护索引有序性，在插入新值的时候需要做必要的维护。

![dcda101051f28502bd5c4402b292e38d.png](img/dcda101051f28502bd5c4402b292e38d.png)

自增主键是指自增列上定义的主键，在建表语句中一般是这么定义的： NOT NULL PRIMARY KEY AUTO_INCREMENT。插入新纪录的时候可以不指定ID的值，系统会获取当前ID最大值加1作为下一条记录的ID值。

* 自增主键：符合了递增插入的场景，每次插入一条新纪录，都是追加操作，都不涉及到挪动其他记录，也不会触发叶子节点的分裂
* 业务逻辑的字段做主键：不容易保证有序插入，写数据成本相对较高

显然，主键长度越小，普通索引的叶子节点就越小，普通索引占用的空间也就越小。

从性能和存储空间方面考量，自增主键往往是更合理的选择。

1. 只有一个索引；
2. 该索引必须是唯一索引
3. 类似典型的KV场景
4. “尽量使用主键查询”原则，直接将这个索引设置为主键，可以避免每次查询需要搜索两棵树

### 小结 ###

InnoDB采用的B+树结构，以及InnoDB要选择B+树的原因。B+ 树能够很好地配合磁盘的读写特性，减少单次查询的磁盘访问次数。由于 InnoDB 是索引组织表，一般情况下我会建议你创建一个自增主键，这样非主键索引占用的空间最小。但事无绝对，我也跟你讨论了使用业务逻辑字段做主键的应用场景。

### 精选留言 ###

* 每一个表是好几棵B+树，树结点的key值就是某一行的主键，value是该行的其他数据。新建索引就是新增一个B+树，查询不走索引就是遍历主B+树。

#### Q ####

重建索引k

	alter table T drop index k;
	alter table T add index(k);

如果重建主键索引

	alter table T drop primary key;
	alter table T add primary key(id);

#### A ####

* 如果删除，新建主键索引，会同时去修改普通索引对应的主键索引，性能消耗比较大。
* 删除重建普通索引貌似影响不大，不过要注意在业务低谷期操作，避免影响业务。

#### Q ####

没有主键的表，有一个普通索引。怎么回表？

#### A ####

没有主键的表，innodb会给默认创建一个Rowid做主键

#### Q ####

“N叉树”的N值在MySQL中是可以被人工调整的么？

#### A ####

可以按照调整key的大小的思路来说；

如果你能指出来5.6以后可以通过page大小来间接控制应该能加分吧

面试回答不能太精减，计算方法、前缀索引什么的一起上😄

1， 通过改变key值来调整
N叉树中非叶子节点存放的是索引信息，索引包含Key和Point指针。Point指针固定为6个字节，假如Key为10个字节，那么单个索引就是16个字节。如果B+树中页大小为16K，那么一个页就可以存储1024个索引，此时N就等于1024。我们通过改变Key的大小，就可以改变N的值
2， 改变页的大小
页越大，一页存放的索引就越多，N就越大。

#### Q ####

定位到page，page内部怎么去定位行数据

#### A ####

内部有个有序数组，二分法

## 05 | 深入浅出牵引（下） ##

mysql> create table t05 (
ID int primary key,
k int NOT NULL DEFAULT 0, 
s varchar(16) NOT NULL DEFAULT '',
index k(k))
engine=InnoDB;

insert into t05 values(100,1, 'aa'),(200,2,'bb'),(300,3,'cc'),(500,5,'ee'),(600,6,'ff'),(700,7,'gg');

`select * from t05 where k between 3 and 5`，这条SQL查询查询语句的执行流程：

1. 在k索引树上找到k=3的记录，取得ID=300;（读了k索引树）
2. 再到ID索引树查到ID=300对应的R3;（回表）
3. 在k索引树取下一个值k=5，取得ID=500;（读了k索引树）
4. 再回到ID索引树查到ID=500对应的R4;（回表）
5. 在k索引树取下一个值k=6，不满足条件，循环结束。（读了k索引树）

回到主键索引树搜索的过程，我们称为回表。由于查询结果所需要的数据只在主键索引上有，所以不得不回表。经过性能优化来避免回表。

### 覆盖索引 ###

`select * from t05 where k between 3 and 5`

由于覆盖索引可以减少树的搜索次数，显著提升查询性能，所以使用覆盖索引是一个常用的性能优化手段。

索引字段的维护总是有代价的。在建立冗余索引来支持覆盖索引时就需要权衡考虑了。

	CREATE TABLE `t05_user` (
	  `id` int(11) NOT NULL,
	  `id_card` varchar(32) DEFAULT NULL,
	  `name` varchar(32) DEFAULT NULL,
	  `age` int(11) DEFAULT NULL,
	  `ismale` tinyint(1) DEFAULT NULL,
	  PRIMARY KEY (`id`),
	  KEY `id_card` (`id_card`),
	  KEY `name_age` (`name`,`age`)
	) ENGINE=InnoDB

身份证号是t05_user的唯一标识，可以通过身份证号查询所有的市民信息的需求。只要在身份证号字段上建立索引就够了。而再建立一个（身份证号、姓名）的联合索引，是不是浪费空间？如果有个高频请求，要根据市民的身份证号来查询他的性命，那么这个联合索引就有意义了。在高频请求上用到覆盖索引，避免回表查整行记录。

### 最左前缀原则 ###

B+ 树这种索引结构，可以利用索引的“最左前缀”，来定位记录。用(name, age)这个联合索引来分析。

在建立联合索引的时候，如何安排索引内的字段顺序。

1. 如果通过调整顺序，可以少维护一个索引，那么这个索引往往就是需要优先考虑采用。
	* 既有联合查询，又有基于a、b各自的查询：只有b的语句，是无法使用（a，b）这个联合索引，这时候不得不维护另外一个索引，同时维护（a，b）、（b）这两个索引
2. 考虑的原则就是空间

### 索引下推 ###

	mysql> select * from tuser where name like '张%' and age=10 and ismale=1;

* 在MySQL5.6之前，只能从ID3开始一个个回表。到主键索引上找出数据行，再对比字段值。
	* 在(name, age)索引里面，InnoDB并不会去看age的值，只是按顺序把“name第一个字是‘张’”的记录一条条取出来回表，需要回表4次。 
* MySQL5.6引入的索引下推优化（index condition pushdown），可以在索引遍历过程中，对索引中包含的字段先做判断，直接过滤掉不满足条件的记录，减少回表次数。
	*  InnoDB在（name， age）索引内部就判断了age是否等于10，对于不等于10的记录，直接判断并跳过。只需要对 ID4、ID5 这两条记录回表取数据判断，就只需要回表 2 次。

### 小结 ###

数据库索引的概念，包括了覆盖索引、前缀索引、索引下推。在满足语句需求的情况下， 尽量少地访问资源是数据库设计的重要原则之一。在使用数据库的时候，尤其是在设计表结构时，也要以减少资源消耗作为目标。

### 精选留言 ###

#### Q ####
	CREATE TABLE `t05_geek` (
	  `a` int(11) NOT NULL,
	  `b` int(11) NOT NULL,
	  `c` int(11) NOT NULL,
	  `d` int(11) NOT NULL,
	  PRIMARY KEY (`a`,`b`),
	  KEY `c` (`c`),
	  KEY `ca` (`c`,`a`),
	  KEY `cb` (`c`,`b`)
	) ENGINE=InnoDB;
	INSERT INTO `t05_geek` VALUES(1,2,3,d), (1,3,2,d),(1,4,3,d),(2,1,3,d),(2,2,2,d),(2,3,4,d);
#### A ####
索引 ca 的组织是先按 c 排序，再按 a 排序，同时记录主键–c--|–a--|–主键部分b-- （注意，这里不是 ab，而是只有 b）
2 1 3
2 2 2
3 1 2
3 1 4
3 2 1
4 2 3
这个跟索引 c 的数据是一模一样的。

索引 cb 的组织是先按 c 排序，在按 b 排序，同时记录主键–c--|–b--|–主键部分a-- （同上）
2 2 2
2 3 1
3 1 2
3 2 1
3 4 1
4 3 2
所以，结论是 ca 可以去掉，cb 需要保留。

## 06 | 全局锁和表锁：给表加个字段怎么有这么多阻碍 ##

数据库锁设计的初衷是处理并发问题，作为多用户共享的资源，当出现并发访问的时候，数据库需要合理地控制资源的访问规则。而锁就是用来实现这些访问规则的重要数据结构。

*根据加锁的范围，MySQL 里面的锁大致可以分成全局锁、表级锁和行锁三类。*

### 全局锁 ###

全局锁就是对整个数据库实例加锁。通过 Flush tables with read lock(FTWRL) 确保不会有其他线程对数据库做更新，然后对整个库做备份。注意，在备份过程中整个库完全处于只读状态。unlock tables解锁。

*全局锁的典型使用场景是，做全库逻辑备份。*（整库每个表都select出来存成文本）。

* 如果你在主库上备份，那么在备份期间都不能执行更新，业务基本上就得停摆；
* 如果你在从库上备份，那么备份期间从库不能执行主库同步过来的 binlog，会导致主从延迟。

如果不加锁的话，备份系统备份的得到的库不是一个逻辑时间点，这个视图是逻辑不一致的。

*一致性读是好，但前提是引擎要支持这个隔离级别。*对于MyISAM不支持事务的引擎，备份过程中有更新，总是只能取到最新的数据，破坏了备份的一致性。——FTWRL命令

single-transaction方法只适用于所有的表适用事务引擎的库。如果有的表适用了不支持事务的引擎，那么备份就只能通过FTWRL方法。（DBA要求业务开发人员适用InnoDB代替MyISAM的原因之一）。

*既然要全库只读，为什么不使用 set global readonly=true 的方式呢？*

1. 在有些系统中，readonly 的值会被用来做其他逻辑，比如用来判断一个库是主库还是备库。因此，修改 global 变量的方式影响面更大，我不建议你使用。
2. 在异常处理机制上有差异。如果执行 FTWRL 命令之后由于客户端发生异常断开，那么 MySQL 会自动释放这个全局锁，整个库回到可以正常更新的状态。而将整个库设置为 readonly 之后，如果客户端发生异常，则数据库就会一直保持 readonly 状态，这样会导致整个库长时间处于不可写状态，风险较高。

### 表级锁 ###

MySQL 里面表级别的锁有两种：

1. 表锁：`lock tables...read/write`。
	* lock table语法除了会限制别的进程的读写外，还限定了本线程接下来的操作对象，session A执行了`lock tables t1 read,t2 write;`session B写t1、读写t2的语句会被阻塞，session A在执行unlock tables之前，写t1不允许，也不能访问到其他表
	*  可以用 unlock tables 主动释放锁，也可以在客户端断开的时候自动释放。lock tables 语法除了会限制别的线程的读写外，也限定了本线程接下来的操作对象
	*  在客户端断开的时候自动释放
2. 元数据锁（meta data lock，MDL)。

对于InnoDB支持行锁的引擎，一般不使用lock tables命令来控制并发，毕竟锁住整个表的影响面很大。

*另一类表级的锁是 MDL（metadata lock)*。MDL不需要显式使用，在访问一个表的时候会自动加上。

MySQL5.5版本中引入了MDL，当对一个表做增删改查操作的时候，加上MDL读锁；当要对表做结构变更操作的时候，加MDL写锁。

* 读锁之间不互斥，因此可以有多个线程同时对一张表增删改查。
* 读写锁之间、写锁之间是互斥的，用来保证变更表结构操作的安全性。因此，如果有两个线程要同时给一个表加字段，其中一个要等另一个执行玩才能开始执行。

MDL锁是系统默认会加的。

给一个表加字段，或者修改字段，或者加索引，需要扫描全表的数据。

#### 如何安全地给小表加字段？ ####

1. 解决长事务，事务不提交，就会一直占着MDL锁。
	* 在 MySQL 的 information_schema 库的 innodb_trx 表中，查到当前执行中的事务。
	* 如果你要做 DDL 变更的表刚好有长事务在执行，要考虑先暂停 DDL，或者 kill 掉这个长事务。
2. 变更的表是一个热点表（数据量不大，但是上面的请求很频繁，不得不加字段）
	* kill未必管用，新的请求马上就来了
	* 在alert table语句里面设定等待时间（如果在这个指定的等待时间里面能够拿到 MDL 写锁最好，拿不到也不要阻塞后面的业务语句，先放弃）
	* repeat

具体语句：

	ALTER TABLE tbl_name NOWAIT add column ...
	ALTER TABLE tbl_name WAIT N add column ... 

### 小结 ###

介绍了 MySQL 的全局锁和表级锁

全局锁主要用在逻辑备份过程中。对于全部是 InnoDB 引擎的库，我建议你选择使用–single-transaction 参数。

表锁一般是在数据库引擎不支持行锁的时候才会被用到的。lock tables

* 要么是你的系统现在还在用 MyISAM 这类不支持事务的引擎，那要安排升级换引擎；
* lock tables 和 unlock tables 改成 begin 和 commit

MDL 会直到事务提交才释放，在做表结构变更的时候，你一定要小心不要导致锁住线上查询和更新。

## 07 | 行锁功过：怎么减少行锁对性能的影响？ ##

MySQL 的行锁是在引擎层由各个引擎自己实现的。但并不是所有的引擎都支持行锁，比如 MyISAM 引擎就不支持行锁。不支持行锁意味着并发控制只能使用表锁，对于这种引擎的表，同一张表上任何时刻只能有一个更新在执行，这就会影响到业务并发度。InnoDB 是支持行锁的，这也是 MyISAM 被 InnoDB 替代的重要原因之一。

InnoDB 的行锁，以及如何通过减少锁冲突来提升业务并发度。行锁就是针对数据表中行记录的锁。比如事务 A 更新了一行，而这时候事务 B 也要更新同一行，则必须等事务 A 的操作完成后才能进行更新。

### 从两阶段锁说起 ###

事务A在执行完两条update语句后，持有了那些锁，都是在commit的时候才释放的。

在InnoDB事务中，行锁是在需要的时候才加上的，但并不是不需要了就立刻释放，而是要等到事务结束时才释放。这个就是两阶段锁协议。

### 死锁和死锁检测 ###

当并发系统中不同线程出现循环资源依赖，涉及的线程都在等待别的线程释放资源时，就会导致这几个线程都进入无限等待的状态，称为死锁。

在InnoDB事务中，行锁是在需要的时候才加上的，但并不是不需要就立刻释放，而是要等到事务结束才释放。这个就是两阶段锁协议。所以*如果事务中需要锁多个行，要把最可能造成锁冲突、最可能影响并发度的锁尽量往后放*。

1. 直接进入等待，直到超时。这个超时时间可以通过参数 innodb_lock_wait_timeout 来设置。
2. 发起死锁检测，发现死锁后，主动回滚死锁链条中的某一个事务，让其他事务得以继续执行。将参数 innodb_deadlock_detect 设置为 on，表示开启这个逻辑。

在InnoDB中，innodb_lock_wait_timeout的默认值是50s（如果采用第一个策略，当出现死锁以后，第一个被锁住的线程要过 50s 才会超时退出，然后其他线程才有可能继续执行）——对于在线服务来说，这个等待时间往往是无法接受的。

每个新来的被堵住的线程，都要判断会不会由于自己的加入导致了死锁，这是一个时间复杂度是 O(n) 的操作。假设有 1000 个并发线程要同时更新同一行，那么死锁检测操作就是 100 万这个量级的。虽然最终检测的结果是没有死锁，但是这期间要消耗大量的 CPU 资源。

1. 如果确保这个业务一定不会出现死锁，可以临时把死锁检测关掉
2. 控制并发度：这个并发控制要做在数据库服务端
	* 在中间件则考虑在中间件实现
	* 如果可以修改MySQL源码，则在MySQL里面。在进入引擎之前排队

如果团队里暂时没有数据库方面的专家，不能实现这样的方案，能不能从设计上优化这个问题呢？

将一行改成逻辑上的多行来减少锁冲突，还是以影院账户为例，可以考虑放在多条记录上，比如 10 个记录，影院的账户总额等于这 10 个记录的值的总和。

### 小结 ###

MySQL的行锁，涉及了两阶段锁协议、死锁和死锁检测这两大部分内容。

1. 两阶段协议为起点
2. 如果事务中需要多个行，把最可能造成锁冲突、最可能影响并发度的锁的申请时机尽量往后放
3. 调整语句顺序并不能完全避免死锁
	* 死锁
	* 死锁的检测

### 精选留言 ###

#### Q ####

最后，我给你留下一个问题吧。如果你要删除一个表里面的前 10000 行数据，有以下三种方法可以做到：
1. 直接执行 delete from T limit 10000;
2. 在一个连接中循环执行 20 次 delete from T limit 500;
3. 在 20 个连接中同时执行 delete from T limit 500。

#### A ####

1. 单一线程很长，会阻塞其他人操作
2. 增加并发性
3. 互斥，人为制造冲突

#### Q ####

对于行锁来说。两个update同时更新一条数据是互斥的。这个是因为多种锁同时存在时，以粒度最小的锁为准的原因么？

#### A ####

不是“以粒度最小为准”，而是如果有多种锁，必须得“全部不互斥”才能并行，只要有一个互斥，就得等。

## 08 | 事务到底是隔离的还是不隔离的？ ##

	mysql> CREATE TABLE `t08` (
	  `id` int(11) NOT NULL,
	  `k` int(11) DEFAULT NULL,
	  PRIMARY KEY (`id`)
	) ENGINE=InnoDB;
	insert into t08(id, k) values(1,1),(2,2);

`begin/start transaction`命令并不是一个事务的起点，在执行到它们之后的第一个操作InnoDB表的语句，事务才真正启动。如果想要马上启动一个事务，可以使用`start transaction with consistent snapshot` 这个命令。

1. 一致性视图是在执行第一个快照读语句时创建的；
2. 一致性视图是在执行 start transaction with consistent snapshot 时创建的。

在MySQL里，有两个“视图”的概念：

* view，是一个用查询语句定义的虚拟表，在调用的时候执行查询语句并生成结果。创建视图的语法是`create view...`，而它的查询方法与表一样。
* InnoDB在实现MVCC时用到的一致性读视图，即consitent read view，用于支持RC（Read Committed，读操作）和RR（Repeatable Read，可重复读）隔离级别的实现。

没有物理结构，作用的是事务执行期间用来定义“我能看到什么数据”。

### “快照”在MVCC里怎么工作的？ ###

在可重复读隔离级别下，事务在启动的时候就“拍了个快照”。注意，这个快照是基于整库的。

* transaction id，InnoDB 里面每个事务有一个唯一的事务 ID，在事务开始的时候向 InnoDB 的事务系统申请的，是按申请顺序严格递增的。
* row trx_id，每行数据也都是有很多版本的。每次事务更新数据的时候，都会生成一个新的数据版本；旧的数据版本要保留，并且在新的数据版本中，能够有信息可以直接拿到；数据表中的一行记录，其实可能有多个版本（row），每个版本有自己的row_trx_id。

![9416c310e406519b7460437cb0c5c149.png](img/9416c310e406519b7460437cb0c5c149.png)

1. 事务C[99, 100, 101, 102]、事务B[99, 100, 101]和事务A[99, 100]
2. 第一个有效更新是事务C，数据的最新版本的row trx_id是102
3. 第二个有效更新是事务B，row trx_id是101
4. 第三个有效更新是事务A

----

* 版本位提交，不可见
* 版本已提交，但是是在视图创建后提交，不可见
* 版本已提交，而且是在视图创建前提交的，可见

#### undo log在哪呢？ ####

对于当前事务的启动瞬间来说，一个数据版本的row_trx_id，有以下几种可能：

1. 已提交的事务或者当前的事务自己生成的，这个数据是可见的
2. 将来启动的事务生成的，是肯定不可兼得
3. 未提交事务集合
	* 若 row trx_id 在数组中，表示这个版本是由还没提交的事务生成的，不可见；
	* 若 row trx_id 不在数组中，表示这个版本是已经提交了的事务生成的，可见。 

*InnoDB 利用了“所有数据都有多个版本”的这个特性，实现了“秒级创建快照”的能力。*

InnnoDB为每个事务构造了一个数组，用来保存这个事务启动瞬间，当前正在“活跃”的所有事务ID。“活跃”指的就是，启动了但还没提交。数组里面事务ID的最小值记为低水位，当前系统里面已经创建过的事务ID的最大值加1记为高水位。

### 更新逻辑 ###

*事务 B 的 update 语句，如果按照一致性读，好像结果不对哦？*

*更新数据都是先读后写的，而这个读，只能读当前的值，称为“当前读”（current read）。*

读提交的逻辑和可重复读的逻辑类似，最主要的区别是：

1. 在可重复读隔离级别下，只需要在事务开始的时候创建一致性视图，之后事务里的其他查询都共用这个一致性视图；
2. 在读提交隔离级别下，每一个语句执行前都会重新算出一个新的视图。

### 小结 ###

InnoDB 的行数据有多个版本，每个数据版本有自己的 row trx_id，每个事务或者语句有自己的一致性视图。普通查询语句是一致性读，一致性读会根据 row trx_id 和一致性视图确定数据版本的可见性。

* 对于可重复读，查询只承认在事务启动前就已经提交完成的数据；
* 对于读提交，查询只承认在语句启动前就已经提交完成的数据；

### 精选留言 ###

#### Q ####

下面的表结构和初始化语句作为试验环境，事务隔离级别是可重复读。现在，要把所有“字段c和id值相等的行”的c值清零，但是却发现了一个 “诡异”的、改不掉的情况

	mysql> CREATE TABLE `t08_home` (
	  `id` int(11) NOT NULL,
	  `c` int(11) DEFAULT NULL,
	  PRIMARY KEY (`id`)
	) ENGINE=InnoDB;
	insert into t08_home(id, c) values(1,1),(2,2),(3,3),(4,4);

1. 使用sessionA和sessionB
2. 通过sessionA进行线程更新，通过使用线程A进行更新数据为0，并使用sleep
3. 通过线程B进行数据检索
4. 因为事务隔离级别是可重复读，当他sessionA使用更新，会创建view，而sessionB是不可见的，所以sessionB检索时只能查到1，2，3，4原始值，而不是0，0，0，0

#### A ####

思考题，RR下，用另外一个事物在update执行之前，先把所有c值修改，应该就可以。比如update t set c = id + 1。
这个实际场景还挺常见——所谓的“乐观锁”。时常我们会基于version字段对row进行cas式的更新，类似update ...set ... where id = xxx and version = xxx。如果version被其他事务抢先更新，则在自己事务中更新失败，trx_id没有变成自身事务的id，同一个事务中再次select还是旧值，就会出现“明明值没变可就是更新不了”的“异象”（anomaly）。解决方案就是每次cas更新不管成功失败，结束当前事务。如果失败则重新起一个事务进行查询更新。

#### Q ####

请教一个问题，业务上有这样的需求，A、B两个用户，如果互相喜欢，则成为好友。设计上是有两张表，一个是like表，一个是friend表，like表有user_id、liker_id两个字段，我设置为复合唯一索引即uk_user_id_liker_id。语句执行顺序是这样的：
以A喜欢B为例：
1、先查询对方有没有喜欢自己（B有没有喜欢A）
select * from like where user_id = B and liker_id = A
2、如果有，则成为好友
insert into friend
3、没有，则只是喜欢关系
insert into like

如果A、B同时喜欢对方，会出现不会成为好友的问题。因为上面第1步，双方都没喜欢对方。第1步即使使用了排他锁也不行，因为记录不存在，行锁无法生效。请问这种情况，在mysql锁层面有没有办法处理

#### A ####

# 实践篇 #

## 09 | 普通索引和唯一索引，应该怎么选择？ ##

	select name from CUser where id_card = 'xxxxxxxyyyyyyzzzzz';

id_card字段比较大，不建议当主键，可以有两个选择，要么给id_card字段创建唯一索引，要么创建普通索引。

### 查询过程 ###

* 对于普通索引来说，查找到满足条件的第一个记录（5,500）后，需要查找下一个记录，直到碰到第一个不满足k=5条件的记录。
* 对于唯一索引来说，由于索引定义了唯一性，查找到第一个满足条件的记录后，就会停止继续检索。

InnoDB 的数据是按数据页为单位来读写的。当需要读一条记录的时候，并不是将这个记录本身从磁盘读出来，而是以页为单位，将其整体读入内存。在 InnoDB 中，每个数据页的大小默认是 16KB。

查询语句在索引树上查找的过程

1. 通过 B+ 树从树根开始
2. 按层搜索到叶子节点
3. 数据页通过二分法定位记录

这里普通索引和唯一索引影响微乎其微。

### 更新过程 ###

当需要更新一个数据页时，如果数据页在内存中就直接更新，而如果这个数据页还没有在内存中的话，在不影响数据一致性的前提下，InooDB 会将这些更新操作缓存在 change buffer 中，这样就不需要从磁盘中读入这个数据页了。在下次查询需要访问这个数据页的时候，将数据页读入内存，然后执行 change buffer 中与这个页有关的操作。通过这种方式就能保证这个数据逻辑的正确性。

change buffer：可以持久化的数据，在内存中有拷贝，也会被写入到磁盘上。

将change buffer中的操作应用到原数据页，得到最新结果的过程称为merge。除了访问这个数据页会触发merge外，系统有后台线程会定期merge。在数据库正常关闭（shutdown）的过程中，也会执行 merge 操作。

将更新操作先记录在 change buffer，减少读磁盘，语句的执行速度会得到明显的提升。而且，数据读入内存是需要占用 buffer pool 的，所以这种方式还能够避免占用内存，提高内存利用率。

*什么条件下可以使用 change buffer 呢？*

一索引的更新就不能使用 change buffer，实际上也只有普通索引可以使用。

1. 记录要在更新的目标页在内存中
	* 对于唯一索引来说，找到 3 和 5 之间的位置，判断到没有冲突，插入这个值，语句执行结束；
	* 对于普通索引来说，找到 3 和 5 之间的位置，插入这个值，语句执行结束。
	* 普通索引和唯一索引对更新语句性能影响的差别，只是一个判断，只会耗费微小的 CPU 时间。
2. 记录要更新的目标页不在内存中
	* 对于唯一索引来说，需要将数据页读入内存，判断到没有冲突，插入这个值，语句执行结束；
	* 对于普通索引来说，则是将更新记录在 change buffer，语句执行就结束了。

将数据从磁盘读入内存涉及随机 IO 的访问，是数据库里面成本最高的操作之一。change buffer 因为减少了随机磁盘访问，所以对更新性能的提升是会很明显的。

*这时候唯一索引损耗十分明显*

#### change buffer 的使用场景 ####

使用 change buffer 对更新过程的加速作用，也清楚了 change buffer 只限于用在普通索引的场景下，而不适用于唯一索引。

* 对于写多读少的业务来说，页面在写完以后马上被访问到的概率比较小，此时 change buffer 的使用效果最好。
* 一个业务的更新模式是写入之后马上会做查询，那么即使满足了条件，将更新记录先记录在change buffer，但之后由于马上要访问这个数据页，会立即触发merge过程。这个随机访问IO的次数不会减少，反而增加了change buffer的维护代价（副作用）

#### 索引选择和实践 ####

普通索引和唯一索引应该怎么选择。这两类索引在查询能力上是没差别的，主要考虑的是对更新性能的影响。尽量选择普通索引。

如果所有的更新后面，都马上伴随着对这个记录的查询，那么你应该关闭 change buffer。而在其他情况下，change buffer 都能提升更新性能。

普通索引和 change buffer 的配合使用，对于数据量大的表的更新优化还是很明显的。

### change buffer 和 redo log ###

* redo log 主要节省的是随机写磁盘的 IO 消耗（转成顺序写）
* change buffer 主要节省的则是随机读磁盘的 IO 消耗。

TODO 未理解

### 小结 ###

1. 从普通索引和唯一索引的选择开始
2. 分享了数据的查询和更新过程
3. change buffer 的机制以及应用场景
4. 索引选择的实践

由于唯一索引用不上 change buffer 的优化机制，因此如果业务可以接受，从性能角度出发我建议你优先考虑非唯一索引。最后，又到了思考题时间。

* 保证业务正确性
* 在一些“归档库”的场景，可以考虑使用普通索引

### 精选留言 ###

#### A ####

change buffer的前身是insert buffer，只能对insert操作优化，后来增加了update/delete的支持。

#### A ####

* 系统表空间就是用来放系统信息的，比如数据字典什么的，对应的磁盘文件是ibdata1,
* 数据表空间就是一个个的表数据文件，对应的磁盘文件就是 表名.ibd

#### Q ####

change buffer相当于推迟了更新操作，那对并发控制相关的是否有影响，比如加锁？我一直以为加锁需要把具体的数据页读到内存中来，才能加锁，然而并不是？

#### A ####

锁是一个单独的数据结构，如果数据页上有锁，change buffer 在判断“是否能用”的时候，就会认为否。

## 10 | MySQL为什么有时候会选错索引？ ##

表定义：

	CREATE TABLE `t10` (
	  `id` int(11) NOT NULL,
	  `a` int(11) DEFAULT NULL,
	  `b` int(11) DEFAULT NULL,
	  PRIMARY KEY (`id`),
	  KEY `a` (`a`),
	  KEY `b` (`b`)
	) ENGINE=InnoDB;

存储过程定义：

	delimiter ;;
	create procedure idata()
	begin
	  declare i int;
	  set i=1;
	  while(i<=100000)do
	    insert into t values(i, i, i);
	    set i=i+1;
	  end while;
	end;;
	delimiter ;
	call idata();

SQL语句：

	mysql> select * from t10 where a between 10000 and 20000;

explain命令看到的这条语句的执行情况：

	mysql> explain select * from t10 where a between 10000 and 20000;

key这个字段值是'a'，表示优化器选择了索引a。

|session A| session B|
|--|--|
|start transaction with consistent snapshot;||
||delete from t;|
||call idata();|
|||
||explain select * from t10 where a between 10000 and 20000;|
|commit;||

此时session B的查询语句`select * from t10 where a between 10000 and 2000`不会再选择索引a。

### 优化器的逻辑 ###

选择索引是优化器的工作。而优化器选择索引的目的，是找到一个最优的执行方案，并用最小的代价去执行语句。在数据库里面，扫描行数是影响执行代价的因素之一。扫描的行数越少，意味着访问磁盘数据的次数越少，消耗的 CPU 资源越少。扫描行数也不是唯一的判断标准，优化器还会结合是否使用临时表、是否排序等因素进行综合判断。

MySQL选错索引在判断扫描行数的时候出了问题，*扫描行数是怎么判断的？*

基数：一个索引上不同的值，这个索引的区分度就越好。而一个索引上不同的值的个数。基数越大，索引的区分度越好。（MySQL再真正执行语句之前，并不能精确地知道满足这个条件的记录有多少条，而只能根据统计信息来计算记录数。）

使用`show index`方法，看到索引的基数。

### 索引选择异常和处理 ###

1. 采用 force index 强行选择一个索引。MySQL会根据语法解析的结果分析可能使用的索引作为候选项，然后在候选列表中依次判断每个索引需要扫面多少行
2. 修改语句，引导MySQL使用期望的索引
3. 在有些场景下，我们可以新建一个更合适的索引，来提供给优化器做选择，或删掉误用的索引

### 小结 ###

索引统计的更新机制，优化器存在选错索引的可能性。

* 对于索引统计信息不准确导致的问题，用analyze table来解决。
* 其他优化器误判的情况，可以在应用端用force index来强行指定索引，也可以用过修改语句来引导优化器，还可以通过增加或者删除索引来绕过这个问题。

### 精选留言 ###

#### Q ####

前面我们在构造第一个例子的过程中，通过 session A 的配合，让 session B 删除数据后又重新插入了一遍数据，然后就发现 explain 结果中，rows 字段从 10001 变成 37000 多。而如果没有 session A 的配合，只是单独执行 delete from t 、call idata()、explain 这三句话，会看到 rows 字段其实还是 10000 左右。你可以自己验证一下这个结果。

#### A ####

## 11 | 怎么给字符串字段加索引？ ##

	mysql> create table t11(
	ID bigint unsigned primary key,
	email varchar(64), 
	... 
	)engine=innodb; 

	mysql> select f1, f2 from SUser where email='xxx';

MySQL是支持前缀索引的，定义字符串的一部分为索引。默认地，如果创建索引的语句不指定前缀长度，那么索引就会包含整个字符串。

	mysql> alter table SUser add index index1(email);
第一个语句创建的 index1 索引里面，包含了每个记录的整个字符串

	mysql> alter table SUser add index index2(email(6));
第二个语句创建的 index2 索引里面，对于每个记录都是只取前 6 个字节。


	select id,name,email from SUser where email='zhangssxyz@xxx.com';

如果使用的是index1（即 email 整个字符串的索引结构），执行顺序是这样的：

1. 从 index1 索引树找到满足索引值是’zhangssxyz@xxx.com’的这条记录，取得 ID2 的值；
2. 到主键上查到主键值是 ID2 的行，判断 email 的值是正确的，将这行记录加入结果集；
3. 取 index1 索引树上刚刚查到的位置的下一条记录，发现已经不满足 email='zhangssxyz@xxx.com’的条件了，循环结束。

只需要回主键索引取一次数据，所以系统认为只扫描了一行。

如果使用的是index2（即 email(6) 索引结构），执行顺序是这样的：

1. 从 index2 索引树找到满足索引值是’zhangs’的记录，找到的第一个是 ID1；
2. 到主键上查到主键值是 ID1 的行，判断出 email 的值不是’zhangssxyz@xxx.com’，这行记录丢弃；
3. 取 index2 上刚刚查到的位置的下一条记录，发现仍然是’zhangs’，取出 ID2，再到 ID 索引上取整行然后判断，这次值对了，将这行记录加入结果集；
4. 重复上一步，直到在 idxe2 上取到的值不是’zhangs’时，循环结束。

**使用前缀索引，定义好长度，就可以做到既节省空间，又不用额外增加太多的查询成本。**

我们在建立索引时关注的是区分度，区分度越高越好。因为区分度越高，意味着重复的键值越少。因此，我们可以通过统计索引上有多少个不同的值来判断要使用多长的前缀。

1. 可以使用`mysql> select count(distinct email) as L from SUser;`算出列上有多少个不同值
2. 依次选取不同长度的前缀来查看这个值。

		mysql> select 
		  count(distinct left(email,4)）as L4,
		  count(distinct left(email,5)）as L5,
		  count(distinct left(email,6)）as L6,
		  count(distinct left(email,7)）as L7,
		from SUser;

3. 预设一个可接受的损失区分度的比例

### 前缀索引对覆盖索引的影响 ###

使用前缀索引可能会增加扫描行数，这会影响到性能。使用前缀索引就用不上覆盖索引对查询性能的优化。

### 其他方式 ###

遇到前缀的区分度不够好的情况时：

1. 使用倒序存储。
	* 如果存储身份证号的时候倒过来存，每次查询的时候，也这么写：
    mysql> select field_list from t where id_card = reverse('input_id_card_string');
2. 使用hash字段。
	* 可以在表上再创建一个整数字段，来保存身份证的校验码，同时在这个字段上创建索引。
    mysql> alter table t add id_card_crc int unsigned, add index(id_card_crc);
	* 插入新记录的时候，都同时使用crc32()函数得到校验码填到这个新字段。

### 小结 ###

1. 直接创建完整索引，比较占用空间
2. 创建前缀索引，节省空间，但会增加查询扫描次数，并且不能使用覆盖索引
3. 倒序存储，再创建前缀索引，用于绕过字符串本身前缀的区分度不够的问题
4. 创建hash字段索引，查询性能稳定，有额外的存储和计算消耗，跟第三种方式一样，都不支持范围扫描

### 精选留言 ###

#### Q ####

如果你在维护一个学校的学生信息数据库，学生登录名的统一格式是”学号 @gmail.com", 而学号的规则是：十五位的数字，其中前三位是所在城市编号、第四到第六位是学校编号、第七位到第十位是入学年份、最后五位是顺序编号。系统登录的时候都需要学生输入登录名和密码，验证正确后才能继续使用系统。就只考虑登录验证这个行为的话，你会怎么设计这个登录名的索引呢？

#### A ####

1. 第7到第10位是入学年份，最后5位是顺序编号。考虑到前缀索引区分度不大，可以使用倒序索引进行区分。倒序8位。
2. 原谅我偷懒的想法，一个学校每年预估2万新生，50年才100万记录，能节省多少空间，直接全字段索引。省去了开发转换及局限性风险，碰到超大量迫不得已再用后两种办法

## 12 | 为什么我的MySQL会“抖”一下？ ##

*你的SQL语句为什么变“慢”了*

WAL机制：InnoDB 在处理更新语句的时候，只做了写日志这一个磁盘操作。这个日志叫作 redo log（重做日志）

**当内存数据页跟磁盘数据页内容不一致的时候，我们称这个内存页为“脏页”。内存数据写入到磁盘后，内存和磁盘上的数据页的内容就一致了，称为“干净页”。**

### InnoDB 刷脏页的控制策略 ###

用到 innodb_io_capacity 这个参数了，告诉 InnoDB 你的磁盘能力。这个值我建议你设置成磁盘的 IOPS。磁盘的 IOPS 可以通过 fio 这个工具来测试，下面的语句是我用来测试磁盘随机读写的命令：

	fio -filename=$filename -direct=1 -iodepth 1 -thread -rw=randrw -ioengine=psync -bs=16k -size=500M -numjobs=10 -runtime=10 -group_reporting -name=mytest 

**如果你来设计策略控制刷脏页的速度，会参考哪些因素呢？**

1. 首先是内存脏页太多，其次是 redo log 写满。
2. InnoDB 的刷盘速度就是要参考这两个因素：一个是脏页比例，一个是 redo log 写盘速度。

InnoDB 会根据这两个因素先单独算出两个数字：

参数 innodb_max_dirty_pages_pct 是脏页比例上限，默认值是 75%。

在 MySQL 8.0 中，innodb_flush_neighbors 参数的默认值已经是 0 了。

### 小结 ###

解释了这个机制后续需要的刷脏页操作和执行时机。利用 WAL 技术，数据库将随机写转换成了顺序写，大大提升了数据库的性能。

由此也带来了内存脏页的问题。脏页会被后台线程自动 flush，也会由于数据页淘汰而触发 flush，而刷脏页的过程由于会占用资源，可能会让你的更新和查询语句的响应时间长一些。

### 精选留言 ###


#### 1 ####

**Q**
当内存不够用了，要将脏页写到磁盘，会有一个数据页淘汰机制（最久不使用），假设淘汰的是脏页，则此时脏页所对应的redo log的位置是随机的，当有多个不同的脏页需要刷，则对应的redo log可能在不同的位置，这样就需要把redo log的多个不同位置刷掉，这样对于redo log的处理不是就会很麻烦吗？（合并间隙，移动位置？）
另外，redo log的优势在于将磁盘随机写转换成了顺序写，如果需要将redo log的不同部分刷掉（刷脏页），不是就在redo log里随机读写了么？

**A**
* 其实由于淘汰的时候，刷脏页过程不用动redo log文件的。
* 这个有个额外的保证，是redo log在“重放”的时候，如果一个数据页已经是刷过的，会识别出来并跳过。

#### 2 ####

redo log是关系型数据库的核心啊,保证了ACID里的D。所以redo log是牵一发而动全身的操作
按照老师说的当内存数据页跟磁盘数据页不一致的时候,把内存页称为'脏页'。如果redo log
设置得太小,redo log写满.那么会涉及到哪些操作呢,我认为是以下几点:
1. 把相对应的数据页中的脏页持久化到磁盘,checkpoint往前推
2. 由于redo log还记录了undo的变化,undo log buffer也要持久化进undo log
3. 当innodb_flush_log_at_trx_commit设置为非1,还要把内存里的redo log持久化到磁盘上
4. redo log还记录了change buffer的改变,那么还要把change buffer purge到idb
以及merge change buffer.merge生成的数据页也是脏页,也要持久化到磁盘
上述4种操作,都是占用系统I/O,影响DML,如果操作频繁,会导致'抖'得向现在我们过冬一样。
但是对于select操作来说,查询时间相对会更快。因为系统脏页变少了,不用去淘汰脏页,直接复用
干净页即可。还有就是对于宕机恢复,速度也更快,因为checkpoint很接近LSN,恢复的数据页相对较少
所以要控制刷脏的频率,频率快了,影响DML I/O,频率慢了,会导致读操作耗时长。
我是这样想的这个问题,有可能不太对,特别是对于第4点是否会merge以及purge,还需要老师的解答

## 13 | 为什么表数据删掉一半，表文件大小没变 ##

数据库表的空间回收

一个 InnoDB 表包含两部分，即：表结构定义和数据。在 MySQL 8.0 版本以前，表结构是存在以.frm 为后缀的文件里。而 MySQL 8.0 版本，则已经允许把表结构定义放在系统数据表中了。因为表结构定义占用的空间很小。

### 参数innodb_file_per_table ###

数据既可以存在共享表空间里，也可以是单独的文件。这个行为是由参数 innodb_file_per_table 控制的：

1. 这个参数设置为 OFF 表示的是，表的数据放在系统共享表空间，也就是跟数据字典放在一起；即使表删掉了，空间也是不会回收的
2. 这个参数设置为 ON 表示的是，每个 InnoDB 表数据存储在一个以 .ibd 为后缀的文件中；通过drop table命令，系统就会直接删除这个文件。

从MySQL5.6.6版本开始，它的默认值就是ON了。

*将 innodb_file_per_table 设置为 ON，是推荐做法，我们接下来的讨论都是基于这个设置展开的。*

删除表的时候，可以使用drop table命令来回收空间；但删除数据的场景往往是删除某些行（表空间却没有被回收）

### 数据删除流程 ###

InnoDB 里的数据都是用 B+ 树的结构组织的。

如果删掉R4这个记录，InnoDB引擎只会把R4这个记录标记为删除。InnoDB的数据是按页存储的，那么删掉了一个数据页上的所有记录，则整个数据页就可以被复用了。

数据页的复用跟记录的复用是不同的。

不止是删除数据会造成空洞，插入数据也会。

### 重建表 ###

如果现在有一个表A，需要做空间收缩，为了把表中存在的空洞去掉。

* 可以新建一个与表 A 结构相同的表 B，然后按照主键 ID 递增的顺序，把数据一行一行地从表 A 里读出来再插入到表 B 中。
* 

使用 alter table A engine=InnoDB 命令来重建表。

### Online 和 inplace ###



### 小结 ###

数据库中收缩表空间的方法。

1. 如果要收缩一个表，只是 delete 掉表里面不用的数据的话，表文件的大小是不会变的。
2. 通过 alter table 命令重建表，才能达到表文件变小的目的。

重建表的两种实现方式：

* Online DDL 的方式是可以考虑在业务低峰期使用的
* MySQL 5.5 及之前的版本，这个命令是会阻塞 DML 的

### 精选留言 ###

#### Q ####

假设现在有人碰到了一个“想要收缩表空间，结果适得其反”的情况，看上去是这样的：

1. 一个表 t 文件大小为 1TB；
2. 对这个表执行 alter table t engine=InnoDB；
3. 发现执行完成后，空间不仅没变小，还稍微大了一点儿，比如变成了 1.01TB。

#### A ####

#### Q ####

#### A ####

## 14 | count(*)这么慢，我该怎么办？ ##

### count(*)的实现方式 ###

在不同的MySQL引擎中，count(*)有不同的实现方式。

* MyISAM引擎把一个表的总行数存在了磁盘上，因此执行count(*)的时候会直接返回这个数，效率很高；
* 而InnoDB引擎执行count(*)的时候，需要把数据一行一行地从引擎里面读出来，然后累积计数。

事务支持、并发能力还是在数据安全方面，InnoDB都优于MyISAM

#### 为什么InnoDB不跟MyISAM一样，也把数字存起来呢？ ####

因为即使在同一时刻的多个查询，由于多版本并发控制（MVCC）的原因，InnoDB表“应该返回多少行”也是不确定的。

假如设计三个用户并行的会话，可能出现拿到的结果却不同。因为InnoDB的事务设计有关系，可重复读是默认的隔离级别。每一行记录都要判断自己是否对这个会话可见，因此对于count(*)请求来说，InnoDB只好把数据一行一行读出来依次判断，可见的行才能够用于计算“基于这个查询”的表的总行数

MySQL在执行count(*)操作的时候做了优化。InnoDB是索引组织表，主键索引树的叶子节点是数据，而普通索引树比主键索引树少很多。对于count(*)这样的操作，遍历哪个索引数得到的结果逻辑上都是一样的。MySQL优化器会找到最小的那棵树来遍历。在保证逻辑正确的前提下，尽量减少扫描的数据量，是数据库系统设计的通用法则之一。

show table status，输出结果里面有一个TABLE_ROWS用于显示这个表当前有多少行。（show table status命令显示的行数也不能直接使用）

### 小结 ###

* MyISAM表虽然count(*)很快，但是不支持事务；
* show table status 命令虽然返回很快，但是不准确；
* InnoDB 表直接 count(*) 会遍历全表，虽然结果准确，但会导致性能问题。

### 用缓存系统保存计数 ###

将计数保存在缓存系统中的方式，还不只是丢失更新的问题。即使Redis正常工作，这个值还是逻辑上不精确的。

1. 缓存可能会丢失更新，
2. 逻辑上值不精确

### 在数据库保存计数 ###

把计数直接放到数据库单独里的一张计数表C中

1. 解决了崩溃丢失的问题，InnoDB是支持崩溃恢复不丢数据的
2. 利用InnoDB的事务特性解决查计数值和“最近100条记录”看到的结果

### 不同的count用法 ###

count(*)、count(主键 id)、count(字段) 和 count(1) 等不同用法的性能

1. count()是一个聚合函数，对于返回的结果集，一行行地判断，如果count函数的参数不是NULL，累计值就加1，否则不加。最后返回累计值。

至于分析性能差别，可以记住这几个原则：

1. server层要什么就给什么；
2. InnoDB 只给必要的值；
3. 现在的优化器只优化了 count(*) 的语义为“取行数”，其他“显而易见”的优化并没有做。

---

* 对于 count(主键 id) 来说：
* 对于 count(1) 来说：
* 对于 count(字段) 来说：
	1. 如果这个“字段”是定义为not null的话，一行行地从记录里面读出这个字段，判断不能为null，按行累加；
	2. 如果这个“字段”定义为null，那么执行的时候，判断到有可能是null，还要把值取出来再判断一下，不是null才累加。
* 但是 count(*) 是例外：不会把全部字段取出来，专门做了优化，不取值。count(*)肯定不是null，按行累加。

按照效率排序的话，count(字段)<count(主键 id)<count(1)≈count(*)，所以我建议你，尽量使用 count(*)。

### 小结 ###

1. 不同引擎中count(*)的实现方式不一样
2. 用缓存系统来存储计数值存在问题——这两个不同的存储构成的系统，不支持分布式事务，无法拿到精确一致的视图
3. 把计数值放在MySQL中，解决一致性视图问题

InnoDB引擎支持事务，利用好事务的原子性和隔离性，简化在业务开发时的逻辑。


## 15 | 答疑文章（一）：日志和索引相关问题 ##

### 日志相关问题 ###

#### 追问 1：MySQL 怎么知道 binlog 是完整的? ####

一个事务的binlog是有完整格式的：

* statement格式的binlog，最后会有COMMIT；
* row歌是的binlog，最后会有一个XID event。

在MySQL5.6.2版本以后，还引入了binlog-checksum参数，用来验证binlog内容的正确性。

#### 追问 2：redo log 和 binlog 是怎么关联起来的? ####

有一个共同的数据字段，叫XID。崩溃恢复的时候，会按顺序扫描 redo log：

* 如果碰到既有 prepare、又有 commit 的 redo log，就直接提交；
* 如果碰到只有 parepare、而没有 commit 的 redo log，就拿着 XID 去 binlog 找对应的事务。

追问 3：处于 prepare 阶段的 redo log 加上完整 binlog，重启就能恢复，MySQL 为什么要这么设计?

追问 4：如果这样的话，为什么还要两阶段提交呢？干脆先 redo log 写完，再写 binlog。崩溃恢复的时候，必须得两个日志都完整才可以。是不是一样的逻辑？

**追问 5：不引入两个日志，也就没有两阶段提交的必要了。只用 binlog 来支持崩溃恢复，又能支持归档，不就可以了？**

只保存binlog，然后把提交流程改成：...->“数据更新到内存”->“写binlog”->“提交事务”

**追问 6：那能不能反过来，只用 redo log，不要 binlog？**

**追问 7：redo log 一般设置多大？**

### 小结 ###

### 精选留言 ###

#### Q ####

	mysql> CREATE TABLE `t16` (
	`id` int(11) NOT NULL primary key auto_increment,
	`a` int(11) DEFAULT NULL
	) ENGINE=InnoDB;
	insert into t16 values(1,2);
	
	mysql> update t16 set a=2 where id=1;

#### A ####




## 16 | "order by"是怎么工作的？ ##

表定义

	CREATE TABLE `t16` (
	  `id` int(11) NOT NULL,
	  `city` varchar(16) NOT NULL,
	  `name` varchar(16) NOT NULL,
	  `age` int(11) NOT NULL,
	  `addr` varchar(128) DEFAULT NULL,
	  PRIMARY KEY (`id`),
	  KEY `city` (`city`)
	) ENGINE=InnoDB;


	select city,name,age from t16 where city='杭州' order by name limit 1000 ;

### 全字段排序 ###

为避免全表扫描，需要再city字段加上索引。在city字段上创建索引之后，用explain命令来看看这个语句的执行情况。

1. 初始化sort_buffer，确定放入name、city、age这三个字段；
2. 从索引city找到第一个满足city='杭州'条件的主键id，也就是图中的ID_X；
3. 到主键id索引取出整行，取name、city、age这三个字段的值，存入sort_buffer中；
4. 从索引city取下一个记录的主键id；
5. 重复步骤 3、4 直到 city 的值不满足查询条件为止，对应的主键 id 也就是图中的 ID_Y；
6. 对 sort_buffer 中的数据按照字段 name 做快速排序；
7. 按照排序结果取前 1000 行返回给客户端。

sort_buffer_size，就是MySQL为排序开辟的内存（sort_buffer）的大小。如果要排序的数据量小于sort_buffer_size，排序就在内存中完成。但如果排序数据量太大，内存放不下，则不得不利用磁盘临时文件辅助排序。

MySQL将需要排序的数据分成12份，每一份单独排序后存在这些临时文件种。然后把12个有序文件再合并成一个有序的大文件。

OPTIMIZER_TRACE 

* filesort_summary
	* rows:
	* examined_rows：表示参与排序的的行数
	* number_of_tmp_files
	* sort_buffer_size
		* sort_buffer_size超过了需要排序的数据量的大小，number_of_tmp_files就是0，表示排序可以直接在内存中完成
		* 否则需要放在临时文件加中排序。sort_buffer_size 越小，需要分成的份数越多，number_of_tmp_files 的值就越大
	* sort_mode 

### rowid排序 ###

如果MySQL认为排序的单行长度太长会怎么做呢？

	SET max_length_for_sort_data = 16;

max_length_for_sort_data，是MySQL中专门控制用于排序的行数据的长度的一个参数。

1. 初始化sort_buffer，确定放入name和id；
2. 从索引city找到第一个满足city='杭州'条件的主键id，也就是图中的ID_X；
3. 到主键id索引取出整行，取name、id这两个字段的值，存入sort_buffer中；
4. 从索引city取下一个记录的主键id；
5. 重复步骤 3、4 直到 city='杭州' 条件为止，也就是 ID_Y；
6. 对 sort_buffer 中的数据按照字段 name 做快速排序；
7. 按照排序结果取前 1000 行返回给客户端。

### 全字段排序 VS rowid 排序 ###

1. 如果MySQL担心排序内存太小，会影响排序效率，才会采用rowid排序算法。这样排序过程一次可以排序更多行，但是需要再回到原表里取数据。
2. 如果MySQL认为内存足够大，会优先选择全字段排序，把需要的字段都放到sort_buffer中，这样排序后会直接从内存里面返回查询结果了，不用再回到原表中去取数据。

如果内存够，就要多利用内存，尽量减少磁盘访问。对于InnoDB表来说，rowid排序会要求回表多造成磁盘读，因此不会被优先选择。

覆盖索引是指，索引上的信息足够满足查询请求，不需要再回到主键索引上去取数据。

每个查询都用上覆盖索引，就要把语句中涉及的字段都建上联合索引，毕竟索引还是有维护代价。需要权衡决定。

## 17 | 如何正确地显示随机消息 ##

	mysql> CREATE TABLE `t17` (
	  `id` int(11) NOT NULL AUTO_INCREMENT,
	  `word` varchar(64) DEFAULT NULL,
	  PRIMARY KEY (`id`)
	) ENGINE=InnoDB;
	
	delimiter ;;
	create procedure idata_t17()
	begin
	  declare i int;
	  set i=0;
	  while i<10000 do
	    insert into t17(word) values(concat(char(97+(i div 1000)), char(97+(i % 1000 div 100)), char(97+(i % 100 div 10)), char(97+(i % 10))));
	    set i=i+1;
	  end while;
	end;;
	delimiter ;

	call idata_t17();

### 内存临时表 ###

	mysql> explain select word from words order by rand() limit 3;

Extra 字段显示 Using temporary，表示的是需要使用临时表；Using filesort，表示的是需要执行排序操作。(需要临时表，并且需要在临时表上排序)

对于 InnoDB 表来说，执行全字段排序会减少磁盘访问，因此会被优先选择；对于内存表，回表过程只是简单地根据数据行的位置，直接访问内存得到数据，根本不会导致多访问磁盘。

1. 创建一个临时表。这个临时表使用的是memory引擎，表里有两个字段，第一个字段是 double 类型，第二个字段是 varchar(64) 类型。并且，这个表没有建索引。
2. 从words，按主键顺序取出所有的 word 值。对于每一个 word 值，调用 rand() 函数生成一个大于 0 小于 1 的随机小数，并把这个随机小数和 word 分别存入临时表的 R 和 W 字段中，到此，扫描行数是 10000。
3. 现在临时表有 10000 行数据了，接下来你要在这个没有索引的内存临时表上，按照字段 R 排序。
4. 初始化 sort_buffer。sort_buffer 中有两个字段，一个是 double 类型，另一个是整型。
5. 从内存临时表中一行一行地取出 R 值和位置信息，分别存入 sort_buffer 中的两个字段里。这个过程要对内存临时表做全表扫描，此时扫描行数增加 10000，变成了 20000。
6. 在 sort_buffer 中根据 R 的值进行排序。注意，这个过程没有涉及到表操作，所以不会增加扫描行数。
7. 排序完成后，取出前三个结果的位置信息，依次到内存临时表中取出 word 值，返回给客户端。这个过程中，访问了表的三行数据，总扫描行数变成了 20003。

	# Query_time: 0.900376  Lock_time: 0.000347 Rows_sent: 3 Rows_examined: 20003
	SET timestamp=1541402277;
	select word from words order by rand() limit 3;


### 磁盘临时表 ###

tmp_table_size这个配置限制了内存临时表的大小，默认值是16M。

### 随机排序方法 ###



### 小结 ###

如果你直接使用 order by rand()，这个语句需要 Using temporary 和 Using filesort，查询的执行代价往往是比较大的。所以，在设计的时候你要尽量避开这种写法。

在实际应用的过程中，比较规范的用法就是：尽量将业务逻辑写在业务代码中，让数据库只做“读写数据”的事情。



## 18 | 为什么这些SQL语句逻辑相同，性能却差异巨大？ ##

## 19 | 为什么我只查一行的语句，也执行这么慢？ ##

	mysql> CREATE TABLE `t19` (
	  `id` int(11) NOT NULL,
	  `c` int(11) DEFAULT NULL,
	  PRIMARY KEY (`id`)
	) ENGINE=InnoDB;
	
	delimiter ;;
	create procedure t19idata()
	begin
	  declare i int;
	  set i=1;
	  while(i<=100000) do
	    insert into t values(i,i);
	    set i=i+1;
	  end while;
	end;;
	delimiter ;
	
	call t19idata();

### 第一类：查询长时间不返回 ###

	mysql> select * from t19 where id=1;

碰到这种情况

1. 表t被锁住了
2. 首先执行一下show processlist命令查看原因
3. 针对每种状态，分析他们产生的原因、如何复现，以及如何处理

### 等MDL锁 ###
使用`show processlist`命令查看`Waiting for table metadata lock`

如果出现`Sleep`*这个状态表示的是，现在有一个线程正在表 t 上请求或者持有 MDL 写锁，把 select 语句堵住了。*

|session A|session B|
|--|--|
|lock table t19 write;||
||select * from t where id=1;|

session A通过lock table命令持有表t的MDL写锁，而session B的查询需要获取MDL读锁。所以，session B进入等待状态。通过找到谁持有MDL写锁，然后kill掉解决问题。

**解决方法**

* 通过show processlist的结果里面，session A的Command列是“Sleep”，导致查询不方便
* 通过查询`sys.schema_table_lock_waits`这张表，直接找出造成阻塞的process id，kill掉断开连接即可

	select blocking_pid from sys.schema_table_lock_waits;

可以直接在查询界面进行删除内容

### 等flush ###

	mysql> select * from information_schema.processlist where id=1;
查出来这个线程的状态是 Waiting for table flush

	-- 只关闭表t
	flush tables t with read lock;
	-- 关闭MySQL里所有打开的表
	flush tables with read lock;
正常这两个语句执行起来比较快，但可能也被其他的线程堵住了；出现`Waiting for table flush`状态可能是：有一个flush tables命令被别的语句堵住了，然后它又堵住了select语句。

### 等行锁 ###
	mysql> select * from t where id=1 lock in share mode;
由于访问id=1这个记录时要加读锁，如果这时候已经有一个事务在这行记录上持有一个写锁。select语句就会被堵住。 

|session A|session B|
|--|--|
|begin;||
|update t19 set c=c+1 where id=1;||
||select * from t where id=1;|

查询谁占着写锁，通过`sys.innodb_lock_waits`表查到
	mysql> select * from t sys.innodb_lock_waits where locked_table=`'test'.'t'`\G

    KILL4和KILL QUERY 4
TODO kill query4 

### 查询慢 ###

	mysql> select * from t where c=50000 limit 1;

由于字段c上没有索引，只能走id主键顺序查询，因此需要扫描5万行。 

*坏查询不一定是慢查询*

### 小结 ###

select * from t where id=1 lock in share mode。由于 id 上有索引，所以可以直接定位到 id=1 这一行，因此读锁也是只加在了这一行上。

	begin;
	select * from t where c=5 for update;
	commit;

### 精选留言 ###

## 20 | 幻读是什么，幻读有什么问题？ ##

	CREATE TABLE `t20` (
	  `id` int(11) NOT NULL,
	  `c` int(11) DEFAULT NULL,
	  `d` int(11) DEFAULT NULL,
	  PRIMARY KEY (`id`),
	  KEY `c` (`c`)
	) ENGINE=InnoDB;
	
	insert into t20 values(0,0,0),(5,5,5),
	(10,10,10),(15,15,15),(20,20,20),(25,25,25);
这个表除了主键 id 外，还有一个索引 c，初始化语句在表中插入了 6 行数据。

### 幻读是什么？ ###

||session A|session B|session C|
|--|--|--|--|
|T1|begin;|||
||select * from t20 where d=5 for update; /*Q1*/|||
||result:(5,5,5)|||
|T2||update t20 set d=5 ||
|||where id=0;||
|T3|select * from t20 where d=5 fro update; /*Q2*/|||
||result:(0,0,5),(5,5,5)|||
|T4|||insert into t20 values(1, 1, 5)|
|T5|select * from t20 where d=5 for update; /*Q3*/|||
||result:(0,0,5),(1,1,5),(5,5,5)|||
|T6|commit;|||

幻读指的是一个事务在前后两次查询同一个范围的时候，后一次查询看到了前一次查询没有看到的行。

幻读：

1. 在可重复读隔离级别下，普通的查询是快照读，是不会看到别的事务插入的数据的。因此，幻读在“当前读”下才会出现。
2. 上面 session B 的修改结果，被 session A 之后的 select 语句用“当前读”看到，不能称为幻读。幻读仅专指“新插入的行”。



### 幻读有什么问题？ ###

1. 语义上
2. 数据一致性的问题：锁的设计是为了保证数据的一致性。而这个一致性，不止是数据库内部数据状态在此刻的一致性，还包含了数据和日志在逻辑上的一致性。

即使把所有的记录都加上锁，还是阻止不了新插入的记录。

### 如何解决幻读 ###

||读锁|写锁|
|读锁|兼容|冲突|
|写锁|冲突|冲突|

产生幻读的原因是，行锁只能锁住行，但是新插入记录这个动作，要更新的是记录之间的“间隙”。

间隙锁，锁的就是两个值之间的空隙。比如文章开头的表 t20，初始化插入了 6 个记录，这就产生了 7 个间隙。

跟间隙锁存在冲突关系的，是“往这个间隙中插入一个记录”这个操作。间隙锁之间都不存在冲突关系。

间隙锁和行锁合称next-key lock，每个next-ket lock是前开后闭区间。我们的表t20初始化以后，如果用select * from t20 for update要把整个表所有记录锁起来，就形成了7个next-key lock，分别是(负无穷,0]、(0,5]、(5,10]、(10,15]、(15,20]、(20,25]、(25,+supernum]。

间隙锁和next-key lock的引入，解决幻读的问题，但同时也带来了一些“困扰”。

### 小结 ###

间隙锁：给所有的行都加上行锁，仍然无法解决幻读。就是生产库上会经常出现由于间隙锁导致的死锁现象。行锁确实比较直观，判断规则也相对简单，间隙锁的引入会影响系统的并发度，也增加了锁分析的复杂度，但也有章可循。

### 精选留言 ###

#### Q ####

session B和session C的insert语句都会进入锁等待状态。

#### A ####

间隙锁

TODO

## 21 | 为什么我只改一行的语句锁这么多 ##

1. MySQL 5.x 系列 <=5.7.24，8.0 系列 <=8.0.13
2. 间隙锁在可重复读隔离级别下才有效

我总结的加锁规则里面，包含了两个“原则”、两个“优化”和一个“bug”。

1. 原则1：加锁的基本单位是next-key lock。
2. 原则2：查找过程中访问到的对象才会加锁。
3. 优化1：索引上的等值查询，给唯一索引加锁的时候，next-key lock退化为行锁。
4. 优化2：索引上的等值查询，向右遍历时且最后一个值不满足等值条件

	CREATE TABLE `t21` (
	  `id` int(11) NOT NULL,
	  `c` int(11) DEFAULT NULL,
	  `d` int(11) DEFAULT NULL,
	  PRIMARY KEY (`id`),
	  KEY `c` (`c`)
	) ENGINE=InnoDB;
	
	insert into t21 values(0,0,0),(5,5,5),
	(10,10,10),(15,15,15),(20,20,20),(25,25,25);

### 案例一：等值查询间隙锁 ###

|session A|session B|session C|
|--|--|--|
|begin;|||
|update t21 set d=d+1 where id=7|||
||insert into t21 values(8,8,8);(block)||
|||update t21 set d=d+1 where id=10;(Query OK)|

1. 根据原则1，加锁单位是next-key lock，session A加锁范围就是(5,10]
2. 同时根据优化2，这是一个等值查询（id=7），而id=10不满足查询条件，next-key lock退化成间隙锁，因此最终加锁的范围是（5,10)

### 案例二：非唯一索引等值锁 ###

* lock in share mode只锁覆盖索引
* for update给主键索引上满足条件的行加上行锁

### 案例三：主键索引范围锁 ###

### 案例四：非唯一索引范围锁 ###

### 案例五：唯一索引范围锁 bug ###

### 案例六：非唯一索引上存在"等值"的例子 ###

### 案例七：limit语句加锁 ###

|session A|session B|
|--|--|
|begin;||
|delete from t where c=10 limit 2;||
||insert into t values(12,12,12);|
||(Query OK)|

在删除数据的时候尽量加 limit。不仅可以控制删除数据的条数，还可以减少加锁的范围。

### 案例八：一个死锁的例子 ###

|session A|session B|
|--|--|
|begin;||
|select id from t where c=10 lock in share mode;||
||update t set d=d+1 where c=10;(blocked)|
|insert into values(8,8,8);||
||ERROR 1213|

1. sessin A启动事务后查询语句加lock in share mode，在索引c
2. 上加了next-key lock(5,10]和间隙锁(10,15)

### 小结 ###

可重复读隔离级别 (repeatable-read) 下验证的。可重复读隔离级别遵守两阶段锁协议，所有加锁的资源，都是在事务提交或者回滚的时候才释放的。

在最后的案例中，你可以清楚地知道 next-key lock 实际上是由间隙锁加行锁实现的。如果切换到读提交隔离级别 (read-committed) 的话，过程中去掉间隙锁的部分，也就是只剩下行锁的部分。

在读提交隔离级别下还有一个优化，即：语句执行过程中加上的行锁，在语句执行完成后，就要把“不满足条件的行”上的行锁直接释放，不需要等到事务提交。

读提交隔离级别下，锁的范围更小，锁的时间更短。

## 22 | MySQL有哪些“饮鸩止渴”提高性能的方法？ ##

## 23 | MySQL是怎么保证数据不丢的？ ##

只要redo log和binlog保证持久化到磁盘，就能确保MySQL异常重启，数据可以回复。

### binlog的写入机制 ###

事务执行过程中，先把日志写到binglog cache，事务提交的时候，再把binlog cache写道binlog文件中。

一个事务的binlog不能被拆开的，因此无论这个事务多大，也要确保一次性写入。——binlog cache的保存问题

系统给 binlog cache 分配了一片内存，每个线程一个，参数 binlog_cache_size 用于控制单个线程内 binlog cache 所占内存的大小。如果超过了这个参数规定的大小，就要暂存到磁盘。

事务提交的时候，执行器把 binlog cache 里的完整事务写入到 binlog 中，并清空 binlog cache。

每个线程有自己的binlog cache，但是共用同一份binlog文件。

### redo log的写入机制 ###

redo log buffer：事务在执行过程中，生成的redo log是要先写到redo log buffer的。

redo log buffer里面的内容不会每次生成后直接持久化到磁盘。

![9d057f61d3962407f413deebc80526d4.png](img/9d057f61d3962407f413deebc80526d4.png)

redo log存在的三种状态：

* 存在redo log buffer中，物理上是在MySQL进程内存中，图中红色部分（快）
* 写到磁盘（write），但是没有持久化（sync），物理上是在文件系统的page cache里面，图中黄色部分（快）
* 持久化到磁盘，对应的是hard disk，图中绿色部分（慢）

为了控制redo log的写入策略，InnoDB提供了innodb_flush_log_at_trx_commit参数，有三种可能取值：

1. 设置为0的时候，表示每次事务提交时都只是把redo log留在redo log buffer中；
2. 设置为1的时候，表示每次事务提交都将redo log直接持久化到磁盘；
3. 设置为2的时候，表示每次事务提交时都只是把redo log写到page cache。

WAL机制主要得益于l两个方面：

1. redo log和binlog都是顺序写，磁盘的顺序写比随机写速度要快
2. 组提交机制，可以大幅度降低磁盘的IOPS消耗


### 小结 ###



## 24 | MySQL是怎么保证主备一致的？ ##

binlog可以用来归档，也可以用来主备同步。

### MySQL主备的基本原理 ###

在状态 1 中，客户端的读写都直接访问节点 A，而节点 B 是 A 的备库，只是将 A 的更新都同步过来，到本地执行。这样可以保持节点 B 和 A 的数据是相同的。

当需要切换的时候，就切成状态 2。这时候客户端读写访问的都是节点 B，而节点 A 是 B 的备库。

在状态 1 中，虽然节点 B 没有被直接访问，但是我依然建议你把节点 B（也就是备库）设置成只读（readonly）模式。这样做，有以下几个考虑：

1. 有时候一些运营类的查询语句会被放到备库上去查，设置为只读可以防止误操作；
2. 防止切换逻辑有 bug，比如切换过程中出现双写，造成主备不一致；
3. 可以用 readonly 状态，来判断节点的角色。

### binlog的三种格式对比 ###

1. statement
2. row
3. mixed，其实就是前两种格式的混合。

	mysql> CREATE TABLE `t24` (
	  `id` int(11) NOT NULL,
	  `a` int(11) DEFAULT NULL,
	  `t_modified` timestamp NOT NULL DEFAULT CURRENT_TIMESTAMP,
	  PRIMARY KEY (`id`),
	  KEY `a` (`a`),
	  KEY `t_modified`(`t_modified`)
	) ENGINE=InnoDB;

	insert into t24 values(1,1,'2018-11-13');
	insert into t24 values(2,2,'2018-11-12');
	insert into t24 values(3,3,'2018-11-11');
	insert into t24 values(4,4,'2018-11-10');
	insert into t24 values(5,5,'2018-11-09');

### 为什么会右mixed格式的binlog？ ###

为什么会有 mixed 这种 binlog 格式的存在场景？

* 因为有些 statement 格式的 binlog 可能会导致主备不一致，所以要使用 row 格式。
* 但 row 格式的缺点是，很占空间。比如你用一个 delete 语句删掉 10 万行数据，用 statement 的话就是一个 SQL 语句被记录到 binlog 中，占用几十个字节的空间。但如果用 row 格式的 binlog，就要把这 10 万条记录都写到 binlog 中。（不仅会占用更大的空间，同时写 binlog 也要耗费 IO 资源，影响执行速度。）
* MySQL 就取了个折中方案，也就是有了 mixed 格式的 binlog。mixed 格式的意思是，MySQL 自己会判断这条 SQL 语句是否可能引起主备不一致，如果有可能，就用 row 格式，否则就用 statement 格式。

### 循环复制问题 ###

binlog 的特性确保了在备库执行相同的 binlog，可以得到与主库相同的状态。

### 小结 ###

介绍了MySQL binlog的格式和一些基本机制，时后面要介绍读写分离等系列文章的背景知识

MySQL高可用方案的基础，演化出了诸如多节点、半同步、MySQL group replication等相对复杂的方案。

### 精选留言 ###

我们说 MySQL 通过判断 server id 的方式，断掉死循环。但是，这个机制其实并不完备，在某些场景下，还是有可能出现死循环。

## 25 | MySQL时怎么保证高可用的？ ##

在一个主备关系中，每个备库接收主库的binlog并执行。正常情况下，只要主库执行更新生成的所有 binlog，都可以传到备库并被正确地执行，备库就能达到跟主库一致的状态，这就是最终一致性。

![MySQL 主备切换流程 -- 双 M 结构(/img/89290bbcf454ff9a3dc5de42a85a69cc.png)

### 主备延迟 ###

主动操作和被动操作

主动切换的场景：

在介绍主动切换流程的详细步骤之前，先说明一个概念，即“同步延迟”。与数据有关的时间点主要包括以下三个：

1. 主库 A 执行完成一个事务，写入 binlog，我们把这个时刻记为 T1;
2. 之后传给备库 B，我们把备库 B 接收完这个 binlog 的时刻记为 T2;
3. 备库 B 执行完成这个事务，我们把这个时刻记为 T3。

所谓主备延迟，就是同一个事务，在备库执行完成

### 主备延迟的来源 ###

1. 有些部署条件下，备库所在机器的性能要比主库所在的机器性能差。
2. 常见的可能了，即备库的压力大
3. 大事务
4. 大表



### 可靠性优先策略 ###

### 可用性优先策略 ###

### 小结 ###

MySQL高可用系统的基础，主备且换逻辑。

1. 几种主备延迟的情况
2. 主备延迟的存在，且换策略就有不同的选择
3. 可靠性优先和可用性优先策略的不同

在实际的应用中，我更建议使用可靠性优先的策略。毕竟保证数据准确，应该是数据库服务的底线。在这个基础上，通过减少主备延迟，提升系统的可用性。

### 精选留言 ###

#### Q ####

#### A ####

## 26 | 备库为什么会延迟好几个小时？ ##

## 27 | 主库出问题了，从库怎么办？ ##

## 28 | 读写分离有哪些坑？ ##

读写分离的主要目标是分摊主库的压力。

1. 客户端（client）主动做负载均衡
2. 在 MySQL 和客户端之间有一个中间代理层 proxy，客户端只连接 proxy， 由 proxy 根据请求类型和上下文决定请求的分发路由。

客户端直连和带proxy的读写分离架构

疑问：由于主从可能存在延迟，客户端执行完一个更新事务后马上发起查询，如果查询选择的是从库的话，就有可能读到刚刚的事务更新之前的状态。

**这种“在从库上会读到系统的一个过期状态”的现象，在这篇文章里，我们暂且称之为“过期读”。**

过期读的解决方法：

* 强制走主库方案；
* sleep 方案；
* 判断主备无延迟方案；
* 配合 semi-sync 方案；
* 等主库位点方案；
* 等 GTID 方案。

### 强制走主库方案 ###

将查询请求做分类，将查询请求分为两类：

* 对于必须要拿到最新结果的请求，强制将其发到主库上。
* 对于可以读到旧数据的请求，才将其发到从库上。

### Sleep 方案 ###

主库更新后，读从库之前先 sleep 一下。具体的方案就是，类似于执行一条 select sleep(1) 命令。

### 判断主备无延迟方案 ###

`show slave status` 结果里的 seconds_behind_master 参数的值，可以用来衡量主备延迟时间的长短。

1. 第一种确保主备无延迟的方法是：每次从库执行查询请求前，先判断 seconds_behind_master 是否已经等于 0。如果还不等于 0 ，那就必须等到这个参数变为 0 才能执行查询请求。
2. 对比位点确保主备无延迟：
	* Master_Log_File 和 Read_Master_Log_Pos，表示的是读到的主库的最新位点；
	* Relay_Master_Log_File 和 Exec_Master_Log_Pos，表示的是备库执行的最新位点。
	* Master_Log_File 和 Relay_Master_Log_File、Read_Master_Log_Pos 和 Exec_Master_Log_Pos 这两组值完全相同，就表示接收到的日志已经同步完成。
3. 对比 GTID 集合确保主备无延迟
	* Auto_Position=1，表示这对主备关系使用了 GTID 协议，
	* Retireved_Gtid_Set，是备库收到的所有日志的 GTID 集合；
	* Executed_Gtid_Set，是备库所有已经执行完成的 GTID 集合。

一个事务的binlog在主备库之间的状态：

1. 主库执行完成，写入binlog，并反馈给客户端
2. binlog被主库发送给备库，备库收到
3. 在备库执行binlog完成

#### 可能出现问题 ####



### 配合 semi-sync ###



## 29 | 如何判断一个数据库是不是出问题了？ ##

在一主一备的双M架构里，主备切换只需要客户端流量切到备库；而在一主多从架构里，主备且换除了要把客户端流量切换到备库外，还需要把从库接到新主库上。

## 30 | 答疑文章（二）：用动态的观点看加锁 ##

InnoDB的间隙锁、next-key lock，以及加锁规则。

加锁规则，包含了两个“原则”、两个“优化”和一个“bug”：

* 原则1:加锁的基本单位是next-key lock。next-key lock是前开后闭区间。
* 原则2:查找过程中访问到的对象才会加锁。
* 优化1:索引上的等值查询，给唯一索引加锁的时候，next-key lock退化为行锁。
* 优化2:索引上的等值查询，向右遍历时且最后一个值不满足等值条件的时候，next-key lock退化为间隙锁。
* 一个bug：唯一索引上的范围查询会访问到不满足条件的第一个值为止。

	CREATE TABLE `t30` (
	  `id` int(11) NOT NULL,
	  `c` int(11) DEFAULT NULL,
	  `d` int(11) DEFAULT NULL,
	  PRIMARY KEY (`id`),
	  KEY `c` (`c`)
	) ENGINE=InnoDB;
	
	insert into t30 values(0,0,0),(5,5,5),
	(10,10,10),(15,15,15),(20,20,20),(25,25,25);

### 不等号条件里的等值查询 ###

	begin;
	select * from t where id>9 and id<12 order by id desc for update;

利用上面的加锁规则，加锁范围是主键索引上的(0,5]、(5,10]和(10,15)。

### 等值查询的过程 ###

	begin;
	select id from t where c in(5,20,10) lock in share mode;

### 怎么看死锁？ ###

1. 由于锁是一个个加的，要避免锁，对同一组资源，要按照尽量相同的顺序访问。
2. 在发生死锁的时刻，for update这条语句占有的资源更多，回滚成本更大，所以InnoDB选择了回滚成本更小的lock in share mode语句，来回滚。

### 怎么看锁等待？ ###

### update的例子 ###

### 小结 ###

通过 explain 的结果，就能够脑补出一个 SQL 语句的执行流程。达到这样的程度，才算是对索引组织表、索引、锁的概念有了比较清晰的认识。你同样也可以用这个方法，来验证自己对这些知识点的掌握程度。

`show engine innodb status` 输出结果中的事务信息和死锁信息

所谓“间隙”，其实根本就是由“这个间隙右边的那个记录”定义的。

## 31 | 误删数据后除了跑路，还能怎么办？ ##

## 32 | 为什么还有kill不掉的语句？ ##

## 33 | 我查这么多数据，会不会数据库内存打爆？ ##

### 全表扫描对server层的影响 ###

	mysql -h$host -P$port -u$user -p$pwd -e "select * from db1.t" > $target_file

InnoDB的数据是保存在主键索引上的，所以全表扫描实际上是直接扫描表t的主键索引。

服务端不需要保存一个完整的结果集。取数据和发数据的流程是：

1. 获取一行，写到net_buffer中，这个内存的大小是由参数net_buffer_length定义的，默认是16k。
2. 重复获取行，直到net_buffer写满，调用网络接口发出去。
3. 如果发送成功，就清空net_buffer，然后继续取下一行，并写入net_buffer
4. 如果发送函数返回 EAGAIN 或 WSAEWOULDBLOCK，就表示本地网络栈（socket send buffer）写满了，进入等待。直到网络栈重新可写，再继续发送。

可以看出

1. 一个查询在发送过程中，占用的MySQL内部的内存最大就是net_buffer_length这么大，并不会达到200G；
2. socket send buffer也不可能达到200G（默认定义 /proc/sys/net/core/wmem_default），如果socket send buffer被写满，就会暂停读数据的流程。

MySQL是“边读变发的”。如果客户端接收的慢，会导致MySQL服务端由于结果发布出去，这个事务的执行时间变长。

	show processlist;

对于正常的线上业务来说，如果一个查询的返回结果不会很多的话，使用mysql_store_result这个接口，直接把查询结果保存到本地内存。

### 全表扫描对InnoDB的影响 ###

分析了 InnoDB 内存的一个作用，是保存更新的结果，再配合 redo log，就避免了随机写盘。

内存的数据页是在Buffer Pool(BP)中管理的，在WAL里Buffer Pool 

### 小结 ###

由于 MySQL 采用的是边算边发的逻辑，因此对于数据量很大的查询结果来说，不会在 server 端保存完整的结果集。所以，如果客户端读结果不及时，会堵住 MySQL 的查询过程，但是不会把内存打爆。

对于 InnoDB 引擎内部，由于有淘汰策略，大查询也不会导致内存暴涨。并且，由于 InnoDB 对 LRU 算法做了改进，冷数据的全表扫描，对 Buffer Pool 的影响也能做到可控。

全表扫描还是比较耗费 IO 资源的，所以业务高峰期还是不能直接在线上主库执行全表扫描的。

## 34 | 到底可不可以使用join？ ##

1. 我们 DBA 不让使用 join，使用 join 有什么问题呢？
2. 如果有两个大小不同的表做 join，应该用哪个表做驱动表呢？

### Index Nested-Loop Join ###

	select * from t34_1 straight_join t34_2 on (t34_1.a=t34_2.a);

t1是驱动表，t2是被驱动表，被驱动表t2的字段a上有索引，join过程用上了这个索引。

1. 从表t1中读入一行数据R；
2. 从数据行R中，取出a字段到表t2里去查找；
3. 取出表t2中满足条件的行，跟R组成一行，作为结果集的一部分；
4. 重复执行步骤1到3，直到表t1的末尾循环结束。

先遍历表t1，然后根据从表t1中取出的每行数据的a值，去表t2中查找满足条件的记录。在形式上，这个过程跟我们的嵌套查询类似，并且可以用上被驱动表的索引。——Index Nested-Loop Join，简称NLJ

1. 对驱动表t1做了全表扫描，这个过程需要扫描100行；
2. 而对于每一行R，根据a字段去表t2查找，走的是树搜索过程。由于我们构造的数据都是一一对应，因此每次的搜索过程都只搜索一行，也是总共扫描100行；
3. 所以，整个执行流程，总扫描行数是200。

#### 能不能使用join ####

用单表查询

1. 执行select * from t1，查出表 t1 的所有数据，这里有 100 行；
2. 循环遍历这 100 行数据：
	* 从每一行 R 取出字段 a 的值 $R.a；
	* 执行select * from t2 where a=$R.a；
	* 把返回的结果和 R 构成结果集的一行。

扫描了200行，但总共执行了101条语句，比直接join多了100次交互。

#### 怎么选择驱动表 ####

在这个 join 语句执行过程中，驱动表是走全表扫描，而被驱动表是走树搜索。

N+N*2*log2^M

1. 使用join语句，性能比强行拆成多个单表执行SQL语句的性能要好；
2. 如果使用join语句的话，需要让小表做驱动表。

### Simple Nested-Loop Join ###



## 35 | join语句怎么优化？ ##

## 36 | 为什么临时表可以重名？ ##

## 37 | 什么时候会使用内部临时表？ ##

## 38 | 都说InnoDB好，还要不要使用Memory引擎 ##

## 39 | 自增主键为什么不是连续的？ ##

	CREATE TABLE `t39` (
	  `id` int(11) NOT NULL AUTO_INCREMENT,
	  `c` int(11) DEFAULT NULL,
	  `d` int(11) DEFAULT NULL,
	  PRIMARY KEY (`id`),
	  UNIQUE KEY `c` (`c`)
	) ENGINE=InnoDB;

### 自增值保存在哪儿？ ###

`insert into t39 values(null, 1, 1);`插入一行数据
`show create table t39`看到表定义里面出现了AUTO_INCREMENT=2，表示下一次插入数据时，如果需要自动生成自增值，会生成id=2

表的结构定义存放在后缀名为.frm 的文件中，但是并不会保存自增值。不同的引擎对于自增值的保存策略不同：

* MyISAM引擎的自增值保存在数据文件中
* InnoDB引擎的自增值，其实时保存在了内存里，并且到了MySQL8.0版本后，才有了“自增值持久化”的能力，也就是才实现了“如果发生重启，表的自增值可以恢复为MySQL重启前的值”，具体情况是：
	* 在 MySQL 5.7 及之前的版本，自增值保存在内存里，并没有持久化。每次重启后，第一次打开表的时候，都会去找自增值的最大值 max(id)，然后将 max(id)+1 作为这个表当前的自增值。﻿
	* 在 MySQL 8.0 版本，将自增值的变更记录在了 redo log 中，重启的时候依靠 redo log 恢复重启之前的值。

### 自增值修改机制 ###

在MySQL里面，如果字段id被定义为AUTO_INCREMENT，在插入一行数据的时候，自增值的行为如下：

### 自增值的修改时机 ###

### 自增锁的优化 ###

### 小结 ###

“自增主键为什么会出现不连续的值”，这个问题开始，讨论了自增值的存储。

* 在 MyISAM 引擎里面，自增值是被写在数据文件上的。
* 在 InnoDB 中，自增值是被记录在内存的。
	* MySQL 直到 8.0 版本，才给 InnoDB 表的自增值加上了持久化的能力，确保重启前后一个表的自增值不变。

## 40 | insert语句的锁为什么这么多？ ##

## 41 | 怎么最快地复制一张表 ##

	create database db1;
	use db1;
	
	create table t(id int primary key, a int, b int, index(a))engine=innodb;
	delimiter ;;
	  create procedure idata()
	  begin
	    declare i int;
	    set i=1;
	    while(i<=1000)do
	      insert into t values(i,i,i);
	      set i=i+1;
	    end while;
	  end;;
	delimiter ;
	call idata();
	
	create database db2;
	create table db2.t like db1.t

### mysqldump 方法 ###

使用 mysqldump 命令将数据导出成一组 INSERT 语句

	mysqldump -h$host -P$port -u$user --add-locks=0 --no-create-info --single-transaction  --set-gtid-purged=OFF db1 t --where="a>900" --result-file=/client_tmp/t.sql

1. `–single-transaction` 的作用是，在导出数据的时候不需要对表 db1.t 加表锁，而是使用 START TRANSACTION WITH CONSISTENT SNAPSHOT 的方法；
2. `–add-locks` 设置为 0，表示在输出的文件结果里，不增加" LOCK TABLES t WRITE;" ；
3. `–no-create-info` 的意思是，不需要导出表结构；
4. `–set-gtid-purged=off` 表示的是，不输出跟 GTID 相关的信息；
5. `–result-file` 指定了输出文件的路径，其中 client 表示生成的文件是在客户端机器上的。

一条 INSERT 语句里面会包含多个 value 对，这是为了后续用这个文件来写入数据的时候，执行速度可以更快。

	mysql -h127.0.0.1 -P13000  -uroot db2 -e "source /client_tmp/t.sql"

source并不是

### 导出CSV文件 ###

将结果直接导出成.csv文件。

	select * from db1.t where a>900 into outfile '/server_tmp/t.csv';

1. 这条语句会将结果保存在服务端。如果你执行命令的客户端和 MySQL 服务端不在同一个机器上，客户端机器的临时目录下是不会生成 t.csv 文件的。
2. into outfile 指定了文件的生成位置（/server_tmp/），这个位置必须受参数 secure_file_priv 的限制。参数 secure_file_priv 的可选值和作用分别是：
	* 如果设置为 empty，表示不限制文件生成的位置，这是不安全的设置；
	* 如果设置为一个表示路径的字符串，就要求生成的文件只能放在这个指定的目录，或者它的子目录；
	* 如果设置为 NULL，就表示禁止在这个 MySQL 实例上执行 select … into outfile 操作。
3. 这条命令不会帮你覆盖文件，因此你需要确保 /server_tmp/t.csv 这个文件不存在，否则执行语句时就会因为有同名文件的存在而报错。
4. 这条命令生成的文本文件中，原则上一个数据行对应文本文件的一行。但是，如果字段中换行符，在生成的文本中也有换行符。不过类似换行符、制表符这类符号，前面都会跟上“\”这个转义符，这样就可以跟字段之间、数据行之间的分隔符区分开。

### 物理拷贝方法 ###

逻辑导逻辑的方法，从db1.t中读出来，生成文本，再写入目标表db2.t中。

一个InnoDB表，除了包含这两个物理文件外，还需要在数据字典中注册。直接拷贝这两个文件的话，因为数据字典中没有 db2.t 这个表，系统是不会识别和接受它们的。

在 MySQL 5.6 版本引入了可传输表空间(transportable tablespace) 的方法，可以通过导出 + 导入表空间的方式，实现物理拷贝表的功能。

假设我们现在的目标是在 db1 库下，复制一个跟表 t 相同的表 r，具体的执行步骤如下：

1. 执行 create table r like t，创建一个相同表结构的空表；
2. 执行 alter table r discard tablespace，这时候 r.ibd 文件会被删除；
3. 执行 flush table t for export，这时候 db1 目录下会生成一个 t.cfg 文件；
4. 在 db1 目录下执行 cp t.cfg r.cfg; cp t.ibd r.ibd；这两个命令（这里需要注意的是，拷贝得到的两个文件，MySQL 进程要有读写权限）；
5. 执行 unlock tables，这时候 t.cfg 文件会被删除；
6. 执行 alter table r import tablespace，将这个 r.ibd 文件作为表 r 的新的表空间，由于这个文件的数据内容和 t.ibd 是相同的，所以表 r 中就有了和表 t 相同的数据。

关于拷贝表的这个流程

1. 在第 3 步执行完 flsuh table 命令之后，db1.t 整个表处于只读状态，直到执行 unlock tables 命令后才释放读锁；
2. 在执行 import tablespace 的时候，为了让文件里的表空间 id 和数据字典中的一致，会修改 r.ibd 的表空间 id。而这个表空间 id 存在于每一个数据页中。因此，如果是一个很大的文件（比如 TB 级别），每个数据页都需要修改，所以你会看到这个 import 语句的执行是需要一些时间的。当然，如果是相比于逻辑导入的方法，import 语句的耗时是非常短的。

### 小结 ###

三种方法将一个表的数据导入到另外一个表中，各自的优缺点：

1. 物理拷贝的方式速度最快，尤其对于大表拷贝来说是最快的方法。如果出现误删表的情况，用备份恢复出误删之前的临时表，然后再把临时库中的表拷贝到生产库上，是恢复数据最快的方法。但有一定的局限性：
* 必须是全表拷贝，不能只拷贝部分数据；
* 需要到服务器上拷贝数据，在用户无法登录数据库主机的场景下无法使用；
* 由于是通过拷贝物理文件实现的，源表和目标表都是使用 InnoDB 引擎时才能使用。
2. 用 mysqldump 生成包含 INSERT 语句文件的方法，可以在 where 参数增加过滤条件，来实现只导出部分数据。这个方式的不足之一是，不能使用 join 这种比较复杂的 where 条件写法。
3. 用 select … into outfile 的方法是最灵活的，支持所有的 SQL 写法。但，这个方法的缺点之一就是，每次只能导出一张表的数据，而且表结构也需要另外的语句单独备份。

逻辑备份方式，是可以跨引擎使用的

## 42 | grant之后要跟着flush privileges吗 ##

## 43 | 要不要使用分区表？ ##

## 44 | 答疑文章（三）：说一说这些好问题 ##

## 45 | 自增id用完怎么办？ ##


## 直播回顾 | 林晓斌：我的 MySQL 心路历程 ##

## 结束语 | 点线网面，一起构建MySQL知识网络 ##

### 1. 路径千万条，实践第一条 ###

* 跟着专栏中的案例做实验，继续坚持下去。在阅读其他技术文章、图书的时候，也是同样的道理。如果你觉得自己理解了一个知识点，也一定要尝试设计一个例子来验证它。
* 在设计案例的时候，我建议你也设计一个对照的反例，从而达到知识融汇贯通的目的。

### 2. 原理说不清，双手白费劲 ###

1. 先实践再搞清楚原理
2. 先明白原理再通过实践去验证

怎么证明是是不是真的把原理弄清楚了呢？——说出来、写出来

如果有人请教你这个问题：

1. 验证自己搞懂了这个知识点
2. 提升自己的技术表达能力

“写出来”又是一个更高的境界。因为，你在写的过程中，就会发现这个“明白”很可能只是一个假象。所以，在专栏下面写下自己对本章知识点的理解，也是一个不错的夯实学习成果的方法。

### 3. 知识没体系，转身就忘记 ###

知识点“写下来”，还有一个好处，就是你会发现这个知识点的关联知识点。深究下去，点就连成线，然后再跟别的线找交叉。

### 4. 手册补全面，案例扫盲点 ###

一开始就看手册？看手册的时机，应该是知识网络构建得差不多的时候。

“差不多”的标准：

* 能否解释清楚错误日志（error log）、慢查询日志（slow log）中每一行的意思？
* 能否快速评估出一个表结构或者一条 SQL 语句，设计得是否合理？
* 能否通过 explain 的结果，来“脑补”整个执行过程（我们已经在专栏中练习几次了）？
* 到网络上找 MySQL 的实践建议，对于每一条做一次分析：
	* 如果觉得不合理，能否给出自己的意见？
	* 如果觉得合理，能否给出自己的解释？
	* 找有经验的人讨论

# 特别放送 #

## MySQL中6个常见的日志问题 ##

MySQL里有两个日志，即：重做日志（redo log）和归档日志（binlog）。

* binlog可以给

### 问题 ###

问题1：MySQL怎么知道binlog是完整的？

回答：一个事务的binlog是有完整格式的：

* statement格式的binlog，最后会有COMMIT；
* row格式的binlog，最后会有一个XID event。

在MySQL5.6.2版本以后，还引入了binlog-checksum参数，用来验证binlog内容的正确性。对于binlog日志由于磁盘原因，可能会在日志中间出错的情况，MySQl可以通过校验checksum的结果来发现。所以，MySQL还是有版本验证事务binlog的完整性的。

问题2：redo log和binlog是怎么关联起来的？

回答：他们有共同的数据字段，叫XID。崩溃恢复的时候，会按顺序扫描redo log：

* 如果碰到既有prepare，又有commit的redo log，就直接提交。
* 如果碰到只有prepare，而没有commit的redo log，就拿着XID去binlog找对应的事务。

问题3：处于prepare阶段的redo log加上完整binlog，重启就能恢复，MySQL为什么要这么设计？

问题4：如果这样的话，为什么还要两阶段提交呢？干脆先redo log写完，再写binlog。崩溃恢复的时候，必须得两个日志都完整才可以。是不是一样得逻辑？

答：两阶段提交是经典的分布式系统问题，并不是MySQL独有的。

两阶段提交就是为了给所有人一个几回，当每个人都说“我ok”的时候，再一起提交。

问题5：不引入两个日志，也就没有两阶段提交的必要了。只用binlog来支持崩溃恢复，又能支持归档，不就可以了？

答：binlog没有能力恢复“数据页”

问题6：那能不能反过来，只用redo log，不要binlog？

# 直播回顾 #

## 直播回顾 | 林晓斌：我的 MySQL 心路历程 ##

