# MySQL实战45讲 #

# 开篇词 #

## 开篇词 | 这一次，让我们一起来搞懂MySQL ##

平时使用数据库时高频出现的知识，如事务、索引、锁等内容构成专栏的主线。

点->线->面，形成自己的MySQL知识网络

# 基础篇 #

## 01 | 基础架构：一条SQL查询语句是如何执行的？ ##

mysql> select * from T where ID = 10;

![0d2070e8f84c4801adbfa03bda1f98d9](img/0d2070e8f84c4801adbfa03bda1f98d9.png)

MySQL分为Server层和存储引擎层两部分。

Server层包括连接器、查询缓存、分析器、优化器、执行器等，涵盖MySQL的大部分核心服务功能，以及所有的内置函数（如日期、时间、数学和加密函数等），所有跨存储引擎的功能都在这一层实现，比如存储过程、触发器、视图等。

存储引擎层负责数据的存储和提取。其架构模式是插件式的，支持 InnoDB、MyISAM、Memory 等多个存储引擎。现在最常用的存储引擎是 InnoDB，它从 MySQL 5.5.5 版本开始成为了默认存储引擎。

不同的存储引擎共用一个Server层，也就是从连接器到执行器的部分。

### 连接器 ###

1. 先连接到这个数据库上，这时候接待你的就是连接器。连接器负责跟客户端建立连接、获取权限、维持和管理连接。`mysql -h$ip -P$port -u$user -p`（连接命令中的 mysql 是客户端工具，用来跟服务端建立连接。在完成经典的 TCP 握手后，连接器就要开始认证你的身份，这个时候用的就是你输入的用户名和密码。）

建立连接的过程通常比较复杂的，所以在使用中要尽量减少建立连接的动作，也尽量使用长连接。但全部使用长连接后，有时候MySQL占用内存涨得特别快，因为MySQL在执行过程中临时使用的内存是管理在连接对象里面的。=>内存占用太大，被系统强行杀掉（OOM）=>MySQL异常重启

解决方案：

1. 定期断开长连接。使用一段时间后，或者程序里面判断执行过一个占用内存的大查询后，断开连接，之后要查询再重连。
2. 如果是 MySQL 5.7 或更新版本，可以在每次执行一个比较大的操作后，通过执行 mysql_reset_connection 来重新初始化连接资源。这个过程不需要重连和重新做权限验证，但是会将连接恢复到刚刚创建完时的状态。

    wait_timeout默认值是8小时

### 查询缓存 ###

*但是大多数情况下我会建议你不要使用查询缓存，为什么呢？因为查询缓存往往弊大于利。*

查询缓存的失效非常频繁，只要有对一个表的更新，这个表上所有的查询缓存都会被清空。除非你的业务就是有一张静态表，很长时间才会更新一次。比如，一个系统配置表，那这张表上的查询才适合使用查询缓存。

8.0以后没有这块功能。

### 分析器 ###

如果没有命中查询缓存

1. MySQL需要对SQL语句做解析
2. 词法分析， `select * from T where ID = 10;` 比如select，识别出来是一个查询语句，把字符串“T”识别成“表名T”，把字符串“ID”识别成“列ID”。
3. 语法分析，语法分析器会根据语法判断，判断是否满足MySQL语法。

### 优化器 ###

优化器是在表里面有多个索引的时候，决定使用哪个索引；或者在一个语句有多表关联（join）的时候，决定各个表的连接顺序。比如你执行下面这样的语句，这个语句是执行两个表的 join：

	mysql> select * from t1 join t2 using(ID) where t1.c=10 and t2.d=20;

* 既可以先从表 t1 里面取出 c=10 的记录的 ID 值，再根据 ID 值关联到表 t2，再判断 t2 里面 d 的值是否等于 20。
* 也可以先从表 t2 里面取出 d=20 的记录的 ID 值，再根据 ID 值关联到 t1，再判断 t1 里面 c 的值是否等于 10。

逻辑结果一样，但执行的效率不同，优化器的作用就是决定选择使用哪个方案。

优化器阶段完成后，这个语句的执行方案就确定下来了。

### 执行器 ###

执行器阶段，开始执行语句。

开始执行的时候，要先判断一下你对这个表 T 有没有执行查询的权限，如果没有，就会返回没有权限的错误，如下所示 (在工程实现上，如果命中查询缓存，会在查询缓存返回结果的时候，做权限验证。查询也会在优化器之前调用 precheck 验证权限)。

如果有权限，就打开表继续执行。打开表的时候，执行器就会根据表的引擎定义，去使用这个引擎提供的接口。

比如我们这个例子中的表 T 中，ID 字段没有索引，那么执行器的执行流程是这样的：

1. 调用InnoDB引擎接口取这个表的第一行，判断ID值是不是10，如果不是则跳过，如果是则将这行存在结果集中；
2. 调用引擎接口取“下一行”，重复相同的判断逻辑，直到取到这个表的最后一行。
3. 执行器将上述遍历过程中所有满足条件的行组成的记录集作为结果集返回给客户端。

数据库的慢查询日志中看到一个 rows_examined 的字段，表示这个语句执行过程中扫描了多少行。这个值就是在执行器每次调用引擎获取数据行的时候累加的。

引擎扫描行数跟 rows_examined 并不是完全相同的。

### 小结 ###

MySQL的逻辑架构，对一个SQL语句完整执行流程的各个阶段有一个初步的印象。由于篇幅的限制，只是用一个查询的例子将各个环节过了一遍。

### 精选留言 ###

#### Q ####

如果表 T 中没有字段 k，而你执行了这个语句 select * from T where k=1, 那肯定是会报“不存在这个列”的错误： “Unknown column ‘k’ in ‘where clause’”。你觉得这个错误是在我们上面提到的哪个阶段报出来的呢？

#### A ####

* Oracle会在分析阶段判断语句是否正确，表是否存在，列是否存在等。

* 对权限的检查不在优化器之前做，，SQL语句要操作的表不只是SQL字面上那些。比如如果有个触发器，得在执行器阶段（过程中）才能确定。优化器阶段前是无能为力的。

分析器，优化器是对SQL语句进行择优处理，执行器需要判断选取表中的权限。

#### Q ####

为什么对权限的检查不在优化器之前做？

#### A ####

SQL语句要操作的表不只是SQL字面上那些。比如如果有个触发器，得在执行器阶段（过程中）才能确定。

#### Q ####

加粗内容为未掌握

1. MySQL的框架有几个组件, 各是什么作用? 
**2. Server层和存储引擎层各是什么作用?**
**3. you have an error in your SQL syntax 这个保存是在词法分析里还是在语法分析里报错?**
4. 对于表的操作权限验证在哪里进行?
**5. 执行器的执行查询语句的流程是什么样的? **

#### A ####

1. Server层和存储引擎，Server层分连接器、查询缓存、分析器、优化器、执行器。
**2. Server层：涵盖MySQL的大多数核心服务功能，以及内置函数；存储引擎：负责数据的存储和提取**
3. 语法分析——**词法分析：识别里面的字符串分别是什么，代表什么；语法分析：**
4. 执行器，在做执行器之前，通过获取一张表的数据，验证该用户是否有表的操作权限
**5. 执行器进行检索流程如下：
	* 调用InnoDB引擎接口取这个表的第一行
	* 调用引擎接口取“下一行”，重复相同的判断逻辑，直到取到这个表的最后一行
	* 执行器将上述遍历过程中所有满足条件的行组成的记录集作为结果集返回给客户端 **

#### A ####

## 02 | 日志系统：一条SQL更新语句是如何执行的？ ##

一条查询语句的执行过程一般是经过连接器、分析器、优化器、执行器等功能模块，最后到达存储引擎。

查询语句对策那一套流程，更新语句也是同样走一遍。

1. 连接器：执行语句前要先连接数据库
2. 查询缓存：一个表上有更新的时候，跟这个表有关的查询缓存会失效。所以这条语句就会把表T上所有缓存结果都清空（不建议使用查询缓存的原因）
3. 分析器：通过词法和语法解析知道这是一条更新语句
4. 优化器：使用ID索引
5. 执行器：负责具体执行，找到这一行，更新

### 重要的日志模块：redo log ###

WAL的全称是Write-Ahead Logging，关键点是先写日志，再写磁盘。

当有一条记录需要更新的时候，InnoDB引擎就会先把记录写到redo log（粉板）里面，并更新内存。InnoDB引擎会在适当的时候，将这个操作记录更新到磁盘里面（系统比较空闲的时候处理）。如果写满了，需要腾出空间才行。

InnoDB 的 redo log 是固定大小的，比如可以配置为一组 4 个文件，每个文件的大小是 1GB，那么这块“粉板”总共就可以记录 4GB 的操作。从头开始写，写到末尾就又回到开头循环写。

![16a7950217b3f0f4ed02db5db59562a7.png](img/16a7950217b3f0f4ed02db5db59562a7.png)

* write pos是当前记录的位置，一边写一边后移，写到第3号文件末尾后就回到0号文件开头。
* checkpoint是当前要擦除的位置，也是往后推移并且循环，擦除记录前要把记录更新到数据文件。
* write pos和checkpoint之间的是“粉版”上还空着的部分，可以用来记录新的操作。如果 write pos 追上 checkpoint，表示“粉板”满了，这时候不能再执行新的更新，得停下来先擦掉一些记录，把 checkpoint 推进一下。

crash-safe：因为redo log，InnoDB 就可以保证即使数据库发生异常重启，之前提交的记录都不会丢失

### 重要的日志模块：binlog ###

MySQL整体来看，有两块

1.  Server 层，主要做的是MySQL功能层面的事情
2.  引擎层，负责存储相关的具体事宜

---

1. redo log 是 InnoDB 引擎特有的；binlog 是 MySQL 的 Server 层实现的，所有引擎都可以使用。
2. redo log 是物理日志，记录的是“在某个数据页上做了什么修改”；binlog 是逻辑日志，记录的是这个语句的原始逻辑，比如“给 ID=2 这一行的 c 字段加 1 ”。
3. redo log 是循环写的，空间固定会用完；binlog 是可以追加写入的。“追加写”是指 binlog 文件写到一定大小后会切换到下一个，并不会覆盖以前的日志。

执行器和 InnoDB 引擎在执行这个简单的 update 语句时的内部流程：`mysql> update T set c=c+1 where ID=2;`

![2e5bff4910ec189fe1ee6e2ecc7b4bbe.png](img/2e5bff4910ec189fe1ee6e2ecc7b4bbe.png)

1. 
	* 执行器：执行器先找引擎取 ID=2 这一行。
	* 引擎：ID 是主键，引擎直接用树搜索找到这一行。如果 ID=2 这一行所在的数据页本来就在内存中，就直接返回给执行器；否则，需要先从磁盘读入内存，然后再返回。
2. 执行器：执行器拿到引擎给的行数据，把这个值加上 1，比如原来是 N，现在就是 N+1，得到新的一行数据，再调用引擎接口写入这行新数据。 
3. 引擎：引擎将这行新数据更新到内存中，同时将这个更新操作记录到 redo log 里面，此时 redo log 处于 prepare 状态。然后告知执行器执行完成了，随时可以提交事务。
4. 执行器：执行器生成这个操作的 binlog，并把 binlog 写入磁盘。
5. 引擎：执行器调用引擎的提交事务接口，引擎把刚刚写入的 redo log 改成提交（commit）状态，更新完成。

redo log的写入拆成了两个步骤：prepare和commit，两阶段提交

### 两阶段提交 ###

两阶段提交的原因是为了让两份日志的逻辑一致。=>怎样让数据库恢复到半个月内任意一秒的状态？

1. binlog会记录所有的逻辑操作
2. 系统定期做整库备份
3. 找到最近一天的全量备份，然后通过这个备份恢复到临时库
4. 从备份时间开始，将备份的binlog依次取出，重新进行到误操作时间点的那个时候

如果不使用两阶段提交。

1. 先写 redo log 后写 binlog。
	* redo log 写完之后，系统即使崩溃，仍然能够把数据恢复回来，所以恢复后这一行 c 的值是 1。
	* 但是由于 binlog 没写完就 crash 了，这时候 binlog 里面就没有记录这个语句。如果需要用这个 binlog 来恢复临时库的话，由于这个语句的 binlog 丢失，这个临时库就会少了这一次更新，恢复出来的这一行 c 的值就是 0，与原库的值不同。
2. 先写 binlog 后写 redo log。
	* 如果在 binlog 写完之后 crash，由于 redo log 还没写，崩溃恢复以后这个事务无效，所以这一行 c 的值是 0。但是 binlog 里面已经记录了“把 c 从 0 改成 1”这个日志。所以，在之后用 binlog 来恢复的时候就多了一个事务出来，恢复出来的这一行 c 的值就是 1，与原库的值不同。

不只是操作，还有扩容的时候。通常的做法是用全量备份加上应用 binlog 来实现的。

简单说，redo log 和 binlog 都可以用于表示事务的提交状态，而两阶段提交就是让这两个状态保持逻辑上的一致。

### 小结 ###

* 两个日志，即物理日志redo log和逻辑日志binlog。
* redo log用于保证crash-safe能力。
	*  innodb_flush_log_at_trx_commit 这个参数设置成 1 的时候，表示每次事务的 redo log 都直接持久化到磁盘。这个参数我建议你设置成 1，这样可以保证 MySQL 异常重启之后数据不丢失。
* sync_binlog  
	* sync_binlog 这个参数设置成 1 的时候，表示每次事务的 binlog 都持久化到磁盘。这个参数我也建议你设置成 1，这样可以保证 MySQL 异常重启之后 binlog 不丢失。 
* 与 MySQL 日志系统密切相关的“两阶段提交”：两阶段提交是跨系统维持数据逻辑一致性时常用的一个方案，即使你不做数据库内核开发，日常开发中也有可能会用到。   

前面我说到定期全量备份的周期“取决于系统重要性，有的是一天一备，有的是一周一备”。那么在什么场景下，一天一备会比一周一备更有优势呢？或者说，它影响了这个数据库系统的哪个指标？

最长恢复时间更短

### 精选留言 ###

#### Q ####

说到定期全量备份的周期“取决于系统重要性，有的是一天一备，有的是一周一备”。那么在什么场景下，一天一备会比一周一备更有优势呢？或者说，它影响了这个数据库系统的哪个指标？

#### A ####

一天一备，只需要根据前一天的redo log的记录把binlog 复现一遍即可，不容易无操作，隔断时间比较少。但这样redolog的记录会比较大。

Binlog两种模式

1. statement格式的话是记sql语句
2. row格式会记录行的内容，记两条，更新前和更新后的内容

redo是物理的，binlog是逻辑的；现在由于redo是属于InnoDB引擎，所以必须要有binlog（可以使用别的引擎）保证数据库的一致性，必须要保证2份日志一致。使用的2阶段式提交；其他感觉像事务，不是成功就是失败，不能让中间环节出现，也就是一个成功，一个失败。

#### Q ####

只用InonoDB引擎的时候还保留Binlog这种设计的原因

#### A ####

1. redolog只有InnoDB有，别的引擎没有
2. redolog是循环写的，不持久保存，binlog的“归档”这个功能，redolog是不具备的。

#### Q ####

后知后觉，误删除一段时间了，才发现误删除，此时，我把之前误删除的binlog导入，再把误删除之后binlog导入，会出现问题，比如主键冲突，而且binlog导数据，不同模式下时间也有不同，但是一般都是row模式

#### A ####

其实恢复数据只能恢复到误删之前到一刻，误删之后的，不能只靠binlog来做，因为业务逻辑可能因为误删操作的行为，插入了逻辑错误的语句，所以之后的，跟业务一起，从业务快速补数据的。只靠binlog补出来的往往不完整

#### Q ####

如果在重启后，需要通过检查binlog来确认redo log中处于prepare的事务是否需要commit，那是否不需要二阶段提交，直接以binlog的为准，如果binlog中不存在的，就认为是需要回滚的。

#### A ####

1 prepare阶段 2 写binlog 3 commit

当在2之前崩溃时
重启恢复：后发现没有commit，回滚。备份恢复：没有binlog 。
一致
当在3之前崩溃
重启恢复：虽没有commit，但满足prepare和binlog完整，所以重启后会自动commit。备份：有binlog. 一致

#### Q ####

1. redo log的概念是什么? 为什么会存在.
*2. 什么是WAL(write-ahead log)机制, 好处是什么.*
3. redo log 为什么可以保证crash safe机制.
4. binlog的概念是什么, 起到什么作用, 可以做crash safe吗?
5. binlog和redolog的不同点有哪些?
6. 物理一致性和逻辑一直性各应该怎么理解?
7. 执行器和innoDB在执行update语句时候的流程是什么样的?
8. 如果数据库误操作, 如何执行数据恢复?
9. 什么是两阶段提交, 为什么需要两阶段提交, 两阶段提交怎么保证数据库中两份日志间的逻辑一致性(什么叫逻辑一致性)?
10. 如果不是两阶段提交, 先写redo log和先写bin log两种情况各会遇到什么问题?

#### A ####

1. redo log是innoDB存储引擎特有的存储引擎，物理存储日志
*2. WAL技术，全称Write-Ahead Logging，关键点是先写日志，再写磁盘*
3. 31
4. 
4. binlog是
5. 123
6. 123
7. * 检索update的行是否在数据页
8.  * 通过使用误操作之前最近的时间节点的redo log进行全量恢复数据库
	* 并在此基础上使用binlog进行操作
	* 恢复到误操作时的数据库
9. 什么是
10.  

## 03 | 事务隔离：为什么你改了我还看不见？ ##

事务就是要保证一组数据库操作，要么全部成功，要么全部失败。

### 隔离性与隔离级别 ###

ACID（Atomicity，Consistency，Isolation，Durability，即原子性、一致性、隔离性和持久性）

当数据库上有多个事务同时执行的时候=>可能出现脏读（dirty read）、不可重复读（non-repeatable read）、幻读（phantom read）的问题=>有了“隔离级别”的概念，去解决这些问题

#### 隔离级别 ####

隔离级别越高，效率越低=>需要找到平衡点

SQL标准的事务隔离级别：

1. 读未提交（read uncommitted）：一个事务还没提交时，它做的变更就能被别的事务看到。
2. 读提交（read committed）：一个事务提交之后，它做的变更才会被其他事务看到。
3. 可重复读（repeatable read）：一个事务执行过程中看到的数据，总是跟这个事务在启动时看到的数据是一致的。当然在可重复读隔离级别下，未提交变更对其他事务也是不可见的。
4. 串行化（serializable）：对于同一行记录，“写”会加“写锁”，“读”会加“读锁”。当出现读写锁冲突的时候，后访问的事务必须等前一个事务执行完成，才能继续执行。

![7dea45932a6b722eb069d2264d0066f8](img/7dea45932a6b722eb069d2264d0066f8.png)

根据事务隔离级别的不同，V1、V2、V3返回值也会分别不同。

* 读未提交：V2、V3=2，B的事务还没提交，但做个变更就能被别的事务看到，所以V1也是2
* 读提交：V2、V3=2，事务 B 的更新在提交后才能被 A 看到，所以V1也是1
* 可重复读：事务在执行期间看到的数据前后必须是一致的，所以v1，v2=1，v3=2
* 串行化：则在事务 B 执行“将 1 改成 2”的时候，会被锁住。直到事务 A 提交后，事务 B 才可以继续执行。v1、v2值是1，v3的值是2

在实现上，数据库里面会创建一个视图，访问的时候以视图的逻辑结果为准。

* “读提交”隔离级别：在每个 SQL 语句开始执行的时候创建的
* “可重复读”隔离级别：在事务启动时创建的，整个事务存在期间都用这个视图

“读未提交”隔离级别：直接返回记录上的最新值，没有视图概念；而“串行化”隔离级别下直接用加锁的方式来避免并行访问。

在不同的隔离级别下，数据库行为是有所不同的。Oracle 数据库的默认隔离级别其实就是“读提交”，因此对于一些从 Oracle 迁移到 MySQL 的应用，为保证数据库隔离级别的一致，你一定要记得将 MySQL 的隔离级别设置为“读提交”。

	show variables like 'transaction_isolation'

根据业务场景选择不同的隔离级别

### 事务隔离实现 ###

假设一个值从1被按顺序改成2、3、4，在回滚日志会有如下的记录：

![d9c313809e5ac148fc39feff532f0fee](img/d9c313809e5ac148fc39feff532f0fee.png)

回滚日志删除条件：系统判断，当前没有事务再需要用到这些回滚日志时，回滚日志会被删除。当系统里没有比这个回滚日志更早的 read-view 的时候会被正式清除。

在 MySQL 中，实际上每条记录在更新的时候都会同时记录一条回滚操作。记录上的最新值，通过回滚操作，都可以得到前一个状态的值。

长事务意味着系统里面会存在很老的事务视图。由于这些事务随时可能访问数据库里面的任何数据，所以这个事务提交之前，数据库里面它可能用到的回滚记录都必须保留，这就会导致大量占用存储空间。在 MySQL 5.5 及以前的版本，回滚日志是跟数据字典一起放在 ibdata 文件里的，即使长事务最终提交，回滚段被清理，文件也不会变小。（我见过数据只有 20GB，而回滚段有 200GB 的库。最终只好为了清理回滚段，重建整个库。）还有占用锁资源，拖垮整个库的问题。

### 事务的启动方式 ###

MySQL 的事务启动方式有以下几种：

1. 显式启动事务语句， begin 或 start transaction。配套的提交语句是 commit，回滚语句是 rollback。
2. set autocommit=0，这个命令会将这个线程的自动提交关掉。意味着如果你只执行一个 select 语句，这个事务就启动了，而且并不会自动提交。这个事务持续存在直到你主动执行 commit 或 rollback 语句，或者断开连接。

在 autocommit 为 1 的情况下，用 begin 显式启动的事务，如果执行 commit 则提交事务。如果执行 commit work and chain，则是提交事务并自动启动下一个事务，这样也省去了再次执行 begin 语句的开销。同时带来的好处是从程序开发的角度明确地知道每个语句是否处于事务中。

information_schema 库的 innodb_trx 这个表中查询长事务，比如下面这个语句，用于查找持续时间超过 60s 的事务。

	select * from information_schema.innodb_trx where TIME_TO_SEC(timediff(now(),trx_started)) > 60

### 小结 ###

1. 介绍了 MySQL 的事务隔离级别的现象和实现
2. 根据实现原理分析了长事务存在的风险
3. 如何用正确的方式避免长事务

我给你留一个问题吧。你现在知道了系统里面应该避免长事务，如果你是业务开发负责人同时也是数据库负责人，你会有什么方案来避免出现或者处理这种情况呢？

### 精选留言 ###

#### Q ####

现在知道了系统里面应该避免长事务，如果你是业务开发负责人同时也是数据库负责人，你会有什么方案来避免出现或者处理这种情况呢？

#### A ####

少用长事务，尽量将高并发的表放在事务的后面。


## 04 | 深入浅出牵引（上） ##

索引的出现其实就是为了提高数据查询的效率，就像书的目录一样。对于数据库的表而言，索引其实就是它的“目录”。

### 索引的常见模型 ###

索引的出现是为了提高查询效率，三种常见的模型：

1. 哈希表
2. 有序数组
3. 搜索树

#### 哈希表 ####

哈希表是一种以键 - 值（key-value）存储数据的结构，输入待查找的值即 key，就可以找到其对应的值即 Value。哈希的思路很简单，把值放在数组里，用一个哈希函数把 key 换算成一个确定的位置，然后把 value 放在数组的这个位置。

多个key值经过哈希函数的换算，会出现同一个值的情况。处理这种情况的一种方法是，拉出一个链表。

哈希表这种结构适用于只有等值查询的场景

#### 有序数组 ####

有序数组在等值查询和范围查询场景中的性能就都非常优秀。

仅仅看查询效率，有序数组就是最好的数据结构。但需要更新数据的时候就往中间插入一个记录必需挪动后面所有的记录，成本很高。

有序数组索引只适用于静态存储引擎。

#### 搜索树 ####

二叉搜索树：每个节点的左儿子小于父节点，父节点又小于右儿子。

树可以有二叉，也可以有多叉。多叉树就是每个节点有多个儿子，儿子之间的大小保证从左到右递增。二叉树是搜索效率最高的，但是实际上大多数的数据库存储却并不使用二叉树。其原因是，索引不止存在内存中，还要写到磁盘上。

在MySQL中，索引是在存储引擎层实现的，所以并没有统一的索引标准，即不同存储的引擎的工作方式并不一样。而即使多个存储引擎支持同一种类型的索引，其底层的实现也可能不同。

为了让一个查询尽量少地读磁盘，就必须让查询过程访问尽量少的数据块。那么，我们就不应该使用二叉树，而是要使用“N 叉”树。这里，“N 叉”树中的“N”取决于数据块的大小。

#### 实战 ####

在 MySQL 中，索引是在存储引擎层实现的，所以并没有统一的索引标准，即不同存储引擎的索引的工作方式并不一样。而即使多个存储引擎支持同一种类型的索引，其底层的实现也可能不同。由于 InnoDB 存储引擎在 MySQL 数据库中使用最为广泛，所以下面我就以 InnoDB 为例，和你分析一下其中的索引模型。

### InnoDB的索引模型 ###

	mysql> create table t04(
	id int primary key, 
	k int not null, 
	name varchar(16),
	index (k))engine=InnoDB;

	insert into t04(id, k) values (100,1),(200,2),(300,3),(500,5),(600,6);

在 InnoDB 中，表都是根据主键顺序以索引的形式存放的，这种存储方式的表称为索引组织表。InnoDB 使用了 B+ 树索引模型，所以数据都是存储在 B+ 树中的。

每一个索引在 InnoDB 里面对应一棵 B+ 树。

* 主键索引的叶子节点存的是整行数据。在 InnoDB 里，主键索引也被称为聚簇索引（clustered index）。
* 非主键索引的叶子节点内容是主键的值。在 InnoDB 里，非主键索引也被称为二级索引（secondary index）。

#### 基于主键索引和普通索引的查询有什么区别？ ####

* 如果语句是 select * from T where ID=500，即主键查询方式，则只需要搜索 ID 这棵 B+ 树；
* 如果语句是 select * from T where k=5，即普通索引查询方式，则需要先搜索 k 索引树，得到 ID 的值为 500，再到 ID 索引树搜索一次。这个过程称为回表。
* 基于非主键索引的查询需要多扫描一棵索引树。因此，我们在应用中应该尽量使用主键查询。

### 索引维护 ###

B+ 树为了维护索引有序性，在插入新值的时候需要做必要的维护。

![dcda101051f28502bd5c4402b292e38d.png](img/dcda101051f28502bd5c4402b292e38d.png)

自增主键是指自增列上定义的主键，在建表语句中一般是这么定义的： NOT NULL PRIMARY KEY AUTO_INCREMENT。插入新纪录的时候可以不指定ID的值，系统会获取当前ID最大值加1作为下一条记录的ID值。

* 自增主键：符合了递增插入的场景，每次插入一条新纪录，都是追加操作，都不涉及到挪动其他记录，也不会触发叶子节点的分裂
* 业务逻辑的字段做主键：不容易保证有序插入，写数据成本相对较高

显然，主键长度越小，普通索引的叶子节点就越小，普通索引占用的空间也就越小。

从性能和存储空间方面考量，自增主键往往是更合理的选择。

1. 只有一个索引；
2. 该索引必须是唯一索引
3. 类似典型的KV场景
4. “尽量使用主键查询”原则，直接将这个索引设置为主键，可以避免每次查询需要搜索两棵树

### 小结 ###

InnoDB采用的B+树结构，以及InnoDB要选择B+树的原因。B+ 树能够很好地配合磁盘的读写特性，减少单次查询的磁盘访问次数。由于 InnoDB 是索引组织表，一般情况下我会建议你创建一个自增主键，这样非主键索引占用的空间最小。但事无绝对，我也跟你讨论了使用业务逻辑字段做主键的应用场景。

### 精选留言 ###

* 每一个表是好几棵B+树，树结点的key值就是某一行的主键，value是该行的其他数据。新建索引就是新增一个B+树，查询不走索引就是遍历主B+树。

#### Q ####

重建索引k

	alter table T drop index k;
	alter table T add index(k);

如果重建主键索引

	alter table T drop primary key;
	alter table T add primary key(id);

#### A ####

* 如果删除，新建主键索引，会同时去修改普通索引对应的主键索引，性能消耗比较大。
* 删除重建普通索引貌似影响不大，不过要注意在业务低谷期操作，避免影响业务。

#### Q ####

没有主键的表，有一个普通索引。怎么回表？

#### A ####

没有主键的表，innodb会给默认创建一个Rowid做主键

#### Q ####

“N叉树”的N值在MySQL中是可以被人工调整的么？

#### A ####

可以按照调整key的大小的思路来说；

如果你能指出来5.6以后可以通过page大小来间接控制应该能加分吧

面试回答不能太精减，计算方法、前缀索引什么的一起上😄

1， 通过改变key值来调整
N叉树中非叶子节点存放的是索引信息，索引包含Key和Point指针。Point指针固定为6个字节，假如Key为10个字节，那么单个索引就是16个字节。如果B+树中页大小为16K，那么一个页就可以存储1024个索引，此时N就等于1024。我们通过改变Key的大小，就可以改变N的值
2， 改变页的大小
页越大，一页存放的索引就越多，N就越大。

#### Q ####

定位到page，page内部怎么去定位行数据

#### A ####

内部有个有序数组，二分法

## 05 | 深入浅出牵引（下） ##

mysql> create table t05 (
ID int primary key,
k int NOT NULL DEFAULT 0, 
s varchar(16) NOT NULL DEFAULT '',
index k(k))
engine=InnoDB;

insert into t05 values(100,1, 'aa'),(200,2,'bb'),(300,3,'cc'),(500,5,'ee'),(600,6,'ff'),(700,7,'gg');

`select * from t05 where k between 3 and 5`，这条SQL查询查询语句的执行流程：

1. 在k索引树上找到k=3的记录，取得ID=300;（读了k索引树）
2. 再到ID索引树查到ID=300对应的R3;（回表）
3. 在k索引树取下一个值k=5，取得ID=500;（读了k索引树）
4. 再回到ID索引树查到ID=500对应的R4;（回表）
5. 在k索引树取下一个值k=6，不满足条件，循环结束。（读了k索引树）

回到主键索引树搜索的过程，我们称为回表。由于查询结果所需要的数据只在主键索引上有，所以不得不回表。经过性能优化来避免回表。

### 覆盖索引 ###

`select * from t05 where k between 3 and 5`

由于覆盖索引可以减少树的搜索次数，显著提升查询性能，所以使用覆盖索引是一个常用的性能优化手段。

索引字段的维护总是有代价的。在建立冗余索引来支持覆盖索引时就需要权衡考虑了。

	CREATE TABLE `t05_user` (
	  `id` int(11) NOT NULL,
	  `id_card` varchar(32) DEFAULT NULL,
	  `name` varchar(32) DEFAULT NULL,
	  `age` int(11) DEFAULT NULL,
	  `ismale` tinyint(1) DEFAULT NULL,
	  PRIMARY KEY (`id`),
	  KEY `id_card` (`id_card`),
	  KEY `name_age` (`name`,`age`)
	) ENGINE=InnoDB

身份证号是t05_user的唯一标识，可以通过身份证号查询所有的市民信息的需求。只要在身份证号字段上建立索引就够了。而再建立一个（身份证号、姓名）的联合索引，是不是浪费空间？如果有个高频请求，要根据市民的身份证号来查询他的性命，那么这个联合索引就有意义了。在高频请求上用到覆盖索引，避免回表查整行记录。

### 最左前缀原则 ###

B+ 树这种索引结构，可以利用索引的“最左前缀”，来定位记录。用(name, age)这个联合索引来分析。

在建立联合索引的时候，如何安排索引内的字段顺序。

1. 如果通过调整顺序，可以少维护一个索引，那么这个索引往往就是需要优先考虑采用。
	* 既有联合查询，又有基于a、b各自的查询：只有b的语句，是无法使用（a，b）这个联合索引，这时候不得不维护另外一个索引，同时维护（a，b）、（b）这两个索引
2. 考虑的原则就是空间

### 索引下推 ###

	mysql> select * from tuser where name like '张%' and age=10 and ismale=1;

* 在MySQL5.6之前，只能从ID3开始一个个回表。到主键索引上找出数据行，再对比字段值。
	* 在(name, age)索引里面，InnoDB并不会去看age的值，只是按顺序把“name第一个字是‘张’”的记录一条条取出来回表，需要回表4次。 
* MySQL5.6引入的索引下推优化（index condition pushdown），可以在索引遍历过程中，对索引中包含的字段先做判断，直接过滤掉不满足条件的记录，减少回表次数。
	*  InnoDB在（name， age）索引内部就判断了age是否等于10，对于不等于10的记录，直接判断并跳过。只需要对 ID4、ID5 这两条记录回表取数据判断，就只需要回表 2 次。

### 小结 ###

数据库索引的概念，包括了覆盖索引、前缀索引、索引下推。在满足语句需求的情况下， 尽量少地访问资源是数据库设计的重要原则之一。在使用数据库的时候，尤其是在设计表结构时，也要以减少资源消耗作为目标。

### 精选留言 ###

#### Q ####
	CREATE TABLE `t05_geek` (
	  `a` int(11) NOT NULL,
	  `b` int(11) NOT NULL,
	  `c` int(11) NOT NULL,
	  `d` int(11) NOT NULL,
	  PRIMARY KEY (`a`,`b`),
	  KEY `c` (`c`),
	  KEY `ca` (`c`,`a`),
	  KEY `cb` (`c`,`b`)
	) ENGINE=InnoDB;
	INSERT INTO `t05_geek` VALUES(1,2,3,d), (1,3,2,d),(1,4,3,d),(2,1,3,d),(2,2,2,d),(2,3,4,d);
#### A ####
索引 ca 的组织是先按 c 排序，再按 a 排序，同时记录主键–c--|–a--|–主键部分b-- （注意，这里不是 ab，而是只有 b）
2 1 3
2 2 2
3 1 2
3 1 4
3 2 1
4 2 3
这个跟索引 c 的数据是一模一样的。

索引 cb 的组织是先按 c 排序，在按 b 排序，同时记录主键–c--|–b--|–主键部分a-- （同上）
2 2 2
2 3 1
3 1 2
3 2 1
3 4 1
4 3 2
所以，结论是 ca 可以去掉，cb 需要保留。

## 06 | 全局锁和表锁：给表加个字段怎么有这么多阻碍 ##

数据库锁设计的初衷是处理并发问题，作为多用户共享的资源，当出现并发访问的时候，数据库需要合理地控制资源的访问规则。而锁就是用来实现这些访问规则的重要数据结构。

*根据加锁的范围，MySQL 里面的锁大致可以分成全局锁、表级锁和行锁三类。*

### 全局锁 ###

全局锁就是对整个数据库实例加锁。通过 Flush tables with read lock(FTWRL) 确保不会有其他线程对数据库做更新，然后对整个库做备份。注意，在备份过程中整个库完全处于只读状态。unlock tables解锁。

*全局锁的典型使用场景是，做全库逻辑备份。*（整库每个表都select出来存成文本）。

* 如果你在主库上备份，那么在备份期间都不能执行更新，业务基本上就得停摆；
* 如果你在从库上备份，那么备份期间从库不能执行主库同步过来的 binlog，会导致主从延迟。

如果不加锁的话，备份系统备份的得到的库不是一个逻辑时间点，这个视图是逻辑不一致的。

*一致性读是好，但前提是引擎要支持这个隔离级别。*对于MyISAM不支持事务的引擎，备份过程中有更新，总是只能取到最新的数据，破坏了备份的一致性。——FTWRL命令

single-transaction方法只适用于所有的表适用事务引擎的库。如果有的表适用了不支持事务的引擎，那么备份就只能通过FTWRL方法。（DBA要求业务开发人员适用InnoDB代替MyISAM的原因之一）。

*既然要全库只读，为什么不使用 set global readonly=true 的方式呢？*

1. 在有些系统中，readonly 的值会被用来做其他逻辑，比如用来判断一个库是主库还是备库。因此，修改 global 变量的方式影响面更大，我不建议你使用。
2. 在异常处理机制上有差异。如果执行 FTWRL 命令之后由于客户端发生异常断开，那么 MySQL 会自动释放这个全局锁，整个库回到可以正常更新的状态。而将整个库设置为 readonly 之后，如果客户端发生异常，则数据库就会一直保持 readonly 状态，这样会导致整个库长时间处于不可写状态，风险较高。

### 表级锁 ###

MySQL 里面表级别的锁有两种：

1. 表锁：`lock tables...read/write`。
	* lock table语法除了会限制别的进程的读写外，还限定了本线程接下来的操作对象，session A执行了`lock tables t1 read,t2 write;`session B写t1、读写t2的语句会被阻塞，session A在执行unlock tables之前，写t1不允许，也不能访问到其他表
	*  可以用 unlock tables 主动释放锁，也可以在客户端断开的时候自动释放。lock tables 语法除了会限制别的线程的读写外，也限定了本线程接下来的操作对象
	*  在客户端断开的时候自动释放
2. 元数据锁（meta data lock，MDL)。

对于InnoDB支持行锁的引擎，一般不使用lock tables命令来控制并发，毕竟锁住整个表的影响面很大。

*另一类表级的锁是 MDL（metadata lock)*。MDL不需要显式使用，在访问一个表的时候会自动加上。

MySQL5.5版本中引入了MDL，当对一个表做增删改查操作的时候，加上MDL读锁；当要对表做结构变更操作的时候，加MDL写锁。

* 读锁之间不互斥，因此可以有多个线程同时对一张表增删改查。
* 读写锁之间、写锁之间是互斥的，用来保证变更表结构操作的安全性。因此，如果有两个线程要同时给一个表加字段，其中一个要等另一个执行玩才能开始执行。

MDL锁是系统默认会加的。

给一个表加字段，或者修改字段，或者加索引，需要扫描全表的数据。

#### 如何安全地给小表加字段？ ####

1. 解决长事务，事务不提交，就会一直占着MDL锁。
	* 在 MySQL 的 information_schema 库的 innodb_trx 表中，查到当前执行中的事务。
	* 如果你要做 DDL 变更的表刚好有长事务在执行，要考虑先暂停 DDL，或者 kill 掉这个长事务。
2. 变更的表是一个热点表（数据量不大，但是上面的请求很频繁，不得不加字段）
	* kill未必管用，新的请求马上就来了
	* 在alert table语句里面设定等待时间（如果在这个指定的等待时间里面能够拿到 MDL 写锁最好，拿不到也不要阻塞后面的业务语句，先放弃）
	* repeat

具体语句：

	ALTER TABLE tbl_name NOWAIT add column ...
	ALTER TABLE tbl_name WAIT N add column ... 

### 小结 ###

介绍了 MySQL 的全局锁和表级锁

全局锁主要用在逻辑备份过程中。对于全部是 InnoDB 引擎的库，我建议你选择使用–single-transaction 参数。

表锁一般是在数据库引擎不支持行锁的时候才会被用到的。lock tables

* 要么是你的系统现在还在用 MyISAM 这类不支持事务的引擎，那要安排升级换引擎；
* lock tables 和 unlock tables 改成 begin 和 commit

MDL 会直到事务提交才释放，在做表结构变更的时候，你一定要小心不要导致锁住线上查询和更新。

## 07 | 行锁功过：怎么减少行锁对性能的影响？ ##

MySQL 的行锁是在引擎层由各个引擎自己实现的。但并不是所有的引擎都支持行锁，比如 MyISAM 引擎就不支持行锁。不支持行锁意味着并发控制只能使用表锁，对于这种引擎的表，同一张表上任何时刻只能有一个更新在执行，这就会影响到业务并发度。InnoDB 是支持行锁的，这也是 MyISAM 被 InnoDB 替代的重要原因之一。

InnoDB 的行锁，以及如何通过减少锁冲突来提升业务并发度。行锁就是针对数据表中行记录的锁。比如事务 A 更新了一行，而这时候事务 B 也要更新同一行，则必须等事务 A 的操作完成后才能进行更新。

### 从两阶段锁说起 ###

事务A在执行完两条update语句后，持有了那些锁，都是在commit的时候才释放的。

在InnoDB事务中，行锁是在需要的时候才加上的，但并不是不需要了就立刻释放，而是要等到事务结束时才释放。这个就是两阶段锁协议。

### 死锁和死锁检测 ###

当并发系统中不同线程出现循环资源依赖，涉及的线程都在等待别的线程释放资源时，就会导致这几个线程都进入无限等待的状态，称为死锁。

在InnoDB事务中，行锁是在需要的时候才加上的，但并不是不需要就立刻释放，而是要等到事务结束才释放。这个就是两阶段锁协议。所以*如果事务中需要锁多个行，要把最可能造成锁冲突、最可能影响并发度的锁尽量往后放*。

1. 直接进入等待，直到超时。这个超时时间可以通过参数 innodb_lock_wait_timeout 来设置。
2. 发起死锁检测，发现死锁后，主动回滚死锁链条中的某一个事务，让其他事务得以继续执行。将参数 innodb_deadlock_detect 设置为 on，表示开启这个逻辑。

在InnoDB中，innodb_lock_wait_timeout的默认值是50s（如果采用第一个策略，当出现死锁以后，第一个被锁住的线程要过 50s 才会超时退出，然后其他线程才有可能继续执行）——对于在线服务来说，这个等待时间往往是无法接受的。

每个新来的被堵住的线程，都要判断会不会由于自己的加入导致了死锁，这是一个时间复杂度是 O(n) 的操作。假设有 1000 个并发线程要同时更新同一行，那么死锁检测操作就是 100 万这个量级的。虽然最终检测的结果是没有死锁，但是这期间要消耗大量的 CPU 资源。

1. 如果确保这个业务一定不会出现死锁，可以临时把死锁检测关掉
2. 控制并发度：这个并发控制要做在数据库服务端
	* 在中间件则考虑在中间件实现
	* 如果可以修改MySQL源码，则在MySQL里面。在进入引擎之前排队

如果团队里暂时没有数据库方面的专家，不能实现这样的方案，能不能从设计上优化这个问题呢？

将一行改成逻辑上的多行来减少锁冲突，还是以影院账户为例，可以考虑放在多条记录上，比如 10 个记录，影院的账户总额等于这 10 个记录的值的总和。

### 小结 ###

MySQL的行锁，涉及了两阶段锁协议、死锁和死锁检测这两大部分内容。

1. 两阶段协议为起点
2. 如果事务中需要多个行，把最可能造成锁冲突、最可能影响并发度的锁的申请时机尽量往后放
3. 调整语句顺序并不能完全避免死锁
	* 死锁
	* 死锁的检测

### 精选留言 ###

#### Q ####

最后，我给你留下一个问题吧。如果你要删除一个表里面的前 10000 行数据，有以下三种方法可以做到：
1. 直接执行 delete from T limit 10000;
2. 在一个连接中循环执行 20 次 delete from T limit 500;
3. 在 20 个连接中同时执行 delete from T limit 500。

#### A ####

1. 单一线程很长，会阻塞其他人操作
2. 增加并发性
3. 互斥，人为制造冲突

#### Q ####

对于行锁来说。两个update同时更新一条数据是互斥的。这个是因为多种锁同时存在时，以粒度最小的锁为准的原因么？

#### A ####

不是“以粒度最小为准”，而是如果有多种锁，必须得“全部不互斥”才能并行，只要有一个互斥，就得等。

## 08 | 事务到底是隔离的还是不隔离的？ ##

	mysql> CREATE TABLE `t08` (
	  `id` int(11) NOT NULL,
	  `k` int(11) DEFAULT NULL,
	  PRIMARY KEY (`id`)
	) ENGINE=InnoDB;
	insert into t08(id, k) values(1,1),(2,2);

`begin/start transaction`命令并不是一个事务的起点，在执行到它们之后的第一个操作InnoDB表的语句，事务才真正启动。如果想要马上启动一个事务，可以使用`start transaction with consistent snapshot` 这个命令。

1. 一致性视图是在执行第一个快照读语句时创建的；
2. 一致性视图是在执行 start transaction with consistent snapshot 时创建的。

在MySQL里，有两个“视图”的概念：

* view，是一个用查询语句定义的虚拟表，在调用的时候执行查询语句并生成结果。创建视图的语法是`create view...`，而它的查询方法与表一样。
* InnoDB在实现MVCC时用到的一致性读视图，即consitent read view，用于支持RC（Read Committed，读操作）和RR（Repeatable Read，可重复读）隔离级别的实现。

没有物理结构，作用的是事务执行期间用来定义“我能看到什么数据”。

### “快照”在MVCC里怎么工作的？ ###

在可重复读隔离级别下，事务在启动的时候就“拍了个快照”。注意，这个快照是基于整库的。

* transaction id，InnoDB 里面每个事务有一个唯一的事务 ID，在事务开始的时候向 InnoDB 的事务系统申请的，是按申请顺序严格递增的。
* row trx_id，每行数据也都是有很多版本的。每次事务更新数据的时候，都会生成一个新的数据版本；旧的数据版本要保留，并且在新的数据版本中，能够有信息可以直接拿到；数据表中的一行记录，其实可能有多个版本（row），每个版本有自己的row_trx_id。

![9416c310e406519b7460437cb0c5c149.png](img/9416c310e406519b7460437cb0c5c149.png)

1. 事务C[99, 100, 101, 102]、事务B[99, 100, 101]和事务A[99, 100]
2. 第一个有效更新是事务C，数据的最新版本的row trx_id是102
3. 第二个有效更新是事务B，row trx_id是101
4. 第三个有效更新是事务A

----

* 版本位提交，不可见
* 版本已提交，但是是在视图创建后提交，不可见
* 版本已提交，而且是在视图创建前提交的，可见

#### undo log在哪呢？ ####

对于当前事务的启动瞬间来说，一个数据版本的row_trx_id，有以下几种可能：

1. 已提交的事务或者当前的事务自己生成的，这个数据是可见的
2. 将来启动的事务生成的，是肯定不可兼得
3. 未提交事务集合
	* 若 row trx_id 在数组中，表示这个版本是由还没提交的事务生成的，不可见；
	* 若 row trx_id 不在数组中，表示这个版本是已经提交了的事务生成的，可见。 

*InnoDB 利用了“所有数据都有多个版本”的这个特性，实现了“秒级创建快照”的能力。*

InnnoDB为每个事务构造了一个数组，用来保存这个事务启动瞬间，当前正在“活跃”的所有事务ID。“活跃”指的就是，启动了但还没提交。数组里面事务ID的最小值记为低水位，当前系统里面已经创建过的事务ID的最大值加1记为高水位。

### 更新逻辑 ###

*事务 B 的 update 语句，如果按照一致性读，好像结果不对哦？*

*更新数据都是先读后写的，而这个读，只能读当前的值，称为“当前读”（current read）。*

读提交的逻辑和可重复读的逻辑类似，最主要的区别是：

1. 在可重复读隔离级别下，只需要在事务开始的时候创建一致性视图，之后事务里的其他查询都共用这个一致性视图；
2. 在读提交隔离级别下，每一个语句执行前都会重新算出一个新的视图。

### 小结 ###

InnoDB 的行数据有多个版本，每个数据版本有自己的 row trx_id，每个事务或者语句有自己的一致性视图。普通查询语句是一致性读，一致性读会根据 row trx_id 和一致性视图确定数据版本的可见性。

* 对于可重复读，查询只承认在事务启动前就已经提交完成的数据；
* 对于读提交，查询只承认在语句启动前就已经提交完成的数据；

### 精选留言 ###

#### Q ####

下面的表结构和初始化语句作为试验环境，事务隔离级别是可重复读。现在，要把所有“字段c和id值相等的行”的c值清零，但是却发现了一个 “诡异”的、改不掉的情况

	mysql> CREATE TABLE `t08_home` (
	  `id` int(11) NOT NULL,
	  `c` int(11) DEFAULT NULL,
	  PRIMARY KEY (`id`)
	) ENGINE=InnoDB;
	insert into t08_home(id, c) values(1,1),(2,2),(3,3),(4,4);

1. 使用sessionA和sessionB
2. 通过sessionA进行线程更新，通过使用线程A进行更新数据为0，并使用sleep
3. 通过线程B进行数据检索
4. 因为事务隔离级别是可重复读，当他sessionA使用更新，会创建view，而sessionB是不可见的，所以sessionB检索时只能查到1，2，3，4原始值，而不是0，0，0，0

#### A ####

思考题，RR下，用另外一个事物在update执行之前，先把所有c值修改，应该就可以。比如update t set c = id + 1。
这个实际场景还挺常见——所谓的“乐观锁”。时常我们会基于version字段对row进行cas式的更新，类似update ...set ... where id = xxx and version = xxx。如果version被其他事务抢先更新，则在自己事务中更新失败，trx_id没有变成自身事务的id，同一个事务中再次select还是旧值，就会出现“明明值没变可就是更新不了”的“异象”（anomaly）。解决方案就是每次cas更新不管成功失败，结束当前事务。如果失败则重新起一个事务进行查询更新。

#### Q ####

请教一个问题，业务上有这样的需求，A、B两个用户，如果互相喜欢，则成为好友。设计上是有两张表，一个是like表，一个是friend表，like表有user_id、liker_id两个字段，我设置为复合唯一索引即uk_user_id_liker_id。语句执行顺序是这样的：
以A喜欢B为例：
1、先查询对方有没有喜欢自己（B有没有喜欢A）
select * from like where user_id = B and liker_id = A
2、如果有，则成为好友
insert into friend
3、没有，则只是喜欢关系
insert into like

如果A、B同时喜欢对方，会出现不会成为好友的问题。因为上面第1步，双方都没喜欢对方。第1步即使使用了排他锁也不行，因为记录不存在，行锁无法生效。请问这种情况，在mysql锁层面有没有办法处理

#### A ####

# 实践篇 #

## 09 | 普通索引和唯一索引，应该怎么选择？ ##

	select name from CUser where id_card = 'xxxxxxxyyyyyyzzzzz';

id_card字段比较大，不建议当主键，可以有两个选择，要么给id_card字段创建唯一索引，要么创建普通索引。

### 查询过程 ###

* 对于普通索引来说，查找到满足条件的第一个记录（5,500）后，需要查找下一个记录，直到碰到第一个不满足k=5条件的记录。
* 对于唯一索引来说，由于索引定义了唯一性，查找到第一个满足条件的记录后，就会停止继续检索。

InnoDB 的数据是按数据页为单位来读写的。当需要读一条记录的时候，并不是将这个记录本身从磁盘读出来，而是以页为单位，将其整体读入内存。在 InnoDB 中，每个数据页的大小默认是 16KB。

查询语句在索引树上查找的过程

1. 通过 B+ 树从树根开始
2. 按层搜索到叶子节点
3. 数据页通过二分法定位记录

这里普通索引和唯一索引影响微乎其微。

### 更新过程 ###

当需要更新一个数据页时，如果数据页在内存中就直接更新，而如果这个数据页还没有在内存中的话，在不影响数据一致性的前提下，InooDB 会将这些更新操作缓存在 change buffer 中，这样就不需要从磁盘中读入这个数据页了。在下次查询需要访问这个数据页的时候，将数据页读入内存，然后执行 change buffer 中与这个页有关的操作。通过这种方式就能保证这个数据逻辑的正确性。

change buffer：可以持久化的数据，在内存中有拷贝，也会被写入到磁盘上。

将change buffer中的操作应用到原数据页，得到最新结果的过程称为merge。除了访问这个数据页会触发merge外，系统有后台线程会定期merge。在数据库正常关闭（shutdown）的过程中，也会执行 merge 操作。

将更新操作先记录在 change buffer，减少读磁盘，语句的执行速度会得到明显的提升。而且，数据读入内存是需要占用 buffer pool 的，所以这种方式还能够避免占用内存，提高内存利用率。

*什么条件下可以使用 change buffer 呢？*

一索引的更新就不能使用 change buffer，实际上也只有普通索引可以使用。

1. 记录要在更新的目标页在内存中
	* 对于唯一索引来说，找到 3 和 5 之间的位置，判断到没有冲突，插入这个值，语句执行结束；
	* 对于普通索引来说，找到 3 和 5 之间的位置，插入这个值，语句执行结束。
	* 普通索引和唯一索引对更新语句性能影响的差别，只是一个判断，只会耗费微小的 CPU 时间。
2. 记录要更新的目标页不在内存中
	* 对于唯一索引来说，需要将数据页读入内存，判断到没有冲突，插入这个值，语句执行结束；
	* 对于普通索引来说，则是将更新记录在 change buffer，语句执行就结束了。

将数据从磁盘读入内存涉及随机 IO 的访问，是数据库里面成本最高的操作之一。change buffer 因为减少了随机磁盘访问，所以对更新性能的提升是会很明显的。

*这时候唯一索引损耗十分明显*

#### change buffer 的使用场景 ####

使用 change buffer 对更新过程的加速作用，也清楚了 change buffer 只限于用在普通索引的场景下，而不适用于唯一索引。

* 对于写多读少的业务来说，页面在写完以后马上被访问到的概率比较小，此时 change buffer 的使用效果最好。
* 一个业务的更新模式是写入之后马上会做查询，那么即使满足了条件，将更新记录先记录在change buffer，但之后由于马上要访问这个数据页，会立即触发merge过程。这个随机访问IO的次数不会减少，反而增加了change buffer的维护代价（副作用）

#### 索引选择和实践 ####

普通索引和唯一索引应该怎么选择。这两类索引在查询能力上是没差别的，主要考虑的是对更新性能的影响。尽量选择普通索引。

如果所有的更新后面，都马上伴随着对这个记录的查询，那么你应该关闭 change buffer。而在其他情况下，change buffer 都能提升更新性能。

普通索引和 change buffer 的配合使用，对于数据量大的表的更新优化还是很明显的。

### change buffer 和 redo log ###

* redo log 主要节省的是随机写磁盘的 IO 消耗（转成顺序写）
* change buffer 主要节省的则是随机读磁盘的 IO 消耗。

TODO 未理解

### 小结 ###

1. 从普通索引和唯一索引的选择开始
2. 分享了数据的查询和更新过程
3. change buffer 的机制以及应用场景
4. 索引选择的实践

由于唯一索引用不上 change buffer 的优化机制，因此如果业务可以接受，从性能角度出发我建议你优先考虑非唯一索引。最后，又到了思考题时间。

* 保证业务正确性
* 在一些“归档库”的场景，可以考虑使用普通索引

### 精选留言 ###

#### A ####

change buffer的前身是insert buffer，只能对insert操作优化，后来增加了update/delete的支持。

#### A ####

* 系统表空间就是用来放系统信息的，比如数据字典什么的，对应的磁盘文件是ibdata1,
* 数据表空间就是一个个的表数据文件，对应的磁盘文件就是 表名.ibd

#### Q ####

change buffer相当于推迟了更新操作，那对并发控制相关的是否有影响，比如加锁？我一直以为加锁需要把具体的数据页读到内存中来，才能加锁，然而并不是？

#### A ####

锁是一个单独的数据结构，如果数据页上有锁，change buffer 在判断“是否能用”的时候，就会认为否。

## 10 | MySQL为什么有时候会选错索引？ ##

表定义：

	CREATE TABLE `t10` (
	  `id` int(11) NOT NULL,
	  `a` int(11) DEFAULT NULL,
	  `b` int(11) DEFAULT NULL,
	  PRIMARY KEY (`id`),
	  KEY `a` (`a`),
	  KEY `b` (`b`)
	) ENGINE=InnoDB;

存储过程定义：

	delimiter ;;
	create procedure idata()
	begin
	  declare i int;
	  set i=1;
	  while(i<=100000)do
	    insert into t values(i, i, i);
	    set i=i+1;
	  end while;
	end;;
	delimiter ;
	call idata();

SQL语句：

	mysql> select * from t10 where a between 10000 and 20000;

explain命令看到的这条语句的执行情况：

	mysql> explain select * from t10 where a between 10000 and 20000;

key这个字段值是'a'，表示优化器选择了索引a。

|session A| session B|
|--|--|
|start transaction with consistent snapshot;||
||delete from t;|
||call idata();|
|||
||explain select * from t10 where a between 10000 and 20000;|
|commit;||

此时session B的查询语句`select * from t10 where a between 10000 and 2000`不会再选择索引a。

### 优化器的逻辑 ###

选择索引是优化器的工作。而优化器选择索引的目的，是找到一个最优的执行方案，并用最小的代价去执行语句。在数据库里面，扫描行数是影响执行代价的因素之一。扫描的行数越少，意味着访问磁盘数据的次数越少，消耗的 CPU 资源越少。扫描行数也不是唯一的判断标准，优化器还会结合是否使用临时表、是否排序等因素进行综合判断。

MySQL选错索引在判断扫描行数的时候出了问题，*扫描行数是怎么判断的？*

基数：一个索引上不同的值，这个索引的区分度就越好。而一个索引上不同的值的个数。基数越大，索引的区分度越好。（MySQL再真正执行语句之前，并不能精确地知道满足这个条件的记录有多少条，而只能根据统计信息来计算记录数。）

使用`show index`方法，看到索引的基数。

### 索引选择异常和处理 ###

1. 采用 force index 强行选择一个索引。MySQL会根据语法解析的结果分析可能使用的索引作为候选项，然后在候选列表中依次判断每个索引需要扫面多少行
2. 修改语句，引导MySQL使用期望的索引
3. 在有些场景下，我们可以新建一个更合适的索引，来提供给优化器做选择，或删掉误用的索引

### 小结 ###

索引统计的更新机制，优化器存在选错索引的可能性。

* 对于索引统计信息不准确导致的问题，用analyze table来解决。
* 其他优化器误判的情况，可以在应用端用force index来强行指定索引，也可以用过修改语句来引导优化器，还可以通过增加或者删除索引来绕过这个问题。

### 精选留言 ###

#### Q ####

前面我们在构造第一个例子的过程中，通过 session A 的配合，让 session B 删除数据后又重新插入了一遍数据，然后就发现 explain 结果中，rows 字段从 10001 变成 37000 多。而如果没有 session A 的配合，只是单独执行 delete from t 、call idata()、explain 这三句话，会看到 rows 字段其实还是 10000 左右。你可以自己验证一下这个结果。

#### A ####

## 11 | 怎么给字符串字段加索引？ ##

	mysql> create table t11(
	ID bigint unsigned primary key,
	email varchar(64), 
	... 
	)engine=innodb; 

	mysql> select f1, f2 from SUser where email='xxx';

MySQL是支持前缀索引的，定义字符串的一部分为索引。默认地，如果创建索引的语句不指定前缀长度，那么索引就会包含整个字符串。

	mysql> alter table SUser add index index1(email);
第一个语句创建的 index1 索引里面，包含了每个记录的整个字符串

	mysql> alter table SUser add index index2(email(6));
第二个语句创建的 index2 索引里面，对于每个记录都是只取前 6 个字节。


	select id,name,email from SUser where email='zhangssxyz@xxx.com';

如果使用的是index1（即 email 整个字符串的索引结构），执行顺序是这样的：

1. 从 index1 索引树找到满足索引值是’zhangssxyz@xxx.com’的这条记录，取得 ID2 的值；
2. 到主键上查到主键值是 ID2 的行，判断 email 的值是正确的，将这行记录加入结果集；
3. 取 index1 索引树上刚刚查到的位置的下一条记录，发现已经不满足 email='zhangssxyz@xxx.com’的条件了，循环结束。

只需要回主键索引取一次数据，所以系统认为只扫描了一行。

如果使用的是index2（即 email(6) 索引结构），执行顺序是这样的：

1. 从 index2 索引树找到满足索引值是’zhangs’的记录，找到的第一个是 ID1；
2. 到主键上查到主键值是 ID1 的行，判断出 email 的值不是’zhangssxyz@xxx.com’，这行记录丢弃；
3. 取 index2 上刚刚查到的位置的下一条记录，发现仍然是’zhangs’，取出 ID2，再到 ID 索引上取整行然后判断，这次值对了，将这行记录加入结果集；
4. 重复上一步，直到在 idxe2 上取到的值不是’zhangs’时，循环结束。

**使用前缀索引，定义好长度，就可以做到既节省空间，又不用额外增加太多的查询成本。**

我们在建立索引时关注的是区分度，区分度越高越好。因为区分度越高，意味着重复的键值越少。因此，我们可以通过统计索引上有多少个不同的值来判断要使用多长的前缀。

1. 可以使用`mysql> select count(distinct email) as L from SUser;`算出列上有多少个不同值
2. 依次选取不同长度的前缀来查看这个值。

		mysql> select 
		  count(distinct left(email,4)）as L4,
		  count(distinct left(email,5)）as L5,
		  count(distinct left(email,6)）as L6,
		  count(distinct left(email,7)）as L7,
		from SUser;

3. 预设一个可接受的损失区分度的比例

### 前缀索引对覆盖索引的影响 ###

使用前缀索引可能会增加扫描行数，这会影响到性能。使用前缀索引就用不上覆盖索引对查询性能的优化。

### 其他方式 ###

遇到前缀的区分度不够好的情况时：

1. 使用倒序存储。
	* 如果存储身份证号的时候倒过来存，每次查询的时候，也这么写：
    mysql> select field_list from t where id_card = reverse('input_id_card_string');
2. 使用hash字段。
	* 可以在表上再创建一个整数字段，来保存身份证的校验码，同时在这个字段上创建索引。
    mysql> alter table t add id_card_crc int unsigned, add index(id_card_crc);
	* 插入新记录的时候，都同时使用crc32()函数得到校验码填到这个新字段。

### 小结 ###

1. 直接创建完整索引，比较占用空间
2. 创建前缀索引，节省空间，但会增加查询扫描次数，并且不能使用覆盖索引
3. 倒序存储，再创建前缀索引，用于绕过字符串本身前缀的区分度不够的问题
4. 创建hash字段索引，查询性能稳定，有额外的存储和计算消耗，跟第三种方式一样，都不支持范围扫描

### 精选留言 ###

#### Q ####

如果你在维护一个学校的学生信息数据库，学生登录名的统一格式是”学号 @gmail.com", 而学号的规则是：十五位的数字，其中前三位是所在城市编号、第四到第六位是学校编号、第七位到第十位是入学年份、最后五位是顺序编号。系统登录的时候都需要学生输入登录名和密码，验证正确后才能继续使用系统。就只考虑登录验证这个行为的话，你会怎么设计这个登录名的索引呢？

#### A ####

1. 第7到第10位是入学年份，最后5位是顺序编号。考虑到前缀索引区分度不大，可以使用倒序索引进行区分。倒序8位。
2. 原谅我偷懒的想法，一个学校每年预估2万新生，50年才100万记录，能节省多少空间，直接全字段索引。省去了开发转换及局限性风险，碰到超大量迫不得已再用后两种办法

## 12 | 为什么我的MySQL会“抖”一下？ ##

*你的SQL语句为什么变“慢”了*

WAL机制：InnoDB 在处理更新语句的时候，只做了写日志这一个磁盘操作。这个日志叫作 redo log（重做日志）

**当内存数据页跟磁盘数据页内容不一致的时候，我们称这个内存页为“脏页”。内存数据写入到磁盘后，内存和磁盘上的数据页的内容就一致了，称为“干净页”。**

### InnoDB 刷脏页的控制策略 ###

用到 innodb_io_capacity 这个参数了，告诉 InnoDB 你的磁盘能力。这个值我建议你设置成磁盘的 IOPS。磁盘的 IOPS 可以通过 fio 这个工具来测试，下面的语句是我用来测试磁盘随机读写的命令：

	fio -filename=$filename -direct=1 -iodepth 1 -thread -rw=randrw -ioengine=psync -bs=16k -size=500M -numjobs=10 -runtime=10 -group_reporting -name=mytest 

**如果你来设计策略控制刷脏页的速度，会参考哪些因素呢？**

1. 首先是内存脏页太多，其次是 redo log 写满。
2. InnoDB 的刷盘速度就是要参考这两个因素：一个是脏页比例，一个是 redo log 写盘速度。

InnoDB 会根据这两个因素先单独算出两个数字：

参数 innodb_max_dirty_pages_pct 是脏页比例上限，默认值是 75%。

在 MySQL 8.0 中，innodb_flush_neighbors 参数的默认值已经是 0 了。

### 小结 ###

解释了这个机制后续需要的刷脏页操作和执行时机。利用 WAL 技术，数据库将随机写转换成了顺序写，大大提升了数据库的性能。

由此也带来了内存脏页的问题。脏页会被后台线程自动 flush，也会由于数据页淘汰而触发 flush，而刷脏页的过程由于会占用资源，可能会让你的更新和查询语句的响应时间长一些。

### 精选留言 ###


#### 1 ####

**Q**
当内存不够用了，要将脏页写到磁盘，会有一个数据页淘汰机制（最久不使用），假设淘汰的是脏页，则此时脏页所对应的redo log的位置是随机的，当有多个不同的脏页需要刷，则对应的redo log可能在不同的位置，这样就需要把redo log的多个不同位置刷掉，这样对于redo log的处理不是就会很麻烦吗？（合并间隙，移动位置？）
另外，redo log的优势在于将磁盘随机写转换成了顺序写，如果需要将redo log的不同部分刷掉（刷脏页），不是就在redo log里随机读写了么？

**A**
* 其实由于淘汰的时候，刷脏页过程不用动redo log文件的。
* 这个有个额外的保证，是redo log在“重放”的时候，如果一个数据页已经是刷过的，会识别出来并跳过。

#### 2 ####

redo log是关系型数据库的核心啊,保证了ACID里的D。所以redo log是牵一发而动全身的操作
按照老师说的当内存数据页跟磁盘数据页不一致的时候,把内存页称为'脏页'。如果redo log
设置得太小,redo log写满.那么会涉及到哪些操作呢,我认为是以下几点:
1. 把相对应的数据页中的脏页持久化到磁盘,checkpoint往前推
2. 由于redo log还记录了undo的变化,undo log buffer也要持久化进undo log
3. 当innodb_flush_log_at_trx_commit设置为非1,还要把内存里的redo log持久化到磁盘上
4. redo log还记录了change buffer的改变,那么还要把change buffer purge到idb
以及merge change buffer.merge生成的数据页也是脏页,也要持久化到磁盘
上述4种操作,都是占用系统I/O,影响DML,如果操作频繁,会导致'抖'得向现在我们过冬一样。
但是对于select操作来说,查询时间相对会更快。因为系统脏页变少了,不用去淘汰脏页,直接复用
干净页即可。还有就是对于宕机恢复,速度也更快,因为checkpoint很接近LSN,恢复的数据页相对较少
所以要控制刷脏的频率,频率快了,影响DML I/O,频率慢了,会导致读操作耗时长。
我是这样想的这个问题,有可能不太对,特别是对于第4点是否会merge以及purge,还需要老师的解答

## 13 | 为什么表数据删掉一半，表文件大小没变 ##

数据库表的空间回收

一个 InnoDB 表包含两部分，即：表结构定义和数据。在 MySQL 8.0 版本以前，表结构是存在以.frm 为后缀的文件里。而 MySQL 8.0 版本，则已经允许把表结构定义放在系统数据表中了。因为表结构定义占用的空间很小。

### 参数innodb_file_per_table ###

数据既可以存在共享表空间里，也可以是单独的文件。这个行为是由参数 innodb_file_per_table 控制的：

1. 这个参数设置为 OFF 表示的是，表的数据放在系统共享表空间，也就是跟数据字典放在一起；即使表删掉了，空间也是不会回收的
2. 这个参数设置为 ON 表示的是，每个 InnoDB 表数据存储在一个以 .ibd 为后缀的文件中；通过drop table命令，系统就会直接删除这个文件。

从MySQL5.6.6版本开始，它的默认值就是ON了。

*将 innodb_file_per_table 设置为 ON，是推荐做法，我们接下来的讨论都是基于这个设置展开的。*

删除表的时候，可以使用drop table命令来回收空间；但删除数据的场景往往是删除某些行（表空间却没有被回收）

### 数据删除流程 ###

InnoDB 里的数据都是用 B+ 树的结构组织的。

如果删掉R4这个记录，InnoDB引擎只会把R4这个记录标记为删除。InnoDB的数据是按页存储的，那么删掉了一个数据页上的所有记录，则整个数据页就可以被复用了。

数据页的复用跟记录的复用是不同的。

不止是删除数据会造成空洞，插入数据也会。

### 重建表 ###

如果现在有一个表A，需要做空间收缩，为了把表中存在的空洞去掉。

* 可以新建一个与表 A 结构相同的表 B，然后按照主键 ID 递增的顺序，把数据一行一行地从表 A 里读出来再插入到表 B 中。
* 

使用 alter table A engine=InnoDB 命令来重建表。

### Online 和 inplace ###



### 小结 ###

数据库中收缩表空间的方法。

1. 如果要收缩一个表，只是 delete 掉表里面不用的数据的话，表文件的大小是不会变的。
2. 通过 alter table 命令重建表，才能达到表文件变小的目的。

重建表的两种实现方式：

* Online DDL 的方式是可以考虑在业务低峰期使用的
* MySQL 5.5 及之前的版本，这个命令是会阻塞 DML 的

### 精选留言 ###

#### Q ####

假设现在有人碰到了一个“想要收缩表空间，结果适得其反”的情况，看上去是这样的：

1. 一个表 t 文件大小为 1TB；
2. 对这个表执行 alter table t engine=InnoDB；
3. 发现执行完成后，空间不仅没变小，还稍微大了一点儿，比如变成了 1.01TB。

#### A ####

#### Q ####

#### A ####

## 14 | count(*)这么慢，我该怎么办？ ##

### count(*)的实现方式 ###

在不同的MySQL引擎中，count(*)有不同的实现方式。

* MyISAM引擎把一个表的总行数存在了磁盘上，因此执行count(*)的时候会直接返回这个数，效率很高；
* 而InnoDB引擎执行count(*)的时候，需要把数据一行一行地从引擎里面读出来，然后累积计数。

事务支持、并发能力还是在数据安全方面，InnoDB都优于MyISAM

#### 为什么InnoDB不跟MyISAM一样，也把数字存起来呢？ ####

因为即使在同一时刻的多个查询，由于多版本并发控制（MVCC）的原因，InnoDB表“应该返回多少行”也是不确定的。

假如设计三个用户并行的会话，可能出现拿到的结果却不同。因为InnoDB的事务设计有关系，可重复读是默认的隔离级别。每一行记录都要判断自己是否对这个会话可见，因此对于count(*)请求来说，InnoDB只好把数据一行一行读出来依次判断，可见的行才能够用于计算“基于这个查询”的表的总行数

MySQL在执行count(*)操作的时候做了优化。InnoDB是索引组织表，主键索引树的叶子节点是数据，而普通索引树比主键索引树少很多。对于count(*)这样的操作，遍历哪个索引数得到的结果逻辑上都是一样的。MySQL优化器会找到最小的那棵树来遍历。在保证逻辑正确的前提下，尽量减少扫描的数据量，是数据库系统设计的通用法则之一。

show table status，输出结果里面有一个TABLE_ROWS用于显示这个表当前有多少行。（show table status命令显示的行数也不能直接使用）

### 小结 ###

* MyISAM表虽然count(*)很快，但是不支持事务；
* show table status 命令虽然返回很快，但是不准确；
* InnoDB 表直接 count(*) 会遍历全表，虽然结果准确，但会导致性能问题。

### 用缓存系统保存计数 ###

将计数保存在缓存系统中的方式，还不只是丢失更新的问题。即使Redis正常工作，这个值还是逻辑上不精确的。

1. 缓存可能会丢失更新，
2. 逻辑上值不精确

### 在数据库保存计数 ###

把计数直接放到数据库单独里的一张计数表C中

1. 解决了崩溃丢失的问题，InnoDB是支持崩溃恢复不丢数据的
2. 利用InnoDB的事务特性解决查计数值和“最近100条记录”看到的结果

### 不同的count用法 ###

count(*)、count(主键 id)、count(字段) 和 count(1) 等不同用法的性能

1. count()是一个聚合函数，对于返回的结果集，一行行地判断，如果count函数的参数不是NULL，累计值就加1，否则不加。最后返回累计值。

至于分析性能差别，可以记住这几个原则：

1. server层要什么就给什么；
2. InnoDB 只给必要的值；
3. 现在的优化器只优化了 count(*) 的语义为“取行数”，其他“显而易见”的优化并没有做。

---

* 对于 count(主键 id) 来说：
* 对于 count(1) 来说：
* 对于 count(字段) 来说：
	1. 如果这个“字段”是定义为not null的话，一行行地从记录里面读出这个字段，判断不能为null，按行累加；
	2. 如果这个“字段”定义为null，那么执行的时候，判断到有可能是null，还要把值取出来再判断一下，不是null才累加。
* 但是 count(*) 是例外：不会把全部字段取出来，专门做了优化，不取值。count(*)肯定不是null，按行累加。

按照效率排序的话，count(字段)<count(主键 id)<count(1)≈count(*)，所以我建议你，尽量使用 count(*)。

### 小结 ###

1. 不同引擎中count(*)的实现方式不一样
2. 用缓存系统来存储计数值存在问题——这两个不同的存储构成的系统，不支持分布式事务，无法拿到精确一致的视图
3. 把计数值放在MySQL中，解决一致性视图问题

InnoDB引擎支持事务，利用好事务的原子性和隔离性，简化在业务开发时的逻辑。


## 15 | 答疑文章（一）：日志和索引相关问题 ##

### 日志相关问题 ###

#### 追问 1：MySQL 怎么知道 binlog 是完整的? ####

一个事务的binlog是有完整格式的：

* statement格式的binlog，最后会有COMMIT；
* row歌是的binlog，最后会有一个XID event。

在MySQL5.6.2版本以后，还引入了binlog-checksum参数，用来验证binlog内容的正确性。

#### 追问 2：redo log 和 binlog 是怎么关联起来的? ####

有一个共同的数据字段，叫XID。崩溃恢复的时候，会按顺序扫描 redo log：

* 如果碰到既有 prepare、又有 commit 的 redo log，就直接提交；
* 如果碰到只有 parepare、而没有 commit 的 redo log，就拿着 XID 去 binlog 找对应的事务。

追问 3：处于 prepare 阶段的 redo log 加上完整 binlog，重启就能恢复，MySQL 为什么要这么设计?

追问 4：如果这样的话，为什么还要两阶段提交呢？干脆先 redo log 写完，再写 binlog。崩溃恢复的时候，必须得两个日志都完整才可以。是不是一样的逻辑？

**追问 5：不引入两个日志，也就没有两阶段提交的必要了。只用 binlog 来支持崩溃恢复，又能支持归档，不就可以了？**

只保存binlog，然后把提交流程改成：...->“数据更新到内存”->“写binlog”->“提交事务”

**追问 6：那能不能反过来，只用 redo log，不要 binlog？**

**追问 7：redo log 一般设置多大？**

### 小结 ###

### 精选留言 ###

#### Q ####

	mysql> CREATE TABLE `t16` (
	`id` int(11) NOT NULL primary key auto_increment,
	`a` int(11) DEFAULT NULL
	) ENGINE=InnoDB;
	insert into t16 values(1,2);
	
	mysql> update t16 set a=2 where id=1;

#### A ####




## 16 | "order by"是怎么工作的？ ##

表定义

	CREATE TABLE `t16` (
	  `id` int(11) NOT NULL,
	  `city` varchar(16) NOT NULL,
	  `name` varchar(16) NOT NULL,
	  `age` int(11) NOT NULL,
	  `addr` varchar(128) DEFAULT NULL,
	  PRIMARY KEY (`id`),
	  KEY `city` (`city`)
	) ENGINE=InnoDB;

	select city,name,age from t16 where city='杭州' order by name limit 1000 ;

### 全字段排序 ###

为避免全表扫描，需要再city字段加上索引。在city字段上创建索引之后，用explain命令来看看这个语句的执行情况。

1. 初始化sort_buffer，确定放入name、city、age这三个字段；
2. 从索引city找到第一个满足city='杭州'条件的主键id，也就是图中的ID_X；
3. 到主键id索引取出整行，取name、city、age这三个字段的值，存入sort_buffer中；
4. 从索引city取下一个记录的主键id；
5. 重复步骤 3、4 直到 city 的值不满足查询条件为止，对应的主键 id 也就是图中的 ID_Y；
6. 对 sort_buffer 中的数据按照字段 name 做快速排序；
7. 按照排序结果取前 1000 行返回给客户端。

sort_buffer_size，就是MySQL为排序开辟的内存（sort_buffer）的大小。如果要排序的数据量小于sort_buffer_size，排序就在内存中完成。但如果排序数据量太大，内存放不下，则不得不利用磁盘临时文件辅助排序。

MySQL将需要排序的数据分成12份，每一份单独排序后存在这些临时文件种。然后把12个有序文件再合并成一个有序的大文件。

OPTIMIZER_TRACE 

* filesort_summary
	* rows:
	* examined_rows：表示参与排序的的行数
	* number_of_tmp_files
	* sort_buffer_size
		* sort_buffer_size超过了需要排序的数据量的大小，number_of_tmp_files就是0，表示排序可以直接在内存中完成
		* 否则需要放在临时文件加中排序。sort_buffer_size 越小，需要分成的份数越多，number_of_tmp_files 的值就越大
	* sort_mode 

### rowid排序 ###

如果MySQL认为排序的单行长度太长会怎么做呢？

	SET max_length_for_sort_data = 16;

max_length_for_sort_data，是MySQL中专门控制用于排序的行数据的长度的一个参数。

1. 初始化sort_buffer，确定放入name和id；
2. 从索引city找到第一个满足city='杭州'条件的主键id，也就是图中的ID_X；
3. 到主键id索引取出整行，取name、id这两个字段的值，存入sort_buffer中；
4. 从索引city取下一个记录的主键id；
5. 重复步骤 3、4 直到 city='杭州' 条件为止，也就是 ID_Y；
6. 对 sort_buffer 中的数据按照字段 name 做快速排序；
7. 按照排序结果取前 1000 行返回给客户端。

### 全字段排序 VS rowid 排序 ###

1. 如果MySQL担心排序内存太小，会影响排序效率，才会采用rowid排序算法。这样排序过程一次可以排序更多行，但是需要再回到原表里取数据。
2. 如果MySQL认为内存足够大，会优先选择全字段排序，把需要的字段都放到sort_buffer中，这样排序后会直接从内存里面返回查询结果了，不用再回到原表中去取数据。

如果内存够，就要多利用内存，尽量减少磁盘访问。对于InnoDB表来说，rowid排序会要求回表多造成磁盘读，因此不会被优先选择。

覆盖索引是指，索引上的信息足够满足查询请求，不需要再回到主键索引上去取数据。

每个查询都用上覆盖索引，就要把语句中涉及的字段都建上联合索引，毕竟索引还是有维护代价。需要权衡决定。

## 17 | 如何正确地显示随机消息 ##

	mysql> CREATE TABLE `t17` (
	  `id` int(11) NOT NULL AUTO_INCREMENT,
	  `word` varchar(64) DEFAULT NULL,
	  PRIMARY KEY (`id`)
	) ENGINE=InnoDB;
	
	delimiter ;;
	create procedure idata_t17()
	begin
	  declare i int;
	  set i=0;
	  while i<10000 do
	    insert into t17(word) values(concat(char(97+(i div 1000)), char(97+(i % 1000 div 100)), char(97+(i % 100 div 10)), char(97+(i % 10))));
	    set i=i+1;
	  end while;
	end;;
	delimiter ;

	call idata_t17();

### 内存临时表 ###

	mysql> explain select word from words order by rand() limit 3;

Extra 字段显示 Using temporary，表示的是需要使用临时表；Using filesort，表示的是需要执行排序操作。(需要临时表，并且需要在临时表上排序)

对于 InnoDB 表来说，执行全字段排序会减少磁盘访问，因此会被优先选择；对于内存表，回表过程只是简单地根据数据行的位置，直接访问内存得到数据，根本不会导致多访问磁盘。

1. 创建一个临时表。这个临时表使用的是memory引擎，表里有两个字段，第一个字段是 double 类型，第二个字段是 varchar(64) 类型。并且，这个表没有建索引。
2. 从words，按主键顺序取出所有的 word 值。对于每一个 word 值，调用 rand() 函数生成一个大于 0 小于 1 的随机小数，并把这个随机小数和 word 分别存入临时表的 R 和 W 字段中，到此，扫描行数是 10000。
3. 现在临时表有 10000 行数据了，接下来你要在这个没有索引的内存临时表上，按照字段 R 排序。
4. 初始化 sort_buffer。sort_buffer 中有两个字段，一个是 double 类型，另一个是整型。
5. 从内存临时表中一行一行地取出 R 值和位置信息，分别存入 sort_buffer 中的两个字段里。这个过程要对内存临时表做全表扫描，此时扫描行数增加 10000，变成了 20000。
6. 在 sort_buffer 中根据 R 的值进行排序。注意，这个过程没有涉及到表操作，所以不会增加扫描行数。
7. 排序完成后，取出前三个结果的位置信息，依次到内存临时表中取出 word 值，返回给客户端。这个过程中，访问了表的三行数据，总扫描行数变成了 20003。

	# Query_time: 0.900376  Lock_time: 0.000347 Rows_sent: 3 Rows_examined: 20003
	SET timestamp=1541402277;
	select word from words order by rand() limit 3;


### 磁盘临时表 ###

tmp_table_size这个配置限制了内存临时表的大小，默认值是16M。

### 随机排序方法 ###



### 小结 ###

如果你直接使用 order by rand()，这个语句需要 Using temporary 和 Using filesort，查询的执行代价往往是比较大的。所以，在设计的时候你要尽量避开这种写法。

在实际应用的过程中，比较规范的用法就是：尽量将业务逻辑写在业务代码中，让数据库只做“读写数据”的事情。



## 18 | 为什么这些SQL语句逻辑相同，性能却差异巨大？ ##

### 案例一：条件字段函数操作 ###

	mysql> CREATE TABLE `tradelog` (
	  `id` int(11) NOT NULL,
	  `tradeid` varchar(32) DEFAULT NULL,
	  `operator` int(11) DEFAULT NULL,
	  `t_modified` datetime DEFAULT NULL,
	  PRIMARY KEY (`id`),
	  KEY `tradeid` (`tradeid`),
	  KEY `t_modified` (`t_modified`)
	) ENGINE=InnoDB DEFAULT CHARSET=utf8mb4;

	mysql> select count(*) from tradelog where month(t_modified)=7;

其中对索引字段做函数操作，可能会破坏索引值的有序性，因此优化器就决定放弃走树搜索功能。

但并没有放弃使用这个索引，优化器选择遍历主键索引，也可以选择索引t_modified，优化器对比索引大小后发现，索引 t_modified 更小，遍历这个索引比遍历主键索引来得更快。

使用explain命令查看，key="t_modified"表示的是，使用了 t_modified 这个索引；我在测试表数据中插入了 10 万行数据，rows=100335，说明这条语句扫描了整个索引的所有值；Extra 字段的 Using index，表示的是使用了覆盖索引。

	mysql> select count(*) from tradelog where
	    -> (t_modified >= '2016-7-1' and t_modified<'2016-8-1') or
	    -> (t_modified >= '2017-7-1' and t_modified<'2017-8-1') or 
	    -> (t_modified >= '2018-7-1' and t_modified<'2018-8-1');

把 SQL 语句改成基于字段本身的范围查询。按照下面这个写法，优化器就能按照我们预期的，用上 t_modified 索引的快速定位能力了。

### 案例二：隐式类型转换 ###

	mysql> select * from tradelog where tradeid=110717;

1. 类型转换的规则是什么？
2. 为什么有数据类型转换，就需要走全索引扫描？

在MySQL中，select "10" > 9返回的是1，字符串和数字做比较的话，是将字符串转换成数字。

	mysql> select * from tradelog where tradeid=110717;

对于优化器来说，这个语句相当于：

	mysql> select * from tradelog where  CAST(tradid AS signed int) = 110717;

对索引字段做函数操作，优化器会放弃走树搜索功能。

id 的类型是 int，如果执行下面这个语句，是否会导致全表扫描呢？

	select * from tradelog where id="83126";

### 案例三：隐式字符编码转换 ###

假设系统里还有另外一个表 trade_detail，用于记录交易的操作细节。

	mysql> CREATE TABLE `trade_detail` (
	  `id` int(11) NOT NULL,
	  `tradeid` varchar(32) DEFAULT NULL,
	  `trade_step` int(11) DEFAULT NULL, /*操作步骤*/
	  `step_info` varchar(32) DEFAULT NULL, /*步骤信息*/
	  PRIMARY KEY (`id`),
	  KEY `tradeid` (`tradeid`)
	) ENGINE=InnoDB DEFAULT CHARSET=utf8;
	
	insert into tradelog values(1, 'aaaaaaaa', 1000, now());
	insert into tradelog values(2, 'aaaaaaab', 1000, now());
	insert into tradelog values(3, 'aaaaaaac', 1000, now());
	
	insert into trade_detail values(1, 'aaaaaaaa', 1, 'add');
	insert into trade_detail values(2, 'aaaaaaaa', 2, 'update');
	insert into trade_detail values(3, 'aaaaaaaa', 3, 'commit');
	insert into trade_detail values(4, 'aaaaaaab', 1, 'add');
	insert into trade_detail values(5, 'aaaaaaab', 2, 'update');
	insert into trade_detail values(6, 'aaaaaaab', 3, 'update again');
	insert into trade_detail values(7, 'aaaaaaab', 4, 'commit');
	insert into trade_detail values(8, 'aaaaaaac', 1, 'add');
	insert into trade_detail values(9, 'aaaaaaac', 2, 'update');
	insert into trade_detail values(10, 'aaaaaaac', 3, 'update again');
	insert into trade_detail values(11, 'aaaaaaac', 4, 'commit');

要查询 id=2 的交易的所有操作步骤信息

	mysql> select d.* from tradelog l, trade_detail d where d.tradeid=l.tradeid and l.id=2; /*语句Q1*/

1. 第一行显示优化器会先在交易记录表 tradelog 上查到 id=2 的行，这个步骤用上了主键索引，rows=1 表示只扫描一行；
2. 第二行 key=NULL，表示没有用上交易详情表 trade_detail 上的 tradeid 索引，进行了全表扫描。

在执行计划里，是从 tradelog 表中取 tradeid 字段，再去 trade_detail 表里查询匹配字段。我们把 tradelog 称为驱动表，把 trade_detail 称为被驱动表，把 tradeid 称为关联字段。

1. 是根据 id 在 tradelog 表里找到 L2 这一行；
2. 是从 L2 中取出 tradeid 字段的值；
3. 是根据 tradeid 值到 trade_detail 表中查找条件匹配的行。explain 的结果里面第二行的 key=NULL 表示的就是，这个过程是通过遍历主键索引的方式，一个一个地判断 tradeid 的值是否匹配。

因为表 trade_detail 里 tradeid 字段上是有索引的，本来是希望通过使用 tradeid 索引能够快速定位到等值的行。

因为这两个表的字符集不同，一个是 utf8，一个是 utf8mb4，所以做表连接查询的时候用不上关联字段的索引。

连接过程中要求在被驱动表的索引字段上加函数操作

	mysql>select l.operator from tradelog l , trade_detail d where d.tradeid=l.tradeid and d.id=4;

* 比较常见的优化方法是，把 trade_detail 表上的 tradeid 字段的字符集也改成 utf8mb4 

    alter table trade_detail modify tradeid varchar(32) CHARACTER SET utf8mb4 default null;

* 能够修改字段的字符集的话，是最好不过了。但如果数据量比较大， 或者业务上暂时不能做这个 DDL 的话，那就只能采用修改 SQL 语句的方法了

	mysql> select d.* from tradelog l , trade_detail d where d.tradeid=CONVERT(l.tradeid USING utf8) and l.id=2; 

### 小结 ###

对索引字段做函数操作，可能会破坏索引值的有序性，因此优化器就决定放弃走树搜索功能。

## 19 | 为什么我只查一行的语句，也执行这么慢？ ##

	mysql> CREATE TABLE `t19` (
	  `id` int(11) NOT NULL,
	  `c` int(11) DEFAULT NULL,
	  PRIMARY KEY (`id`)
	) ENGINE=InnoDB;
	
	delimiter ;;
	create procedure t19idata()
	begin
	  declare i int;
	  set i=1;
	  while(i<=100000) do
	    insert into t values(i,i);
	    set i=i+1;
	  end while;
	end;;
	delimiter ;
	
	call t19idata();

### 第一类：查询长时间不返回 ###

	mysql> select * from t19 where id=1;

碰到这种情况

1. 表t被锁住了
2. 首先执行一下show processlist命令查看原因
3. 针对每种状态，分析他们产生的原因、如何复现，以及如何处理

### 等MDL锁 ###

使用`show processlist`命令查看`Waiting for table metadata lock`

如果出现`Sleep`*这个状态表示的是，现在有一个线程正在表 t 上请求或者持有 MDL 写锁，把 select 语句堵住了。*

|session A|session B|
|--|--|
|lock table t19 write;||
||select * from t where id=1;|

session A通过lock table命令持有表t的MDL写锁，而session B的查询需要获取MDL读锁。所以，session B进入等待状态。通过找到谁持有MDL写锁，然后kill掉解决问题。

**解决方法**

* 通过show processlist的结果里面，session A的Command列是“Sleep”，导致查询不方便
* 通过查询`sys.schema_table_lock_waits`这张表，直接找出造成阻塞的process id，kill掉断开连接即可

	select blocking_pid from sys.schema_table_lock_waits;

可以直接在查询界面进行删除内容

### 等flush ###

	mysql> select * from information_schema.processlist where id=1;
查出来这个线程的状态是 Waiting for table flush

	-- 只关闭表t
	flush tables t with read lock;
	-- 关闭MySQL里所有打开的表
	flush tables with read lock;
正常这两个语句执行起来比较快，但可能也被其他的线程堵住了；出现`Waiting for table flush`状态可能是：有一个flush tables命令被别的语句堵住了，然后它又堵住了select语句。

### 等行锁 ###

	mysql> select * from t where id=1 lock in share mode;
由于访问id=1这个记录时要加读锁，如果这时候已经有一个事务在这行记录上持有一个写锁。select语句就会被堵住。 

|session A|session B|
|--|--|
|begin;||
|update t19 set c=c+1 where id=1;||
||select * from t where id=1;|

查询谁占着写锁，通过`sys.innodb_lock_waits`表查到
	mysql> select * from t sys.innodb_lock_waits where locked_table=`'test'.'t'`\G

    KILL4和KILL QUERY 4

### 查询慢 ###

	mysql> select * from t where c=50000 limit 1;

由于字段c上没有索引，只能走id主键顺序查询，因此需要扫描5万行。 

*坏查询不一定是慢查询*

### 小结 ###

select * from t where id=1 lock in share mode。由于 id 上有索引，所以可以直接定位到 id=1 这一行，因此读锁也是只加在了这一行上。

	begin;
	select * from t where c=5 for update;
	commit;

### 精选留言 ###

## 20 | 幻读是什么，幻读有什么问题？ ##

	CREATE TABLE `t20` (
	  `id` int(11) NOT NULL,
	  `c` int(11) DEFAULT NULL,
	  `d` int(11) DEFAULT NULL,
	  PRIMARY KEY (`id`),
	  KEY `c` (`c`)
	) ENGINE=InnoDB;
	
	insert into t20 values(0,0,0),(5,5,5),
	(10,10,10),(15,15,15),(20,20,20),(25,25,25);
这个表除了主键 id 外，还有一个索引 c，初始化语句在表中插入了 6 行数据。

下面的语句序列，是怎么加锁的，加的锁又是什么时候释放的呢？
	begin;
	select * from t20 where d=5 for update;
	commit;
在命中d=5的这一行，对应的主键id=5，因此在select语句执行完成后，id=5这一行会加一个写锁，而且由于两阶段锁协议，这个写锁再执行commit语句的时候释放。

### 幻读是什么？ ###

||session A|session B|session C|
|--|--|--|--|
|T1|begin;|||
||select * from t20 where d=5 for update; /*Q1*/|||
||result:(5,5,5)|||
|T2||update t20 set d=5 ||
|||where id=0;||
|T3|select * from t20 where d=5 fro update; /*Q2*/|||
||result:(0,0,5),(5,5,5)|||
|T4|||insert into t20 values(1, 1, 5)|
|T5|select * from t20 where d=5 for update; /*Q3*/|||
||result:(0,0,5),(1,1,5),(5,5,5)|||
|T6|commit;|||

session A里执行了三次查询，分别是Q1、Q2和Q3。

1. Q1只返回id=这一行；
2. 在T2时刻，session B把id=0这一行的d值改成了5，因此T3时刻Q2查出来的是id=0和id=5这两行；
3. 在T4时刻，session C又插入一行（1，1，5），因此T5时刻Q3查出来的事id=0，id=1和id=5这三行。

幻读指的是一个事务在前后两次查询同一个范围的时候，后一次查询看到了前一次查询没有看到的行。

幻读：

1. 在可重复读隔离级别下，普通的查询是快照读，是不会看到别的事务插入的数据的。因此，幻读在“当前读”下才会出现。
2. 上面 session B 的修改结果，被 session A 之后的 select 语句用“当前读”看到，不能称为幻读。幻读仅专指“新插入的行”。

查询加了for update，都是当前读。而当前读的规则，就是要能读到所有已经提交的纪录的最新值。并且，session B 和 sessionC 的两条语句，执行后就会提交，所以 Q2 和 Q3 就是应该看到这两个事务的操作效果，而且也看到了，这跟事务的可见性规则并不矛盾。

### 幻读有什么问题？ ###

1. 语义上
2. 数据一致性的问题：锁的设计是为了保证数据的一致性。而这个一致性，不止是数据库内部数据状态在此刻的一致性，还包含了数据和日志在逻辑上的一致性。

给session A中的所有行都加了写锁 =>
即使把所有的记录都加上锁，还是阻止不了新插入的记录。

### 如何解决幻读 ###

||读锁|写锁|
|--|--|--|
|读锁|兼容|冲突|
|写锁|冲突|冲突|

产生幻读的原因是，行锁只能锁住行，但是新插入记录这个动作，要更新的是记录之间的“间隙”。

间隙锁，锁的就是两个值之间的空隙。比如文章开头的表 t20，初始化插入了 6 个记录，这就产生了 7 个间隙。

跟间隙锁存在冲突关系的，是“往这个间隙中插入一个记录”这个操作。**间隙锁之间都不存在冲突关系。**

间隙锁和行锁合称next-key lock，每个next-ket lock是前开后闭区间。我们的表t20初始化以后，如果用select * from t20 for update要把整个表所有记录锁起来，就形成了7个next-key lock，分别是(负无穷,0]、(0,5]、(5,10]、(10,15]、(15,20]、(20,25]、(25,+supernum]。

间隙锁和next-key lock的引入，解决幻读的问题，但同时也带来了一些“困扰”。

间隙锁是在可重复读隔离级别下才会生效的。所以，你如果把隔离级别设置为读提交的话，就没有间隙锁了。但同时，你要解决可能出现的数据和日志不一致问题，需要把 binlog 格式设置为 row。这，也是现在不少公司使用的配置组合。

### 小结 ###

间隙锁：给所有的行都加上行锁，仍然无法解决幻读。就是生产库上会经常出现由于间隙锁导致的死锁现象。行锁确实比较直观，判断规则也相对简单，间隙锁的引入会影响系统的并发度，也增加了锁分析的复杂度，但也有章可循。

### 精选留言 ###

#### Q ####

session B和session C的insert语句都会进入锁等待状态。

#### A ####

间隙锁

TODO

## 21 | 为什么我只改一行的语句锁这么多 ##

1. MySQL 5.x 系列 <=5.7.24，8.0 系列 <=8.0.13
2. 间隙锁在可重复读隔离级别下才有效

我总结的加锁规则里面，包含了两个“原则”、两个“优化”和一个“bug”。

1. 原则1：加锁的基本单位是next-key lock。
2. 原则2：查找过程中访问到的对象才会加锁。
3. 优化1：索引上的等值查询，给唯一索引加锁的时候，next-key lock退化为行锁。
4. 优化2：索引上的等值查询，向右遍历时且最后一个值不满足等值条件

	CREATE TABLE `t21` (
	  `id` int(11) NOT NULL,
	  `c` int(11) DEFAULT NULL,
	  `d` int(11) DEFAULT NULL,
	  PRIMARY KEY (`id`),
	  KEY `c` (`c`)
	) ENGINE=InnoDB;
	
	insert into t21 values(0,0,0),(5,5,5),
	(10,10,10),(15,15,15),(20,20,20),(25,25,25);

### 案例一：等值查询间隙锁 ###

|session A|session B|session C|
|--|--|--|
|begin;|||
|update t21 set d=d+1 where id=7|||
||insert into t21 values(8,8,8);(block)||
|||update t21 set d=d+1 where id=10;(Query OK)|

1. 根据原则1，加锁单位是next-key lock，session A加锁范围就是(5,10]
2. 同时根据优化2，这是一个等值查询（id=7），而id=10不满足查询条件，next-key lock退化成间隙锁，因此最终加锁的范围是（5,10)

### 案例二：非唯一索引等值锁 ###

* lock in share mode只锁覆盖索引
* for update给主键索引上满足条件的行加上行锁

### 案例三：主键索引范围锁 ###

### 案例四：非唯一索引范围锁 ###

### 案例五：唯一索引范围锁 bug ###

### 案例六：非唯一索引上存在"等值"的例子 ###

### 案例七：limit语句加锁 ###

|session A|session B|
|--|--|
|begin;||
|delete from t where c=10 limit 2;||
||insert into t values(12,12,12);|
||(Query OK)|

在删除数据的时候尽量加 limit。不仅可以控制删除数据的条数，还可以减少加锁的范围。

### 案例八：一个死锁的例子 ###

|session A|session B|
|--|--|
|begin;||
|select id from t where c=10 lock in share mode;||
||update t set d=d+1 where c=10;(blocked)|
|insert into values(8,8,8);||
||ERROR 1213|

1. sessin A启动事务后查询语句加lock in share mode，在索引c
2. 上加了next-key lock(5,10]和间隙锁(10,15)

### 小结 ###

可重复读隔离级别 (repeatable-read) 下验证的。可重复读隔离级别遵守两阶段锁协议，所有加锁的资源，都是在事务提交或者回滚的时候才释放的。

在最后的案例中，你可以清楚地知道 next-key lock 实际上是由间隙锁加行锁实现的。如果切换到读提交隔离级别 (read-committed) 的话，过程中去掉间隙锁的部分，也就是只剩下行锁的部分。

在读提交隔离级别下还有一个优化，即：语句执行过程中加上的行锁，在语句执行完成后，就要把“不满足条件的行”上的行锁直接释放，不需要等到事务提交。

读提交隔离级别下，锁的范围更小，锁的时间更短。

## 22 | MySQL有哪些“饮鸩止渴”提高性能的方法？ ##

### 短连接风暴 ###

MySQL 建立连接的过程，成本是很高的。除了正常的网络连接三次握手外，还需要做登录权限判断和获得这个连接的数据读写权限。

#### 第一种方法：先处理掉那些占着连接但是不工作的线程。 ####

设置 wait_timeout 参数表示的是，一个线程空闲 wait_timeout 这么多秒之后，就会被 MySQL 直接断开连接。

#### 第二种方法：减少连接过程的消耗。 ####

跳过权限验证的方法：重启数据库，并使用–skip-grant-tables 参数启动。整个 MySQL 会跳过所有的权限验证阶段，包括连接过程和语句执行过程在内。

### 慢查询性能问题 ###

1. 索引没有设计好
2. SQL语句没写好
3. MySQL选错了索引

#### 导致慢查询的第一种可能是，索引没有设计好。 ####

假设你现在的服务是一主一备，主库 A、备库 B，这个方案的大致流程是这样的：

1. 在备库 B 上执行 set sql_log_bin=off，也就是不写 binlog，然后执行 alter table 语句加上索引；
2. 执行主备切换；
3. 这时候主库是 B，备库是 A。在 A 上执行 set sql_log_bin=off，然后执行 alter table 语句加上索引。

#### 导致慢查询的第二种可能是，语句没写好。 ####

query_rewrite

#### MySQL 选错了索引。 ####

应急方案就是给这个语句加上force index。或者使用查询重写功能，给原来的语句加上force index。

#### 总结 ####

慢查询导致性能问题的三种可能情况，实际上出现最多的是前两种，即：索引没设计好和语句没写好。

我们通过以下操作事先发现问题：

1. 上线前，在测试环境，把慢查询日志（slow log）打开，并且把 long_query_time 设置成 0，确保每个语句都会被记录入慢查询日志；
2. 在测试表里插入模拟线上的数据，做一遍回归测试；
3. 观察慢查询日志里每类语句的输出，特别留意 Rows_examined 字段是否与预期一致。

全量回归测试：需要工具帮你检查所有的 SQL 语句的返回结果。使用[pt-=query-digest](https://www.percona.com/doc/percona-toolkit/3.0/pt-query-digest.html)


### QPS 突增问题 ###

TO 下掉这个功能

1. 一种是由全新业务的 bug 导致的。假设你的 DB 运维是比较规范的，也就是说白名单是一个个加的。这种情况下，如果你能够确定业务方会下掉这个功能，只是时间上没那么快，那么就可以从数据库端直接把白名单去掉。
2. 如果这个新功能使用的是单独的数据库用户，可以用管理员账号把这个用户删掉，然后断开现有连接。这样，这个新功能的连接不成功，由它引发的 QPS 就会变成 0。
3. 如果这个新增的功能跟主体功能是部署在一起的，那么我们只能通过处理语句来限制。这时，我们可以使用上面提到的查询重写功能，把压力最大的 SQL 语句直接重写成"select 1"返回。(风险很高，优先级最低)
	* 误伤到其他功能
	* 导致后序的业务逻辑连锁失败

虚拟化、白名单机制、业务账号分离

### 小结 ###

拒绝连接和断开连接，通过重写语句来绕过一些坑的方法；既有临时的高危方案，也有未雨绸缪的、相对安全的预案。

在实际开发中，我们也要尽量避免一些低效的方法，比如避免大量地使用短连接。同时，如果你做业务开发的话，要知道，连接异常断开是常有的事，你的代码里要有正确地重连并重试的机制。

DBA 虽然可以通过语句重写来暂时处理问题，但是这本身是一个风险高的操作，做好 SQL 审计可以减少需要这类操作的机会。

## 23 | MySQL是怎么保证数据不丢的？ ##

只要redo log和binlog保证持久化到磁盘，就能确保MySQL异常重启，数据可以回复。

### binlog的写入机制 ###

事务执行过程中，先把日志写到binglog cache，事务提交的时候，再把binlog cache写道binlog文件中。

一个事务的binlog不能被拆开的，因此无论这个事务多大，也要确保一次性写入。——binlog cache的保存问题

系统给 binlog cache 分配了一片内存，每个线程一个，参数 binlog_cache_size 用于控制单个线程内 binlog cache 所占内存的大小。如果超过了这个参数规定的大小，就要暂存到磁盘。

事务提交的时候，执行器把 binlog cache 里的完整事务写入到 binlog 中，并清空 binlog cache。

![binlog写盘状态](img/9ed86644d5f39efb0efec595abb92e3e.png)

每个线程有自己的binlog cache，但是共用同一份binlog文件。

* 图中的write，指的就是指把日志写入到文件系统的page cache，并没有把数据持久化到磁盘，所以速度比较快。
* 图中的fsync，才是将数据持久化到磁盘的操作。一般情况下，人为fsync才占磁盘的IOPS（IOPS（Input/Output Operations Per Second）是一个用于计算机存储设备（如硬盘（HDD）、固态硬盘（SSD）或存储区域网络（SAN））性能测试的量测方式，可以视为是每秒的读写次数。）。

由参数sync_binlog控制的，write和fsync的时机：

1. sync_binlog=0的时候，表示每次提交事务都只write，不fsync
2. sync_binlog=1的时候，表示每次提交事务都会执行fsync；
3. sync_binlog=N(N>1)的时候，表示每次提交事务都write，但累积N个事务后才fsync。

出现IO瓶颈的场景里，将sync_binlog设置成一个比较大的值，可以提升性能。

* 在实际的业务场景中，考虑到丢失日志量的可控性，一般不建议将这个参数设成 0，比较常见的是将其设置为 100~1000 中的某个数值。
* 将 sync_binlog 设置为 N，对应的风险是：如果主机发生异常重启，会丢失最近 N 个事务的 binlog 日志。

### redo log的写入机制 ###

redo log buffer：事务在执行过程中，生成的redo log是要先写到redo log buffer的。

redo log buffer里面的内容不会每次生成后直接持久化到磁盘。

![9d057f61d3962407f413deebc80526d4.png](img/9d057f61d3962407f413deebc80526d4.png)

redo log存在的三种状态：

* 存在redo log buffer中，物理上是在MySQL进程内存中，图中红色部分（快）
* 写到磁盘（write），但是没有持久化（sync），物理上是在文件系统的page cache里面，图中黄色部分（快）
* 持久化到磁盘，对应的是hard disk，图中绿色部分（慢）

为了控制redo log的写入策略，InnoDB提供了innodb_flush_log_at_trx_commit参数，有三种可能取值：

1. 设置为0的时候，表示每次事务提交时都只是把redo log留在redo log buffer中；
2. 设置为1的时候，表示每次事务提交都将redo log直接持久化到磁盘；
3. 设置为2的时候，表示每次事务提交时都只是把redo log写到page cache。

InnoDB有一个后台线程，每隔1秒，就会把redo log buffer中的日志，调用write写到文件系统的page cache，然后调用fsync持久化到磁盘。

后台线程每秒一次的轮询外，还有两种场景会让一个没有提交的事务的redo log写入到磁盘中。

1. redo log buffer占用的空间即将达到innodb_log_buffer_size一半的时候，后台线程会主动写盘。（写盘动作只是 write，而没有调用 fsync，也就是只留在了文件系统的 page cache）
2. 并行的事务提交的时候，顺带将这个事务的 redo log buffer 持久化到磁盘。

WAL机制主要得益于l两个方面：

1. redo log和binlog都是顺序写，磁盘的顺序写比随机写速度要快
2. 组提交机制，可以大幅度降低磁盘的IOPS消耗


### 小结 ###

* 问题1执行一个 update 语句以后，我再去执行 hexdump 命令直接查看 ibd 文件内容，为什么没有看到数据有改变呢？
	* 这可能是因为 WAL 机制的原因。update 语句执行完成后，InnoDB 只保证写完了 redo log、内存，可能还没来得及将数据写到磁盘。 
* 问题2：为什么binlog cache是每个线程自己维护的，而redo log buffer是全局共用的？
	* MySQL中binlog是不能“被打断的”。一个事务的binlog必须连续写，因此要整个事务完成后，再一起写到文件里。
	* redo log没有这个要求，中间有生成的日志可以写到redo logbuffer中。redo log buffer中的内容还能“搭便车”，其他事务提交的时候可以被一起写到磁盘中
* 问题3：事务执行期间，还没到提交阶段，如果发生 crash 的话，redo log 肯定丢了，这会不会导致主备不一致呢？
	* 不会。因为这时候 binlog 也还在 binlog cache 里，没发给备库。crash 以后 redo log 和 binlog 都没有了，从业务角度看这个事务也没有提交，所以数据是一致的。 
* 问题4：如果 binlog 写完盘以后发生 crash，这时候还没给客户端答复就重启了。等客户端再重连进来，发现事务已经提交成功了，这是不是 bug？
	* 不是。整个事务都提交成功了，redo log commit 完成了，备库也收到 binlog 并执行了。但是主库和客户端网络断开了，导致事务成功的包返回不回去，这时候客户端也会收到“网络断开”的异常。

实际上数据库的crash-safe保证的是：

1. 如果客户端收到事务成功的消息，事务就一定持久化了；
2. 如果客户端收到事务失败（比如主键冲突、回滚等）的消息，事务就一定失败了；
3. 如果客户端收到“执行异常”的消息，应用需要重连后通过查询当前状态来继续后续的逻辑。此时数据库只需要保证内部（数据和日志之间，主库和备库之间）一致就可以了。

### 精选留言 ###

当设置sync_binlog=0时，每次commit都只时write到page cache，并不会fsync。但是做实验时binlog文件中还是会有记录。

## 24 | MySQL是怎么保证主备一致的？ ##

binlog可以用来归档，也可以用来主备同步。高可用架构，都直接依赖于 binlog。

### MySQL主备的基本原理 ###

在状态 1 中，客户端的读写都直接访问节点 A，而节点 B 是 A 的备库，只是将 A 的更新都同步过来，到本地执行。这样可以保持节点 B 和 A 的数据是相同的。

当需要切换的时候，就切成状态 2。这时候客户端读写访问的都是节点 B，而节点 A 是 B 的备库。

在状态 1 中，虽然节点 B 没有被直接访问，但是我依然建议你把节点 B（也就是备库）设置成只读（readonly）模式。这样做，有以下几个考虑：

1. 有时候一些运营类的查询语句会被放到备库上去查，设置为只读可以防止误操作；
2. 防止切换逻辑有 bug，比如切换过程中出现双写，造成主备不一致；
3. 可以用 readonly 状态，来判断节点的角色。

因为 readonly 设置对超级 (super) 权限用户是无效的，而用于同步更新的线程，就拥有超级权限。

1. 在备库 B 上通过 change master 命令，设置主库 A 的 IP、端口、用户名、密码，以及要从哪个位置开始请求 binlog，这个位置包含文件名和日志偏移量。
2. 在备库 B 上执行 start slave 命令，这时候备库会启动两个线程，就是图中的 io_thread 和 sql_thread。其中 io_thread 负责与主库建立连接。
3. 主库 A 校验完用户名、密码后，开始按照备库 B 传过来的位置，从本地读取 binlog，发给 B。
4. 备库 B 拿到 binlog 后，写到本地文件，称为中转日志（relay log）。
5. sql_thread 读取中转日志，解析出日志里的命令，并执行。

### binlog的三种格式对比 ###

1. statement
2. row
3. mixed，其实就是前两种格式的混合。

	mysql> CREATE TABLE `t24` (
	  `id` int(11) NOT NULL,
	  `a` int(11) DEFAULT NULL,
	  `t_modified` timestamp NOT NULL DEFAULT CURRENT_TIMESTAMP,
	  PRIMARY KEY (`id`),
	  KEY `a` (`a`),
	  KEY `t_modified`(`t_modified`)
	) ENGINE=InnoDB;

	insert into t24 values(1,1,'2018-11-13');
	insert into t24 values(2,2,'2018-11-12');
	insert into t24 values(3,3,'2018-11-11');
	insert into t24 values(4,4,'2018-11-10');
	insert into t24 values(5,5,'2018-11-09');

### 为什么会有mixed格式的binlog？ ###

为什么会有 mixed 这种 binlog 格式的存在场景？

* 因为有些 statement 格式的 binlog 可能会导致主备不一致，所以要使用 row 格式。
* 但 row 格式的缺点是，很占空间。比如你用一个 delete 语句删掉 10 万行数据，用 statement 的话就是一个 SQL 语句被记录到 binlog 中，占用几十个字节的空间。但如果用 row 格式的 binlog，就要把这 10 万条记录都写到 binlog 中。（不仅会占用更大的空间，同时写 binlog 也要耗费 IO 资源，影响执行速度。）
* MySQL 就取了个折中方案，也就是有了 mixed 格式的 binlog。mixed 格式的意思是，MySQL 自己会判断这条 SQL 语句是否可能引起主备不一致，如果有可能，就用 row 格式，否则就用 statement 格式。

### 循环复制问题 ###

binlog 的特性确保了在备库执行相同的 binlog，可以得到与主库相同的状态。

MySQL 在 binlog 中记录了这个命令第一次执行时所在实例的 server id

1. 规定两个库的 server id 必须不同，如果相同，则它们之间不能设定为主备关系；
2. 一个备库接到 binlog 并在重放的过程中，生成与原 binlog 的 server id 相同的新的 binlog；
3. 每个库在收到从自己的主库发过来的日志后，先判断 server id，如果跟自己的相同，表示这个日志是自己生成的，就直接丢弃这个日志。

按照这个逻辑，如果我们设置了双 M 结构，日志的执行流就会变成这样：

1. 从节点 A 更新的事务，binlog 里面记的都是 A 的 server id；
2. 传到节点 B 执行一次以后，节点 B 生成的 binlog 的 server id 也是 A 的 server id；
3. 再传回给节点 A，A 判断到这个 server id 与自己的相同，就不会再处理这个日志。所以，死循环在这里就断掉了。

### 小结 ###

介绍了MySQL binlog的格式和一些基本机制，时后面要介绍读写分离等系列文章的背景知识

MySQL高可用方案的基础，演化出了诸如多节点、半同步、MySQL group replication等相对复杂的方案。

### 精选留言 ###

我们说 MySQL 通过判断 server id 的方式，断掉死循环。但是，这个机制其实并不完备，在某些场景下，还是有可能出现死循环。

#### Q ####

#### A ####

## 25 | MySQL时怎么保证高可用的？ ##

在一个主备关系中，每个备库接收主库的binlog并执行。正常情况下，只要主库执行更新生成的所有 binlog，都可以传到备库并被正确地执行，备库就能达到跟主库一致的状态，这就是最终一致性。

![MySQL 主备切换流程 -- 双 M 结构(/img/89290bbcf454ff9a3dc5de42a85a69cc.png)

### 主备延迟 ###

主动操作和被动操作

主动切换的场景：

在介绍主动切换流程的详细步骤之前，先说明一个概念，即“同步延迟”。与数据有关的时间点主要包括以下三个：

1. 主库 A 执行完成一个事务，写入 binlog，我们把这个时刻记为 T1;
2. 之后传给备库 B，我们把备库 B 接收完这个 binlog 的时刻记为 T2;
3. 备库 B 执行完成这个事务，我们把这个时刻记为 T3。

所谓主备延迟，就是同一个事务，在备库执行完成

### 主备延迟的来源 ###

1. 有些部署条件下，备库所在机器的性能要比主库所在的机器性能差。
2. 常见的可能了，即备库的压力大
3. 大事务
4. 大表



### 可靠性优先策略 ###

### 可用性优先策略 ###

### 小结 ###

MySQL高可用系统的基础，主备且换逻辑。

1. 几种主备延迟的情况
2. 主备延迟的存在，且换策略就有不同的选择
3. 可靠性优先和可用性优先策略的不同

在实际的应用中，我更建议使用可靠性优先的策略。毕竟保证数据准确，应该是数据库服务的底线。在这个基础上，通过减少主备延迟，提升系统的可用性。

### 精选留言 ###

#### Q ####

#### A ####

## 26 | 备库为什么会延迟好几个小时？ ##

contdinator再分发的时候，需要满足以下两个基本要求：

1. 不能造成更新覆盖。这就要求更新同一行的两个事务，必须被分发到同一个 worker 中。
2. 同一个事务不能被拆开，必须放到同一个 worker 中。

### MySQL5.5版本的并行复制策略 ###

按表分发策略和按行分发策略

#### 按表分发策略 ####

按表分发事务的基本思路是，如果两个事务更新不同的表，可以并行。因为数据存储在表里，所以按表分发，可以保证两个worker不会更新同一行。

每个worker线程对应一个hash表，用于保存当前正在这个worker的“执行队列”里的事务所涉及的表。hash表的key是“库名.表名”，value是一个数字，表示队列中有多少个事务修改这个表。

#### 按行分发策略 ####

### MySQL 5.6版本很的并行复制策略 ###

### MariaDB的并行复制策略 ###

### MySQL 5.7的并行复制策略 ###

5.7版本，由参数slave-parallel-type 来控制并行复制策略：

1. 配置为 DATABASE，表示使用 MySQL 5.6 版本的按库并行策略；
2. 配置为 LOGICAL_CLOCK，表示的就是类似 MariaDB 的策略。

### MySQL 5.7.22的并行复制策略 ###

在MySQL 5.7.22 版本里，MySQL 增加了一个新的并行复制策略，基于 WRITESET 的并行复制。

新增了一个参数 binlog-transaction-dependency-tracking：

1. COMMIT_ORDER，根据同时进入 prepare 和 commit 来判断是否可以并行的策略。
2. WRITESET：表示的是对于事务涉及更新的每一行，计算出这一行的 hash 值，组成集合 writeset。如果两个事务没有操作相同的行，也就是说它们的 writeset 没有交集，就可以并行。
3. WRITESET_SESSION：是在 WRITESET 的基础上多了一个约束，即在主库上同一个线程先后执行的两个事务，在备库执行的时候，要保证相同的先后顺序。

唯一标识：通过“库名+表名+索引名+值”计算出来。如果一个表除了有主键索引外，还有其他唯一索引，那么对于每个唯一索引，insert语句对应的writeset就要多增加一个hash值。

优点：

1. writeset 是在主库生成后直接写入到 binlog 里面的，这样在备库执行的时候，不需要解析 binlog 内容（event 里的行数据），节省了很多计算量；
2. 不需要把整个事务的 binlog 都扫一遍才能决定分发到哪个 worker，更省内存；
3. 由于备库的分发策略不依赖于 binlog 内容，所以 binlog 是 statement 格式也是可以的。

对于“表上没主键”和“外键约束”的场景，WRITESET 策略也是没法并行的，也会暂时退化为单线程模型。

### 小结 ###

为什么要有多线程复制呢？这是因为单线程复制的能力全面低于多线程复制，对于更新压力较大的主库，备库是可能一直追不上主库的。从现象上看就是，备库上 seconds_behind_master 的值越来越大。

从这些分析中，你也会发现大事务不仅会影响到主库，也是造成备库复制延迟的主要原因之一。因此，在平时的开发工作中，我建议你尽量减少大事务操作，把大事务拆成小事务。

官方 MySQL5.7 版本新增的备库并行策略，修改了 binlog 的内容，也就是说 binlog 协议并不是向上兼容的，在主备切换、版本升级的时候需要把这个因素也考虑进去。

## 27 | 主库出问题了，从库怎么办？ ##

读多写少，读性能的问题。而在数据库层解决读性能问题，就要涉及到接下来两篇文章要讨论的架构：一主多从。

### 基于位点的主备切换 ###

### GTID ###

### 基于 GTID 的主备切换 ###

### GTID 和在线 DDL ###

### 小结 ###



## 28 | 读写分离有哪些坑？ ##

读写分离的主要目标是分摊主库的压力。

1. 客户端（client）主动做负载均衡
2. 在 MySQL 和客户端之间有一个中间代理层 proxy，客户端只连接 proxy， 由 proxy 根据请求类型和上下文决定请求的分发路由。

客户端直连和带proxy的读写分离架构

疑问：由于主从可能存在延迟，客户端执行完一个更新事务后马上发起查询，如果查询选择的是从库的话，就有可能读到刚刚的事务更新之前的状态。

**这种“在从库上会读到系统的一个过期状态”的现象，在这篇文章里，我们暂且称之为“过期读”。**

过期读的解决方法：

* 强制走主库方案；
* sleep 方案；
* 判断主备无延迟方案；
* 配合 semi-sync 方案；
* 等主库位点方案；
* 等 GTID 方案。

### 强制走主库方案 ###

将查询请求做分类，将查询请求分为两类：

* 对于必须要拿到最新结果的请求，强制将其发到主库上。
* 对于可以读到旧数据的请求，才将其发到从库上。

### Sleep 方案 ###

主库更新后，读从库之前先 sleep 一下。具体的方案就是，类似于执行一条 select sleep(1) 命令。

### 判断主备无延迟方案 ###

`show slave status` 结果里的 seconds_behind_master 参数的值，可以用来衡量主备延迟时间的长短。

1. 第一种确保主备无延迟的方法是：每次从库执行查询请求前，先判断 seconds_behind_master 是否已经等于 0。如果还不等于 0 ，那就必须等到这个参数变为 0 才能执行查询请求。
2. 对比位点确保主备无延迟：
	* Master_Log_File 和 Read_Master_Log_Pos，表示的是读到的主库的最新位点；
	* Relay_Master_Log_File 和 Exec_Master_Log_Pos，表示的是备库执行的最新位点。
	* Master_Log_File 和 Relay_Master_Log_File、Read_Master_Log_Pos 和 Exec_Master_Log_Pos 这两组值完全相同，就表示接收到的日志已经同步完成。
3. 对比 GTID 集合确保主备无延迟
	* Auto_Position=1，表示这对主备关系使用了 GTID 协议，
	* Retireved_Gtid_Set，是备库收到的所有日志的 GTID 集合；
	* Executed_Gtid_Set，是备库所有已经执行完成的 GTID 集合。

一个事务的binlog在主备库之间的状态：

1. 主库执行完成，写入binlog，并反馈给客户端
2. binlog被主库发送给备库，备库收到
3. 在备库执行binlog完成

#### 可能出现问题 ####

过期读的问题

### 配合 semi-sync ###

引入半同步复制，也就是 semi-sync replication

semi-sync：

1. 事务提交的时候，主库把 binlog 发给从库；
2. 从库收到 binlog 以后，发回给主库一个 ack，表示收到了；
3. 主库收到这个 ack 以后，才能给客户端返回“事务完成”的确认

主库掉电的时候，有些binlog还来不及发给从库，会导致系统数据丢失。

semi-sync + 位点判断的方案，只对一主一备的场景是成立的。在一主多从场景中，主库只要等到一个从库的 ack，就开始给客户端返回确认。

1. 如果查询是落在这个响应了 ack 的从库上，是能够确保读到最新数据；
2. 但如果是查询落到其他从库上，它们可能还没有收到最新的日志，就会产生过期读的问题。

#### 可能出现问题 ####

* 一主多从的时候，在某些从库执行查询请求会存在过期读的现象
* 在持续延迟的情况下，可能出现过度等待的问题

### 等主库位点方案 ###

主库定位点方案

	select master_pos_wait(file, pos[, timeout]);

1. 在从库执行的；
2. 参数file和pos指的是主库上的文件名和位置；
3. timeout可选，设置为正整数N表示这个函数最多等待N秒。

结果

* M：表示从命令开始执行，到应用完 file 和 pos 表示的 binlog 位置，执行了多少事务。
* 如果等待超过 N 秒，就返回 -1；
* 如果刚开始执行的时候，就发现已经执行过这个位置了，则返回 0。

按照不允许过期读的要求，只有两种选择

1. 超时放弃
2. 转到主库查询

### GTID方案 ###

	select wait_for_executed_gtid_set(gtid_set, 1);

1. 等待，直到这个库执行的事务中包含传入的 gtid_set，返回 0；
2. 超时返回1.

### 小结 ###

过期读在本质上是由一写多读导致的。在实际应用中，可能会有别的不需要等待就可以水平扩展的数据库方案，但这往往是用牺牲写性能换来的，也就是需要在读性能和写性能中取权衡。

## 29 | 如何判断一个数据库是不是出问题了？ ##

在一主一备的双M架构里，主备切换只需要客户端流量切到备库；而在一主多从架构里，主备且换除了要把客户端流量切换到备库外，还需要把从库接到新主库上。

主备切换有两种场景，一种是主动切换，一种是被动切换。而其中被动切换，往往是因为主库出问题了，由 HA 系统发起的。

### select 1 判断 ###

select 1 成功返回，只能说明这个库的进程还在，并不能说明主库没问题。

	set global innodb_thread_concurrency=3;
	
	CREATE TABLE `t29` (
	  `id` int(11) NOT NULL,
	  `c` int(11) DEFAULT NULL,
	  PRIMARY KEY (`id`)
	) ENGINE=InnoDB;
	
	 insert into t values(1,1)



### 查表判断 ###

### 更新判断 ###

### 内部统计 ###

### 小结 ###

* select 1： MHA（Master High Availability），默认使用的就是这个方法
* 只做连接（如果连接成功就认为主库没问题）

每个改进方法都会增加额外的损耗，并不能用“对错”做直接判断，需要根据业务实际情况来做权衡。

个人比较倾向的方案，是优先考虑 update 系统表，然后再配合增加检测 performance_schema 的信息。

## 30 | 答疑文章（二）：用动态的观点看加锁 ##

InnoDB的间隙锁、next-key lock，以及加锁规则。

加锁规则，包含了两个“原则”、两个“优化”和一个“bug”：

* 原则1:加锁的基本单位是next-key lock。next-key lock是前开后闭区间。
* 原则2:查找过程中访问到的对象才会加锁。
* 优化1:索引上的等值查询，给唯一索引加锁的时候，next-key lock退化为行锁。
* 优化2:索引上的等值查询，向右遍历时且最后一个值不满足等值条件的时候，next-key lock退化为间隙锁。
* 一个bug：唯一索引上的范围查询会访问到不满足条件的第一个值为止。

	CREATE TABLE `t30` (
	  `id` int(11) NOT NULL,
	  `c` int(11) DEFAULT NULL,
	  `d` int(11) DEFAULT NULL,
	  PRIMARY KEY (`id`),
	  KEY `c` (`c`)
	) ENGINE=InnoDB;
	
	insert into t30 values(0,0,0),(5,5,5),
	(10,10,10),(15,15,15),(20,20,20),(25,25,25);

### 不等号条件里的等值查询 ###

	begin;
	select * from t where id>9 and id<12 order by id desc for update;

利用上面的加锁规则，加锁范围是主键索引上的(0,5]、(5,10]和(10,15)。

### 等值查询的过程 ###

	begin;
	select id from t where c in(5,20,10) lock in share mode;

### 怎么看死锁？ ###

1. 由于锁是一个个加的，要避免锁，对同一组资源，要按照尽量相同的顺序访问。
2. 在发生死锁的时刻，for update这条语句占有的资源更多，回滚成本更大，所以InnoDB选择了回滚成本更小的lock in share mode语句，来回滚。

### 怎么看锁等待？ ###

### update的例子 ###

### 小结 ###

通过 explain 的结果，就能够脑补出一个 SQL 语句的执行流程。达到这样的程度，才算是对索引组织表、索引、锁的概念有了比较清晰的认识。你同样也可以用这个方法，来验证自己对这些知识点的掌握程度。

`show engine innodb status` 输出结果中的事务信息和死锁信息

所谓“间隙”，其实根本就是由“这个间隙右边的那个记录”定义的。

## 31 | 误删数据后除了跑路，还能怎么办？ ##

1. 使用 delete 语句误删数据行；
2. 使用 drop table 或者 truncate table 语句误删数据表；
3. 使用 drop database 语句误删数据库；
4. 使用 rm 命令误删整个 MySQL 实例。

### 误删行 ###

如果是使用 delete 语句误删了数据行，可以用 Flashback 工具通过闪回把数据恢复回来。（恢复数据的原理是修改binlog的内容，拿回原库重放。能够使用这个方案的前提是，需要确保binlog_format=row 和 binlog_row_image=FULL）

具体恢复数据时，对单个事务做如下处理：

1. 对于 insert 语句，对应的 binlog event 类型是 Write_rows event，把它改成 Delete_rows event 即可；
2. 同理，对于 delete 语句，也是将 Delete_rows event 改为 Write_rows event；
3. 而如果是 Update_rows 的话，binlog 里面记录了数据行修改前和修改后的值，对调这两行的位置即可。

如果是多个事务，则需要把涉及的事务倒置。

(A)delete ...	(reverse C)update ... 
(B)insert ... ->(reverse B)delete ...
(C)update ...	(reverse A)insert ...

需要说明的是，我不建议你直接在主库上执行这些操作。(恢复数据比较安全的做法，是恢复出一个备份，或者找一个从库作为临时库，在这个临时库上执行这些操作，然后再将确认过的临时库的数据，恢复回主库。)

做到事前预防，有两个建议：

1. 把 sql_safe_updates 参数设置为 on。这样一来，如果我们忘记在 delete 或者 update 语句中写 where 条件，或者 where 条件里面没有包含索引字段的话，这条语句的执行就会报错。
2. 代码上线前，必须经过 SQL 审计。

delete 全表是很慢的，需要生成回滚日志、写 redo、写 binlog。所以，从性能角度考虑，你应该优先考虑使用 truncate table 或者 drop table 命令。

使用 delete 命令删除的数据，你还可以用 Flashback 来恢复。而使用 truncate /drop table 和 drop database 命令删除的数据，就没办法通过 Flashback 来恢复了。因为即使我们配置了 binlog_format=row，执行这三个命令时，记录的 binlog 还是 statement 格式。binlog 里面就只有一个 truncate/drop 语句，这些信息是恢复不出数据的。

### 误删库/表 ###

要想恢复数据，就需要使用全量备份，加增量日志的方式了。这个方案要求线上有定期的全量备份，并且实时备份 binlog。

在这两个条件都具备的情况下，假如有人中午 12 点误删了一个库，恢复数据的流程如下：

1. 取最近一次全量备份，假设这个库是一天一备，上次备份是当天 0 点；
2. 用备份恢复出一个临时库；
3. 从日志备份里面，取出凌晨 0 点之后的日志；
4. 把这些日志，除了误删除数据的语句外，全部应用到临时库。

### 延迟复制备库 ###

## 32 | 为什么还有kill不掉的语句？ ##

在MySQL中有两个kill命令：

1. 一个是kill query + 线程id，表示终止这个线程中正在执行的语句
2. 一个是kill connection + 线程id，这里connection可缺省，表示断开这个线程的连接

### 收到 kill 以后，线程做什么？ ###

当对一个表做增删改查操作时，会在表上加 MDL 读锁。所以，session B 虽然处于 blocked 状态，但还是拿着一个 MDL 读锁的。如果线程被 kill 的时候，就直接终止，那之后这个 MDL 读锁就没机会被释放了。

kill并不是马上停止的意思，而是告诉执行线程说，已经不需要继续执行了，可以开始“执行停止的逻辑了”。

**实现上，当用户执行 kill query thread_id_B 时，MySQL 里处理 kill 命令的线程做了两件事：**

1. 一个语句执行过程中有多处“埋点”，在这些“埋点”的地方判断线程状态，如果发现线程状态是 THD::KILL_QUERY，才开始进入语句终止逻辑；
2. 如果处于等待状态，必须是一个可以被唤醒的等待，否则根本不会执行到“埋点”处；
3. 语句从开始进入终止逻辑，到终止逻辑完全完成，是有一个过程的。

**再看一个 kill 不掉的例子**

**这个例子是 kill 无效的第一类情况，即：线程没有执行到判断线程状态的逻辑。**

**另一类情况是，终止逻辑耗时较长。**从show processlist结果上看也是Command=Killed，需要等到终止逻辑完成，语句才算真正完成。

1. 大事务执行期间被 kill。这时候，回滚操作需要对事务执行期间生成的所有新数据版本做回收操作，耗时很长。
2. 大查询回滚。如果查询过程中生成了比较大的临时文件，加上此时文件系统压力大，删除临时文件可能需要等待 IO 资源，导致耗时较长。
3. DDL 命令执行到最后阶段，如果被 kill，需要删除中间过程的临时文件，也可能受 IO 资源影响耗时较久。

不可以使用Ctrl+C命令来终止线程

### 另外两个关于客户端的误解 ###

#### 第一个误解是：如果库里面的表特别多，连接就会很慢。 ####

当使用默认参数连接的时候，MySQL 客户端会提供一个本地库名和表名补全的功能。为了实现这个功能，客户端在连接成功后，需要多做一些操作：

1. 执行 show databases；
2. 切到 db1 库，执行 show tables；
3. 这两个命令的结果用于构建一个本地的哈希表。

在这些操作中，最花时间的就是第三步在本地构建哈希表的操作。所以，当一个库中的表个数非常多的时候，这一步就会花比较长的时间。

**我们感知到的连接过程慢，其实并不是连接慢，也不是服务端慢，而是客户端慢。**（如果在连接命令中加上-A，就可以关掉这个自动补全的功能，然后客户端就可以快速返回了）

MySQL客户端发送请求后，接收服务端返回结果的方式有两种：

1. 一种是本地缓存，也就是在本地开一片内存，先把结果存起来，。如果用API开发，对应就是mysql_store_result方法。
2. 另一种是不缓存，读一个处理一个。对应的就是mysql_use_result方法。

MySQL客户端默认采用第一种方法时，而如果加上-quick参数，就会使用第二种不缓存的方式。（采用不缓存的方式时，如果本地处理得慢，就会导致服务端发送结果被阻塞，因此会让服务端变慢。）

1. 跳过表名自动补全功能；
2. mysql_store_result需要申请本地内存来缓存查询结果，如果查询结果太大，会耗费较多的本地内存，可能会影响客户端本地机器的性能；
3. 是不是把执行命令记录到本地的命令历史文件。

-quick参数的意思，是让客户端变得更快。

### 小结 ###

1. 和你介绍了 MySQL 中，有些语句和连接“kill 不掉”的情况。
2. “kill 不掉”的情况，其实是因为发送 kill 命令的客户端，并没有强行停止目标线程的执行，而只是设置了个状态，并唤醒对应的线程。而被 kill 的线程，需要执行到判断状态的“埋点”，才会开始进入终止逻辑阶段。并且，终止逻辑本身也是需要耗费时间的。
3. 发现一个线程处于 Killed 状态，通过影响系统环境，让这个 Killed 状态尽快结束。
	*  InnoDB 并发度的问题，你就可以临时调大 innodb_thread_concurrency 的值，或者停掉别的线程，让出位子给这个线程执行。
4. 如果是回滚逻辑由于受到 IO 资源限制执行得比较慢，就通过减少系统压力让它加速。

## 33 | 我查这么多数据，会不会数据库内存打爆？ ##

### 全表扫描对server层的影响 ###

	mysql -h$host -P$port -u$user -p$pwd -e "select * from db1.t" > $target_file

InnoDB的数据是保存在主键索引上的，所以全表扫描实际上是直接扫描表t的主键索引。

服务端不需要保存一个完整的结果集。取数据和发数据的流程是：

1. 获取一行，写到net_buffer中，这个内存的大小是由参数net_buffer_length定义的，默认是16k。
2. 重复获取行，直到net_buffer写满，调用网络接口发出去。
3. 如果发送成功，就清空net_buffer，然后继续取下一行，并写入net_buffer
4. 如果发送函数返回 EAGAIN 或 WSAEWOULDBLOCK，就表示本地网络栈（socket send buffer）写满了，进入等待。直到网络栈重新可写，再继续发送。

可以看出

1. 一个查询在发送过程中，占用的MySQL内部的内存最大就是net_buffer_length这么大，并不会达到200G；
2. socket send buffer也不可能达到200G（默认定义 /proc/sys/net/core/wmem_default），如果socket send buffer被写满，就会暂停读数据的流程。

MySQL是“边读变发的”。如果客户端接收的慢，会导致MySQL服务端由于结果发布出去，这个事务的执行时间变长。

	show processlist;

对于正常的线上业务来说，如果一个查询的返回结果不会很多的话，使用mysql_store_result这个接口，直接把查询结果保存到本地内存。

### 全表扫描对InnoDB的影响 ###

分析了 InnoDB 内存的一个作用，是保存更新的结果，再配合 redo log，就避免了随机写盘。

内存的数据页是在Buffer Pool(BP)中管理的，在WAL里Buffer Pool 

### 小结 ###

由于 MySQL 采用的是边算边发的逻辑，因此对于数据量很大的查询结果来说，不会在 server 端保存完整的结果集。所以，如果客户端读结果不及时，会堵住 MySQL 的查询过程，但是不会把内存打爆。

对于 InnoDB 引擎内部，由于有淘汰策略，大查询也不会导致内存暴涨。并且，由于 InnoDB 对 LRU 算法做了改进，冷数据的全表扫描，对 Buffer Pool 的影响也能做到可控。

全表扫描还是比较耗费 IO 资源的，所以业务高峰期还是不能直接在线上主库执行全表扫描的。

## 34 | 到底可不可以使用join？ ##

1. 我们 DBA 不让使用 join，使用 join 有什么问题呢？
2. 如果有两个大小不同的表做 join，应该用哪个表做驱动表呢？

### Index Nested-Loop Join ###

	select * from t34_1 straight_join t34_2 on (t34_1.a=t34_2.a);

t1是驱动表，t2是被驱动表，被驱动表t2的字段a上有索引，join过程用上了这个索引。

1. 从表t1中读入一行数据R；
2. 从数据行R中，取出a字段到表t2里去查找；
3. 取出表t2中满足条件的行，跟R组成一行，作为结果集的一部分；
4. 重复执行步骤1到3，直到表t1的末尾循环结束。

先遍历表t1，然后根据从表t1中取出的每行数据的a值，去表t2中查找满足条件的记录。在形式上，这个过程跟我们的嵌套查询类似，并且可以用上被驱动表的索引。——Index Nested-Loop Join，简称NLJ

1. 对驱动表t1做了全表扫描，这个过程需要扫描100行；
2. 而对于每一行R，根据a字段去表t2查找，走的是树搜索过程。由于我们构造的数据都是一一对应，因此每次的搜索过程都只搜索一行，也是总共扫描100行；
3. 所以，整个执行流程，总扫描行数是200。

#### 能不能使用join ####

用单表查询

1. 执行select * from t1，查出表 t1 的所有数据，这里有 100 行；
2. 循环遍历这 100 行数据：
	* 从每一行 R 取出字段 a 的值 $R.a；
	* 执行select * from t2 where a=$R.a；
	* 把返回的结果和 R 构成结果集的一行。

扫描了200行，但总共执行了101条语句，比直接join多了100次交互。

#### 怎么选择驱动表 ####

在这个 join 语句执行过程中，驱动表是走全表扫描，而被驱动表是走树搜索。

N+N*2*log2^M

1. 使用join语句，性能比强行拆成多个单表执行SQL语句的性能要好；
2. 如果使用join语句的话，需要让小表做驱动表。

### Simple Nested-Loop Join ###

	select * from t1 straight_join t2 on (t1.a=t2.b);

由于表 t2 的字段 b 上没有索引，因此再用图 2 的执行流程时，每次到 t2 去匹配的时候，就要做一次全表扫描。

### Block Nested-Loop Join ###

1. 把表 t1 的数据读入线程内存 join_buffer 中，由于我们这个语句中写的是 select *，因此是把整个表 t1 放入了内存；
2. 扫描表 t2，把表 t2 中的每一行取出来，跟 join_buffer 中的数据做对比，满足 join 条件的，作为结果集的一部分返回。

能不能使用 join 语句？

1. 如果可以使用 Index Nested-Loop Join 算法，也就是说可以用上被驱动表上的索引，其实是没问题的；
2. 如果使用 Block Nested-Loop Join 算法，扫描行数就会过多。尤其是在大表上的 join 操作，这样可能要扫描被驱动表很多次，会占用大量的系统资源。所以这种 join 尽量不要用。

如果要使用 join，应该选择大表做驱动表还是选择小表做驱动表？

1. 如果是 Index Nested-Loop Join 算法，应该选择小表做驱动表；
2. 如果是 Block Nested-Loop Join 算法：
	* 在 join_buffer_size 足够大的时候，是一样的；
	* 在 join_buffer_size 不够大的时候（这种情况更常见），应该选择小表做驱动表。

决定哪个表做驱动表的时候，应该是两个表按照各自的条件过滤，过滤完成之后，计算参与 join 的各个字段的总数据量，数据量小的那个表，就是“小表”，应该作为驱动表。

### 小结 ###

MySQL 执行 join 语句的两种可能算法，这两种算法是由能否使用被驱动表的索引决定的。而能否用上被驱动表的索引，对 join 语句的性能影响很大。

通过对 Index Nested-Loop Join 和 Block Nested-Loop Join 两个算法执行过程的分析

1. 如果可以使用被驱动表的索引，join 语句还是有其优势的；
2. 不能使用被驱动表的索引，只能使用 Block Nested-Loop Join 算法，这样的语句就尽量不要使用；
3. 在使用 join 的时候，应该让小表做驱动表。

使用 Block Nested-Loop Join 算法，可能会因为 join_buffer 不够大，需要对被驱动表做多次全表扫描。

### 精选留言 ###

## 35 | join语句怎么优化？ ##

join 语句的两种算法，分别是 Index Nested-Loop Join(NLJ) 和 Block Nested-Loop Join(BNL)。


	create table t35_1(id int primary key, a int, b int, index(a));
	create table t35_2 like t35_1;
	drop procedure it35data;
	delimiter ;;
	create procedure it35data()
	begin
	  declare i int;
	  set i=1;
	  while(i<=1000)do
	    insert into t35_1 values(i, 1001-i, i);
	    set i=i+1;
	  end while;
	  
	  set i=1;
	  while(i<=1000000)do
	    insert into t35_2 values(i, i, i);
	    set i=i+1;
	  end while;

	end;;
	delimiter ;
	call it35data();

在表t1里，插入了1000行数据，每一行的a=1000-id的值。也就是说，表t1中字段a是逆序的。同时，在表t2中插入了100万行数据。

### Multi-Range Read优化 ###

Multi-Range Read优化（MRR）。这个优化的主要目的是尽量使用顺序读盘。

![97ae269061192f6d7a632df56fa03605.png](/img/97ae269061192f6d7a632df56fa03605.png)

如果随着a的值递增顺序查询的话，id的值就变成随机的，那么就会出现随机访问，性能相对较差。虽然“按行查”这个机制不能改，但是调整查询的顺序，还是能够加速的。

因为大多数的数据都是按照主键递增顺序插入得到的，所以我们可以认为，如果按照主键的递增顺序查询的话，对磁盘的读比较接近顺序读，能够提升读性能。

1. 根据索引a，定位到满足条件的记录，将id值放入read_rnd_buffer中；
2. 将read_rnd_buffer中的id进行递增排序；
3. 排序后的id数组，依次到主键id索引中查记录，并作为结果返回。

read_rnd_buffer的大小是由read_rnd_buffer_size参数控制的。如果步骤1中，read_rnd_buffer放满了，就会先执行完步骤2和3，然后清空read_rnd_buffer。之后继续找索引a的下一个记录，并继续循环。

*MRR能够提升性能的核心*,这条查询语句在索引a上做的是范围查询（多值查询），可以得到足够多的主键id。这样通过排序以后，再去主键索引查数据，才能体现出“顺序性”的优势。

### Batched Key Access ###

BKA算法，对NLJ算法的优化

NLJ 算法执行的逻辑是：从驱动表 t1，一行行地取出 a 的值，再到被驱动表 t2 去做 join。也就是说，对于表 t2 来说，每次都是匹配一个值。

### BNL 算法的性能问题 ###

大表 join 操作虽然对 IO 有影响，但是在语句执行结束后，对 IO 的影响也就结束了。但是，对 Buffer Pool 的影响就是持续性的，需要依靠后续的查询请求慢慢恢复内存命中率。

增大join_buffer_size的值，减少对被驱动表的扫描次数。

BNL 算法对系统的影响主要包括三个方面：

1. 可能会多次扫描被驱动表，占用磁盘 IO 资源；
2. 判断 join 条件需要执行 M*N 次对比（M、N 分别是两张表的行数），如果是大表就会占用非常多的 CPU 资源；
3. 可能会导致 Buffer Pool 的热数据被淘汰，影响内存命中率。

### BNL 转 BKA ###

直接在被驱动表建索引，直接转成BKA算法。

### 扩展-hash join ###



### 小结 ###

分享了 Index Nested-Loop Join（NLJ）和 Block Nested-Loop Join（BNL）的优化方法。

1. BKA 优化是 MySQL 已经内置支持的，建议你默认使用；
2. BNL 算法效率低，建议你都尽量转成 BKA 算法。优化的方向就是给被驱动表的关联字段加上索引；
3. 基于临时表的改进方案，对于能够提前过滤出小数据的 join 语句来说，效果还是很好的；
4. MySQL 目前的版本还不支持 hash join，但你可以配合应用端自己模拟出来，理论上效果要好于临时表的方案。

### 思考题 ###



## 36 | 为什么临时表可以重名？ ##

	create temporary table temp_t like t1;
	alter table temp_t add index(b);
	insert into temp_t select * from t2 where b>=1 and b<=2000;
	select * from t1 join temp_t on (t1.b=temp_t.b);

临时表和普通表的区别

* 内存表，指的是使用Memory引擎的表，建表语法是create table... engine=memory。这种表的数据保存在内存里，系统重启的时候会被清空，但是表结构还在。
* 临时表，可以使用各种引擎类型 。如果是使用 InnoDB 引擎或者 MyISAM 引擎的临时表，写数据的时候是写到磁盘上的。当然，临时表也可以使用 Memory 引擎。

### 临时表的特性 ###

1. 表语法是 create temporary table …。
2. 一个临时表只能被创建它的 session 访问，对其他线程不可见。所以，图中 session A 创建的临时表 t，对于 session B 就是不可见的。
3. 临时表可以与普通表同名。
4. session A 内有同名的临时表和普通表的时候，show create 语句，以及增删改查语句访问的是临时表。
5. show tables 命令不显示临时表。

临时表就特别适合我们文章开头的 join 优化这种场景。

1. 不同 session 的临时表是可以重名的，如果有多个 session 同时执行 join 优化，不需要担心表名重复导致建表失败的问题。
2. 不需要担心数据删除问题。如果使用普通表，在流程执行过程中客户端发生了异常断开，或者数据库发生异常重启，还需要专门来清理中间过程中生成的数据表。而临时表由于会自动回收，所以不需要这个额外的操作。

### 临时表的应用 ###

由于不用担心线程之间的重名冲突，临时表经常会被用在复杂查询的优化过程中。

### 为什么临时表可以重名？ ###

### 临时表和主备复制 ###

在主库上执行下面语句序列：

	create table t_normal(id int primary key, c int)engine=innodb;/*Q1*/
	create temporary table temp_t like t_normal;/*Q2*/
	insert into temp_t values(1,1);/*Q3*/
	insert into t_normal select * from temp_t;/*Q4*/

如果关于临时表的操作都不记录，那么在备库就只有 create table t_normal 表和 insert into t_normal select * from temp_t 这两个语句的 binlog 日志，备库在执行到 insert into t_normal 的时候，就会报错“表 temp_t 不存在”。

创建临时表的语句会传到备库执行，因此备库的同步线程就会创建这个临时表。主库在线程退出的时候，会自动删除临时表，但是备库同步线程是持续在运行的。所以，这时候我们就需要在主库上再写一个 DROP TEMPORARY TABLE 传给备库执行。

### 小结 ###

在实际应用中，临时表一般用于处理比较复杂的计算逻辑。由于临时表是每个线程自己可见的，所以不需要考虑多个线程执行同一个处理逻辑时，临时表的重名问题。在线程退出的时候，临时表也能自动删除，省去了收尾和异常处理的工作。

在 binlog_format='row’的时候，临时表的操作不记录到 binlog 中，也省去了不少麻烦，这也可以成为你选择 binlog_format 时的一个考虑因素。

需要注意的是，我们上面说到的这种临时表，是用户自己创建的 ，也可以称为用户临时表。

## 37 | 什么时候会使用内部临时表？ ##

	create table t37_1(id int primary key, a int, b int, index(a));
	delimiter ;;
	create procedure idatat37_1()
	begin
	  declare i int;

	  set i=1;
	  while(i<=1000)do
	    insert into t37_1 values(i, i, i);
	    set i=i+1;
	  end while;
	end;;
	delimiter ;
	call idatat37_1();

### union执行流程 ###

	(select 1000 as f) union (select id from t35_1 order by id desc limit 2);

1. 第二行的 key=PRIMARY，说明第二个子句用到了索引 id。
2. 第三行的 Extra 字段，表示在对子查询的结果集做 union 的时候，使用了临时表 (Using temporary)。

语句的执行流程：

1. 创建一个内存临时表，这个临时表只有一个整型字段f，并且f是主键字段。
2. 执行第一个子查询，得到1000这个值，并存入临时表。
3. 执行第二个子查询：
	* 拿到第一行id=1000，试图插入临时表。但由于1000这个值已经存在于临时表了，违反了唯一性约束，所以插入失败，然后继续执行；
	* 取到第二行id=999，插入临时表成功。
4. 从临时表中按行取出数据，返回结果，并删除临时表，结果中包含两行数据分别是1000和999。

内存临时表起到了暂存数据的作用，而且计算过程用上了临时表主键id的唯一性约束，实现了union的语义。

### group by 执行流程 ###

常见的使用临时表的例子是 group by

	select id%10 as m, count(*) as c from t37_1 group by m;

这个语句的逻辑是把表 t37_1 里的数据，按照 id%10 进行分组统计，并按照 m 的结果排序后输出

1. 在Extra字段里面，看到三个信息（Using index; Using temporary; Using filesort）：

* Using index：表示这个语句使用了覆盖索引，选择了索引a，不需要回表
* Using temporary，表示使用了临时表；
* Using filesort，表示需要排序。

语句的执行流程：

1. 创建内存临时表，表里有两个字段m和c，主键是m；
2. 扫描表t1的索引a，依次取出叶子节点上的id值，计算id%10的结果，记为x；
	* 如果临时表中没有主键为x的行，就插入一个记录（x,1）；
	* 如果表中有主键为 x 的行，就将 x 这一行的 c 值加 1；
3. 遍历完成后，再根据字段 m 做排序，得到结果集返回给客户端。

如果不需要排序则使用order by null

### group by 优化方法 -- 索引 ###

group by 的语义逻辑，是统计不同的值出现的个数。但是，由于每一行的 id%100 的结果是无序的，所以我们就需要有一个临时表，来记录并统计结果。

### group by 优化方法 -- 直接排序 ###

在 group by 语句中加入 SQL_BIG_RESULT 这个提示（hint），就可以告诉优化器：这个语句涉及的数据量很大，请直接用磁盘临时表。

MySQL 的优化器一看，磁盘临时表是 B+ 树存储，存储效率不如数组来得高。所以，既然你告诉我数据量很大，那从磁盘空间考虑，还是直接用数组来存吧。
	
	select SQL_BIG_RESULT id%100 as m, count(*) as c from t1 group by m;

1. 初始化 sort_buffer，确定放入一个整型字段，记为 m；
2. 扫描表 t1 的索引 a，依次取出里面的 id 值, 将 id%100 的值存入 sort_buffer 中；
3. 扫描完成后，对 sort_buffer 的字段 m 做排序（如果 sort_buffer 内存不够用，就会利用磁盘临时文件辅助排序）；
4. 排序完成后，就得到了一个有序数组。

### 小结 ###

1. 如果对 group by 语句的结果没有排序要求，要在语句后面加 order by null
2. ；尽量让 group by 过程用上表的索引，确认方法是 explain 结果里没有 Using temporary 和 Using filesort；
3. 如果 group by 需要统计的数据量不大，尽量只使用内存临时表；也可以通过适当调大 tmp_table_size 参数，来避免用到磁盘临时表；
4. 如果数据量实在太大，使用 SQL_BIG_RESULT 这个提示，来告诉优化器直接使用排序算法得到 group by 的结果。

## 38 | 都说InnoDB好，还要不要使用Memory引擎 ##

## 39 | 自增主键为什么不是连续的？ ##

	CREATE TABLE `t39` (
	  `id` int(11) NOT NULL AUTO_INCREMENT,
	  `c` int(11) DEFAULT NULL,
	  `d` int(11) DEFAULT NULL,
	  PRIMARY KEY (`id`),
	  UNIQUE KEY `c` (`c`)
	) ENGINE=InnoDB;

### 自增值保存在哪儿？ ###

`insert into t39 values(null, 1, 1);`插入一行数据
`show create table t39`看到表定义里面出现了AUTO_INCREMENT=2，表示下一次插入数据时，如果需要自动生成自增值，会生成id=2

表的结构定义存放在后缀名为.frm 的文件中，但是并不会保存自增值。不同的引擎对于自增值的保存策略不同：

* MyISAM引擎的自增值保存在数据文件中
* InnoDB引擎的自增值，其实时保存在了内存里，并且到了MySQL8.0版本后，才有了“自增值持久化”的能力，也就是才实现了“如果发生重启，表的自增值可以恢复为MySQL重启前的值”，具体情况是：
	* 在 MySQL 5.7 及之前的版本，自增值保存在内存里，并没有持久化。每次重启后，第一次打开表的时候，都会去找自增值的最大值 max(id)，然后将 max(id)+1 作为这个表当前的自增值。﻿
	* 在 MySQL 8.0 版本，将自增值的变更记录在了 redo log 中，重启的时候依靠 redo log 恢复重启之前的值。

### 自增值修改机制 ###

在MySQL里面，如果字段id被定义为AUTO_INCREMENT，在插入一行数据的时候，自增值的行为如下：

1. 如果插入数据时 id 字段指定为 0、null 或未指定值，那么就把这个表当前的 AUTO_INCREMENT 值填到自增字段；
2. 如果插入数据时 id 字段指定了具体的值，就直接使用语句里指定的值。

假设，某次要插入的值是 X，当前的自增值是 Y。

1. 如果X<Y，那么这个表的自增值不变
2. 如果X>=Y，就需要把当前自增值修改为新的自增值

新的自增值生成算法是：从 auto_increment_offset 开始，以 auto_increment_increment 为步长，持续叠加，直到找到第一个大于 X 的值，作为新的自增值。

其中，auto_increment_offset 和 auto_increment_increment 是两个系统参数，分别用来表示自增的初始值和步长，默认值都是 1。

当 auto_increment_offset 和 auto_increment_increment 都是 1 的时候，新的自增值生成逻辑很简单，就是：

1. 如果准备插入的值 >= 当前自增值，新的自增值就是“准备插入的值 +1”；
2. 否则，自增值不变。

### 自增值的修改时机 ###

表 t 里面已经有了 (1,1,1) 这条记录，这时我再执行一条插入数据命令：

	insert into t values(null, 1, 1); 

1. 执行器调用 InnoDB 引擎接口写入一行，传入的这一行的值是 (0,1,1);
2. InnoDB 发现用户没有指定自增 id 的值，获取表 t 当前的自增值 2；
3. 将传入的行的值改成 (2,1,1);
4. 将表的自增值改成 3；
5. 继续执行插入数据操作，由于已经存在 c=1 的记录，所以报 Duplicate key error，语句返回。

这个表的自增值改成 3，是在真正执行插入数据的操作之前。这个语句真正执行的时候，因为碰到唯一键 c 冲突，所以 id=2 这一行并没有插入成功，但也没有将自增值再改回去。

在这之后，再插入新的数据行时，拿到的自增 id 就是 3。也就是说，出现了自增主键不连续的情况。

* 唯一键冲突是导致自增主键 id 不连续的第一种原因。
* 事务回滚也会产生类似的现象，这就是第二种原因。

#### 自增值为什么不能回退 ####

假设有两个并行执行的事务，在申请自增值的时候，为了避免两个事务申请到相同的自增 id，肯定要加锁，然后顺序申请。

1. 假设事务 A 申请到了 id=2， 事务 B 申请到 id=3，那么这时候表 t 的自增值是 4，之后继续执行。
2. 事务 B 正确提交了，但事务 A 出现了唯一键冲突。
3. 如果允许事务 A 把自增 id 回退，也就是把表 t 的当前自增值改回 2，那么就会出现这样的情况：表里面已经有 id=3 的行，而当前的自增 id 值是 2。
4. 接下来，继续执行的其他事务就会申请到 id=2，然后再申请到 id=3。这时，就会出现插入语句报错“主键冲突”。

而解决这个主键冲突，有两种方法：

1. 每次申请 id 之前，先判断表里面是否已经存在这个 id。如果存在，就跳过这个 id。但是，这个方法的成本很高。因为，本来申请 id 是一个很快的操作，现在还要再去主键索引树上判断 id 是否存在。
2. 把自增 id 的锁范围扩大，必须等到一个事务执行完成并提交，下一个事务才能再申请自增 id。这个方法的问题，就是锁的粒度太大，系统并发能力大大下降。

InnoDB 放弃了这个设计，语句执行失败也不回退自增 id。也正是因为这样，所以才只保证了自增 id 是递增的，但不保证是连续的。

### 自增锁的优化 ###

1. 在MySQL5.0版本之前，自增锁的范围是语句级别。如果一个语句申请了一个表自增锁，这个锁会等语句执行结束以后才释放。（影响并发度）
2. MySQL 5.1.22 版本引入了一个新策略，新增参数 innodb_autoinc_lock_mode，默认值是 1。
	* 这个参数的值被设置为 0 时，表示采用之前 MySQL 5.0 版本的策略，即语句执行结束后才释放锁；
	* 这个参数的值被设置为 1 时：
		* 普通 insert 语句，自增锁在申请之后就马上释放；
		* 类似 insert … select 这样的批量插入数据的语句，自增锁还是要等语句结束后才被释放；
	* 这个参数的值被设置为 2 时，所有的申请自增主键的动作都是申请后就释放锁。

在生产上，尤其是有 insert … select 这种批量插入数据的场景时，从并发插入数据性能的角度考虑，我建议你这样设置：innodb_autoinc_lock_mode=2 ，并且 binlog_format=row. 这样做，既能提升并发性，又不会出现数据一致性问题。

对于批量插入数据的语句，MySQL 有一个批量申请自增 id 的策略：

1. 语句执行过程中，第一次申请自增 id，会分配 1 个；
2. 1 个用完以后，这个语句第二次申请自增 id，会分配 2 个；
3. 2 个用完以后，还是这个语句，第三次申请自增 id，会分配 4 个；
4. 依此类推，同一个语句去申请自增 id，每次申请到的自增 id 个数都是上一次的两倍。

insert…select，实际上往表 t2 中插入了 4 行数据。但是，这四行数据是分三次申请的自增 id，第一次申请到了 id=1，第二次被分配了 id=2 和 id=3， 第三次被分配到 id=4 到 id=7。

由于这条语句实际只用上了 4 个 id，所以 id=5 到 id=7 就被浪费掉了。之后，再执行 insert into t2 values(null, 5,5)，实际上插入的数据就是（8,5,5)。

这是主键 id 出现自增 id 不连续的第三种原因。

### 小结 ###

“自增主键为什么会出现不连续的值”，这个问题开始，讨论了自增值的存储。

* 在 MyISAM 引擎里面，自增值是被写在数据文件上的。
* 在 InnoDB 中，自增值是被记录在内存的。
	* MySQL 直到 8.0 版本，才给 InnoDB 表的自增值加上了持久化的能力，确保重启前后一个表的自增值不变。

## 40 | insert语句的锁为什么这么多？ ##



## 41 | 怎么最快地复制一张表 ##

	create database db1;
	use db1;
	
	create table t(id int primary key, a int, b int, index(a))engine=innodb;
	delimiter ;;
	  create procedure idata()
	  begin
	    declare i int;
	    set i=1;
	    while(i<=1000)do
	      insert into t values(i,i,i);
	      set i=i+1;
	    end while;
	  end;;
	delimiter ;
	call idata();
	
	create database db2;
	create table db2.t like db1.t

### mysqldump 方法 ###

使用 mysqldump 命令将数据导出成一组 INSERT 语句

	mysqldump -h$host -P$port -u$user --add-locks=0 --no-create-info --single-transaction  --set-gtid-purged=OFF db1 t --where="a>900" --result-file=/client_tmp/t.sql

1. `–single-transaction` 的作用是，在导出数据的时候不需要对表 db1.t 加表锁，而是使用 START TRANSACTION WITH CONSISTENT SNAPSHOT 的方法；
2. `–add-locks` 设置为 0，表示在输出的文件结果里，不增加" LOCK TABLES t WRITE;" ；
3. `–no-create-info` 的意思是，不需要导出表结构；
4. `–set-gtid-purged=off` 表示的是，不输出跟 GTID 相关的信息；
5. `–result-file` 指定了输出文件的路径，其中 client 表示生成的文件是在客户端机器上的。

一条 INSERT 语句里面会包含多个 value 对，这是为了后续用这个文件来写入数据的时候，执行速度可以更快。

	mysql -h127.0.0.1 -P13000  -uroot db2 -e "source /client_tmp/t.sql"

source并不是

### 导出CSV文件 ###

将结果直接导出成.csv文件。

	select * from db1.t where a>900 into outfile '/server_tmp/t.csv';

1. 这条语句会将结果保存在服务端。如果你执行命令的客户端和 MySQL 服务端不在同一个机器上，客户端机器的临时目录下是不会生成 t.csv 文件的。
2. into outfile 指定了文件的生成位置（/server_tmp/），这个位置必须受参数 secure_file_priv 的限制。参数 secure_file_priv 的可选值和作用分别是：
	* 如果设置为 empty，表示不限制文件生成的位置，这是不安全的设置；
	* 如果设置为一个表示路径的字符串，就要求生成的文件只能放在这个指定的目录，或者它的子目录；
	* 如果设置为 NULL，就表示禁止在这个 MySQL 实例上执行 select … into outfile 操作。
3. 这条命令不会帮你覆盖文件，因此你需要确保 /server_tmp/t.csv 这个文件不存在，否则执行语句时就会因为有同名文件的存在而报错。
4. 这条命令生成的文本文件中，原则上一个数据行对应文本文件的一行。但是，如果字段中换行符，在生成的文本中也有换行符。不过类似换行符、制表符这类符号，前面都会跟上“\”这个转义符，这样就可以跟字段之间、数据行之间的分隔符区分开。

### 物理拷贝方法 ###

逻辑导逻辑的方法，从db1.t中读出来，生成文本，再写入目标表db2.t中。

一个InnoDB表，除了包含这两个物理文件外，还需要在数据字典中注册。直接拷贝这两个文件的话，因为数据字典中没有 db2.t 这个表，系统是不会识别和接受它们的。

在 MySQL 5.6 版本引入了可传输表空间(transportable tablespace) 的方法，可以通过导出 + 导入表空间的方式，实现物理拷贝表的功能。

假设我们现在的目标是在 db1 库下，复制一个跟表 t 相同的表 r，具体的执行步骤如下：

1. 执行 create table r like t，创建一个相同表结构的空表；
2. 执行 alter table r discard tablespace，这时候 r.ibd 文件会被删除；
3. 执行 flush table t for export，这时候 db1 目录下会生成一个 t.cfg 文件；
4. 在 db1 目录下执行 cp t.cfg r.cfg; cp t.ibd r.ibd；这两个命令（这里需要注意的是，拷贝得到的两个文件，MySQL 进程要有读写权限）；
5. 执行 unlock tables，这时候 t.cfg 文件会被删除；
6. 执行 alter table r import tablespace，将这个 r.ibd 文件作为表 r 的新的表空间，由于这个文件的数据内容和 t.ibd 是相同的，所以表 r 中就有了和表 t 相同的数据。

关于拷贝表的这个流程

1. 在第 3 步执行完 flsuh table 命令之后，db1.t 整个表处于只读状态，直到执行 unlock tables 命令后才释放读锁；
2. 在执行 import tablespace 的时候，为了让文件里的表空间 id 和数据字典中的一致，会修改 r.ibd 的表空间 id。而这个表空间 id 存在于每一个数据页中。因此，如果是一个很大的文件（比如 TB 级别），每个数据页都需要修改，所以你会看到这个 import 语句的执行是需要一些时间的。当然，如果是相比于逻辑导入的方法，import 语句的耗时是非常短的。

### 小结 ###

三种方法将一个表的数据导入到另外一个表中，各自的优缺点：

1. 物理拷贝的方式速度最快，尤其对于大表拷贝来说是最快的方法。如果出现误删表的情况，用备份恢复出误删之前的临时表，然后再把临时库中的表拷贝到生产库上，是恢复数据最快的方法。但有一定的局限性：
* 必须是全表拷贝，不能只拷贝部分数据；
* 需要到服务器上拷贝数据，在用户无法登录数据库主机的场景下无法使用；
* 由于是通过拷贝物理文件实现的，源表和目标表都是使用 InnoDB 引擎时才能使用。
2. 用 mysqldump 生成包含 INSERT 语句文件的方法，可以在 where 参数增加过滤条件，来实现只导出部分数据。这个方式的不足之一是，不能使用 join 这种比较复杂的 where 条件写法。
3. 用 select … into outfile 的方法是最灵活的，支持所有的 SQL 写法。但，这个方法的缺点之一就是，每次只能导出一张表的数据，而且表结构也需要另外的语句单独备份。

逻辑备份方式，是可以跨引擎使用的

## 42 | grant之后要跟着flush privileges吗 ##

## 43 | 要不要使用分区表？ ##

### 分区表是什么？ ###

	CREATE TABLE `t43` (
	  `ftime` datetime NOT NULL,
	  `c` int(11) DEFAULT NULL,
	  KEY (`ftime`)
	) ENGINE=InnoDB DEFAULT CHARSET=latin1
	PARTITION BY RANGE (YEAR(ftime))
	(PARTITION p_2017 VALUES LESS THAN (2017) ENGINE = InnoDB,
	 PARTITION p_2018 VALUES LESS THAN (2018) ENGINE = InnoDB,
	 PARTITION p_2019 VALUES LESS THAN (2019) ENGINE = InnoDB,
	PARTITION p_others VALUES LESS THAN MAXVALUE ENGINE = InnoDB);
	insert into t43 values('2017-4-1',1),('2018-4-1',1);

* 对于引擎层来说，就是4个表
* 对于Server层来说，这是1个表

### 分区表的引擎层行为 ###

分区表和手工分表，一个是由server层来决定使用哪个分区，一个是由应用层代码来决定使用哪个分表。

### 分区策略 ###

每当第一次访问一个分区表的时候，MySQL 需要把所有的分区都访问一遍。

一个典型的报错情况是这样的：如果一个分区表的分区很多，比如超过了 1000 个，而 MySQL 启动的时候，open_files_limit 参数使用的是默认值 1024，那么就会在访问这个表的时候，由于需要打开所有的文件，导致打开表文件的个数超过了上限而报错。

MyISAM 分区表使用的分区策略，我们称为**通用分区策略**（generic partitioning），每次访问分区都由 server 层控制。通用分区策略，是 MySQL 一开始支持分区表的时候就存在的代码，在文件管理、表管理的实现上很粗糙，因此有比较严重的性能问题。

从MySQL5.7.9开始，InnoDB引擎引入了**本地分区策略**（native partitioning）这个策略是在 InnoDB 内部自己管理打开分区的行为。

5.7.17开始，将MyISAM分区表标记为即将弃用 (deprecated)，意思是“从这个版本开始不建议这么使用，请使用替代方案。在将来的版本中会废弃这个功能”。

从 MySQL 8.0 版本开始，就不允许创建 MyISAM 分区表了，只允许创建已经实现了本地分区策略的引擎。目前来看，只有 InnoDB 和 NDB 这两个引擎支持了本地分区策略。

### 分区表的server层行为 ###

如果从server层看的话，一个分区表就只是一个表。

1. MySQL在第一次打开分区表的时候，需要访问所有的分区；
2. 在 server 层，认为这是同一张表，因此所有分区共用同一个 MDL 锁；
3. 在引擎层，认为这是不同的表，因此 MDL 锁之后的执行过程，会根据分区表规则，只访问必要的分区。

### 分区表的应用场景 ###

分区表的一个显而易见的优势是对业务透明，相对于用户分表来说，使用分区表的业务代码更简洁。还有，分区表可以很方便的清理历史数据。

### 小结 ###

分区表跟用户分表比起来，有两个绕不开的问题：

* 第一次访问的时候需要访问所有分区
* 共用 MDL 锁

如果要使用分区表，就不要创建太多的分区。我见过一个用户做了按天分区策略，然后预先创建了 10 年的分区。这种情况下，访问分区表的性能自然是不好的。这里有两个问题需要注意：

1. 分区并不是越细越好。
2. 分区也不要提前预留太多，在使用之前预先创建即可。

至于分区表的其他问题，比如查询需要跨多个分区取数据，查询性能就会比较慢，基本上就不是分区表本身的问题，而是数据量的问题或者说是使用方式的问题了。

## 44 | 答疑文章（三）：说一说这些好问题 ##

### join的写法 ###

1. 如果用 left join 的话，左边的表一定是驱动表吗？
2. 如果两个表的 join 包含多个条件的等值匹配，是都要写到 on 里面呢，还是只把一个条件写到 on 里面，其他条件写到 where 部分？



## 45 | 自增id用完怎么办？ ##

### 表定义自增值 id ###

表定义的自增值达到上限后的逻辑是，再申请下一个id时，得到的值保持不变。

	create table t45(id int unsigned auto_increment primary key) auto_increment=4294967295;
	insert into t45 values(null);
	//成功插入一行 4294967295
	show create table t45;
	/* CREATE TABLE `t45` (
	  `id` int(10) unsigned NOT NULL AUTO_INCREMENT,
	  PRIMARY KEY (`id`)
	) ENGINE=InnoDB AUTO_INCREMENT=4294967295;
	*/
	
	insert into t45 values(null);
	//Duplicate entry '4294967295' for key 'PRIMARY'

第一个 insert 语句插入数据成功后，这个表的 AUTO_INCREMENT 没有改变（还是 4294967295），就导致了第二个 insert 语句又拿到相同的自增 id 值，再试图执行插入语句，报主键冲突错误。

232-1（4294967295）不是一个特别大的数，对于一个频繁插入删除数据的表来说，是可能会被用完的。因此在建表的时候你需要考察你的表是否有可能达到这个上限，如果有可能，就应该创建成 8 个字节的 bigint unsigned。

### InnoDB系统自增row_id ###

如果你创建的 InnoDB 表没有指定主键，那么 InnoDB 会给你创建一个不可见的，长度为 6 个字节的 row_id。InnoDB 维护了一个全局的 dict_sys.row_id 值，所有无主键的 InnoDB 表，每插入一行数据，都将当前的 dict_sys.row_id 值作为要插入数据的 row_id，然后把 dict_sys.row_id 的值加 1。

1. row_id 写入表中的值范围，是从 0 到 248-1；
2. 当 dict_sys.row_id=248时，如果再有插入数据的行为要来申请 row_id，拿到以后再取最后 6 个字节的话就是 0。

## 直播回顾 | 林晓斌：我的 MySQL 心路历程 ##

## 结束语 | 点线网面，一起构建MySQL知识网络 ##

### 1. 路径千万条，实践第一条 ###

* 跟着专栏中的案例做实验，继续坚持下去。在阅读其他技术文章、图书的时候，也是同样的道理。如果你觉得自己理解了一个知识点，也一定要尝试设计一个例子来验证它。
* 在设计案例的时候，我建议你也设计一个对照的反例，从而达到知识融汇贯通的目的。

### 2. 原理说不清，双手白费劲 ###

1. 先实践再搞清楚原理
2. 先明白原理再通过实践去验证

怎么证明是是不是真的把原理弄清楚了呢？——说出来、写出来

如果有人请教你这个问题：

1. 验证自己搞懂了这个知识点
2. 提升自己的技术表达能力

“写出来”又是一个更高的境界。因为，你在写的过程中，就会发现这个“明白”很可能只是一个假象。所以，在专栏下面写下自己对本章知识点的理解，也是一个不错的夯实学习成果的方法。

### 3. 知识没体系，转身就忘记 ###

知识点“写下来”，还有一个好处，就是你会发现这个知识点的关联知识点。深究下去，点就连成线，然后再跟别的线找交叉。

### 4. 手册补全面，案例扫盲点 ###

一开始就看手册？看手册的时机，应该是知识网络构建得差不多的时候。

“差不多”的标准：

* 能否解释清楚错误日志（error log）、慢查询日志（slow log）中每一行的意思？
* 能否快速评估出一个表结构或者一条 SQL 语句，设计得是否合理？
* 能否通过 explain 的结果，来“脑补”整个执行过程（我们已经在专栏中练习几次了）？
* 到网络上找 MySQL 的实践建议，对于每一条做一次分析：
	* 如果觉得不合理，能否给出自己的意见？
	* 如果觉得合理，能否给出自己的解释？
	* 找有经验的人讨论

# 特别放送 #

## MySQL中6个常见的日志问题 ##

MySQL里有两个日志，即：重做日志（redo log）和归档日志（binlog）。

* binlog可以给

### 问题 ###

问题1：MySQL怎么知道binlog是完整的？

回答：一个事务的binlog是有完整格式的：

* statement格式的binlog，最后会有COMMIT；
* row格式的binlog，最后会有一个XID event。

在MySQL5.6.2版本以后，还引入了binlog-checksum参数，用来验证binlog内容的正确性。对于binlog日志由于磁盘原因，可能会在日志中间出错的情况，MySQl可以通过校验checksum的结果来发现。所以，MySQL还是有版本验证事务binlog的完整性的。

问题2：redo log和binlog是怎么关联起来的？

回答：他们有共同的数据字段，叫XID。崩溃恢复的时候，会按顺序扫描redo log：

* 如果碰到既有prepare，又有commit的redo log，就直接提交。
* 如果碰到只有prepare，而没有commit的redo log，就拿着XID去binlog找对应的事务。

问题3：处于prepare阶段的redo log加上完整binlog，重启就能恢复，MySQL为什么要这么设计？

问题4：如果这样的话，为什么还要两阶段提交呢？干脆先redo log写完，再写binlog。崩溃恢复的时候，必须得两个日志都完整才可以。是不是一样得逻辑？

答：两阶段提交是经典的分布式系统问题，并不是MySQL独有的。

两阶段提交就是为了给所有人一个几回，当每个人都说“我ok”的时候，再一起提交。

问题5：不引入两个日志，也就没有两阶段提交的必要了。只用binlog来支持崩溃恢复，又能支持归档，不就可以了？

答：binlog没有能力恢复“数据页”

问题6：那能不能反过来，只用redo log，不要binlog？

# 直播回顾 #

## 直播回顾 | 林晓斌：我的 MySQL 心路历程 ##

1. 我和MySQL打交道的经历；
2. 你为什么要了解数据库原理；
3. 我建议的MySQL学习路径；
4. DBA的修炼之道。

### 为什么要了解数据库原理？ ###

#### 了解原理能帮你更好地定位问题 ####

#### 了解原理能够让你更巧妙地解决问题 ####

#### 看得懂源码让你有更多的方法 ####

### MySQL学习路径 ###

学习路径：

1. 会用，了解每个参数的意义，要去了解每个参数的实现原理，一旦你了解了这些原理。不要别人怎么用，自己就怎么用。
	* 了解原理
	* 看懂源码
2. 会用，然后发现问题
3. 实践：去看 MySQL 的官方手册
	* 先要有自己的脉络
	* 搭建自己的知识网络
	* 看手册查漏补缺
	* 搭配《高性能MySQL》

### DBA的修炼 ###

#### DBA 和开发工程师有什么相同点？ ####

开发要了解数据库原理，DBA 要了解业务和开发。

#### DBA 有前途吗？ ####

每个岗位都有前途，只需要根据时代变迁稍微调整一下方向。

To：

* 了解业务，做业务的架构师
* 是要有前瞻性，做主动诊断系统

#### 有哪些比较好的习惯和提高 SQL 效率的方法？ ####

要多写 SQL，培养自己对 SQL 语句执行效率的感觉。以后再写或者建索引的时候，知道这个语句执行下去大概的时间复杂度，是全表扫描还是索引扫描、是不是需要回表，在心里都有一个大概的概念。

这样每次写出来的 SQL 都会快一点，而且不容易犯低级错误。

#### 看源码需要什么技术？ ####

看源码的话，一是要掌握 C 和 C++；另外还要熟悉一些调试工具。因为代码是静态的，运行起来是动态的，看代码是单线程的，运行起来是多线程的，所以要会调试。

尽量手写代码

#### 怎么学习 C、C++？ ####

有的人看完技术博客和专栏，会把这篇文章的提纲列一下，写写自己的问题和对这篇文章的理解。这个过程，是非常利于学习的。因为你听进来是一回事儿，讲出去则是另一回事儿。

能落地到文本当中，过一段时间回来看能看得懂

#### 学数据库要保持什么心态？ ####

不只是数据库，所有多线程的服务，调试和追查问题的过程都是很枯燥的，遇到问题都会很麻烦。

鼓噪乏味，但需要坚持

